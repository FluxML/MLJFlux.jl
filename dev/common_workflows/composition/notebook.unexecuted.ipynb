{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model Composition with MLJFlux"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This demonstration is available as a Jupyter notebook or julia script\n",
    "[here](https://github.com/FluxML/MLJFlux.jl/tree/dev/docs/src/common_workflows/composition)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this workflow example, we see how MLJFlux enables composing MLJ models with MLJFlux\n",
    "models. We will assume a class imbalance setting and wrap an oversampler with a deep\n",
    "learning model from MLJFlux."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Pkg\n",
    "Pkg.activate(@__DIR__);\n",
    "Pkg.instantiate();"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Julia version** is assumed to be 1.10.*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Basic Imports"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ               # Has MLJFlux models\n",
    "using Flux              # For more flexibility\n",
    "import RDatasets        # Dataset source\n",
    "import Random           # To create imbalance\n",
    "import Imbalance        # To solve the imbalance\n",
    "import Optimisers       # native Flux.jl optimisers no longer supported"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading and Splitting the Data"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "iris = RDatasets.dataset(\"datasets\", \"iris\");\n",
    "y, X = unpack(iris, ==(:Species), rng=123);\n",
    "X = Float32.(X);      # To be compatible with type of network network parameters"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To simulate an imbalanced dataset, we will take a random sample:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Random.seed!(803429)\n",
    "subset_indices = rand(1:size(X, 1), 100)\n",
    "X, y = X[subset_indices, :], y[subset_indices]\n",
    "Imbalance.checkbalance(y)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Instantiating the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's load `BorderlineSMOTE1` to oversample the data and `Standardizer` to standardize\n",
    "it."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "BorderlineSMOTE1 = @load BorderlineSMOTE1 pkg=Imbalance verbosity=0\n",
    "NeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We didn't need to load Standardizer because it is a local model for MLJ (see\n",
    "`localmodels()`)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "clf = NeuralNetworkClassifier(\n",
    "    builder=MLJFlux.MLP(; hidden=(5,4), Ïƒ=Flux.relu),\n",
    "    optimiser=Optimisers.Adam(0.01),\n",
    "    batch_size=8,\n",
    "    epochs=50,\n",
    "    rng=42,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we wrap the oversampler with the neural network via the `BalancedModel`\n",
    "construct. This comes from `MLJBalancing` And allows combining resampling methods with\n",
    "MLJ models in a sequential pipeline."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "oversampler = BorderlineSMOTE1(k=5, ratios=1.0, rng=42)\n",
    "balanced_model = BalancedModel(model=clf, balancer1=oversampler)\n",
    "standarizer = Standardizer()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's compose the balanced model with a standardizer."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "pipeline = standarizer |> balanced_model"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "By this, any training data will be standardized then oversampled then passed to the\n",
    "model. Meanwhile, for inference, the standardizer will automatically use the training\n",
    "set's mean and std and the oversampler will be transparent."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the Composed Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's indistinguishable from training a single model."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mach = machine(pipeline, X, y)\n",
    "fit!(mach)\n",
    "cv=CV(nfolds=5)\n",
    "evaluate!(mach, resampling=cv, measure=accuracy)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
