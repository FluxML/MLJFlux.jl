<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>MNIST Images · MLJFlux</title><meta name="title" content="MNIST Images · MLJFlux"/><meta property="og:title" content="MNIST Images · MLJFlux"/><meta property="twitter:title" content="MNIST Images · MLJFlux"/><meta name="description" content="Documentation for MLJFlux."/><meta property="og:description" content="Documentation for MLJFlux."/><meta property="twitter:description" content="Documentation for MLJFlux."/><script data-outdated-warner src="../../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../../assets/documenter.js"></script><script src="../../../search_index.js"></script><script src="../../../siteinfo.js"></script><script src="../../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../../assets/themeswap.js"></script><link href="../../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,100;0,300;0,400;0,700;0,900;1,100;1,300;1,400;1,700;1,900&amp;family=Montserrat:ital,wght@0,100..900;1,100..900&amp;display=swap" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../../"><img src="../../../assets/logo.gif" alt="MLJFlux logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../../">MLJFlux</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../../">Introduction</a></li><li><input class="collapse-toggle" id="menuitem-2" type="checkbox"/><label class="tocitem" for="menuitem-2"><span class="docs-label">Interface</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../../interface/Summary/">Summary</a></li><li><a class="tocitem" href="../../../interface/Builders/">Builders</a></li><li><a class="tocitem" href="../../../interface/Custom Builders/">Custom Builders</a></li><li><a class="tocitem" href="../../../interface/Classification/">Classification</a></li><li><a class="tocitem" href="../../../interface/Regression/">Regression</a></li><li><a class="tocitem" href="../../../interface/Multitarget Regression/">Multi-Target Regression</a></li><li><a class="tocitem" href="../../../interface/Image Classification/">Image Classification</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Common Workflows</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../../common_workflows/incremental_training/notebook/">Incremental Training</a></li><li><a class="tocitem" href="../../../common_workflows/hyperparameter_tuning/notebook/">Hyperparameter Tuning</a></li><li><a class="tocitem" href="../../../common_workflows/composition/notebook/">Model Composition</a></li><li><a class="tocitem" href="../../../common_workflows/comparison/notebook/">Model Comparison</a></li><li><a class="tocitem" href="../../../common_workflows/early_stopping/notebook/">Early Stopping</a></li><li><a class="tocitem" href="../../../common_workflows/live_training/notebook/">Live Training</a></li><li><a class="tocitem" href="../../../common_workflows/architecture_search/notebook/">Neural Architecture Search</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox" checked/><label class="tocitem" for="menuitem-4"><span class="docs-label">Extended Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>MNIST Images</a><ul class="internal"><li><a class="tocitem" href="#Basic-training"><span>Basic training</span></a></li><li><a class="tocitem" href="#Wrapping-the-MLJFlux-model-with-iteration-controls"><span>Wrapping the MLJFlux model with iteration controls</span></a></li></ul></li><li><a class="tocitem" href="../../spam_detection/notebook/">Spam Detection with RNNs</a></li></ul></li><li><a class="tocitem" href="../../../contributing/">Contributing</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Extended Examples</a></li><li class="is-active"><a href>MNIST Images</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>MNIST Images</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/FluxML/MLJFlux.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/FluxML/MLJFlux.jl/blob/dev/docs/src/extended_examples/MNIST/notebook.jl" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Using-MLJ-to-classifiy-the-MNIST-image-dataset"><a class="docs-heading-anchor" href="#Using-MLJ-to-classifiy-the-MNIST-image-dataset">Using MLJ to classifiy the MNIST image dataset</a><a id="Using-MLJ-to-classifiy-the-MNIST-image-dataset-1"></a><a class="docs-heading-anchor-permalink" href="#Using-MLJ-to-classifiy-the-MNIST-image-dataset" title="Permalink"></a></h1><p>This tutorial is available as a Jupyter notebook or julia script <a href="https://github.com/FluxML/MLJFlux.jl/tree/dev/docs/src/extended_examples/MNIST">here</a>.</p><p><strong>Julia version</strong> is assumed to be 1.10.*</p><pre><code class="language-julia hljs">using MLJ
using Flux
import MLJFlux
import MLUtils
import MLJIteration # for `skip`</code></pre><p>If running on a GPU, you will also need to <code>import CUDA</code> and <code>import cuDNN</code>.</p><pre><code class="language-julia hljs">using Plots
gr(size=(600, 300*(sqrt(5)-1)));</code></pre><h2 id="Basic-training"><a class="docs-heading-anchor" href="#Basic-training">Basic training</a><a id="Basic-training-1"></a><a class="docs-heading-anchor-permalink" href="#Basic-training" title="Permalink"></a></h2><p>Downloading the MNIST image dataset:</p><pre><code class="language-julia hljs">import MLDatasets: MNIST

ENV[&quot;DATADEPS_ALWAYS_ACCEPT&quot;] = true
images, labels = MNIST(split=:train)[:];</code></pre><p>In MLJ, integers cannot be used for encoding categorical data, so we must force the labels to have the <code>Multiclass</code> <a href="https://juliaai.github.io/ScientificTypes.jl/dev/">scientific type</a>. For more on this, see <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/working_with_categorical_data/">Working with Categorical Data</a>.</p><pre><code class="language-julia hljs">labels = coerce(labels, Multiclass);
images = coerce(images, GrayImage);</code></pre><p>Checking scientific types:</p><pre><code class="language-julia hljs">@assert scitype(images) &lt;: AbstractVector{&lt;:Image}
@assert scitype(labels) &lt;: AbstractVector{&lt;:Finite}</code></pre><p>Looks good.</p><p>For general instructions on coercing image data, see <a href="https://juliaai.github.io/ScientificTypes.jl/dev/#Type-coercion-for-image-data">Type coercion for image data</a></p><pre><code class="language-julia hljs">images[1]</code></pre><img src="c099548a.svg" alt="Example block output"/><p>We start by defining a suitable <code>Builder</code> object. This is a recipe for building the neural network. Our builder will work for images of any (constant) size, whether they be color or black and white (ie, single or multi-channel).  The architecture always consists of six alternating convolution and max-pool layers, and a final dense layer; the filter size and the number of channels after each convolution layer is customisable.</p><pre><code class="language-julia hljs">import MLJFlux
struct MyConvBuilder
    filter_size::Int
    channels1::Int
    channels2::Int
    channels3::Int
end

function MLJFlux.build(b::MyConvBuilder, rng, n_in, n_out, n_channels)
    k, c1, c2, c3 = b.filter_size, b.channels1, b.channels2, b.channels3
    mod(k, 2) == 1 || error(&quot;`filter_size` must be odd. &quot;)
    p = div(k - 1, 2) # padding to preserve image size
    init = Flux.glorot_uniform(rng)
    front = Chain(
        Conv((k, k), n_channels =&gt; c1, pad=(p, p), relu, init=init),
        MaxPool((2, 2)),
        Conv((k, k), c1 =&gt; c2, pad=(p, p), relu, init=init),
        MaxPool((2, 2)),
        Conv((k, k), c2 =&gt; c3, pad=(p, p), relu, init=init),
        MaxPool((2 ,2)),
        MLUtils.flatten)
    d = Flux.outputsize(front, (n_in..., n_channels, 1)) |&gt; first
    return Chain(front, Dense(d, n_out, init=init))
end</code></pre><p><strong>Notes.</strong></p><ul><li><p>There is no final <code>softmax</code> here, as this is applied by default in all MLJFLux classifiers. Customisation of this behaviour is controlled using using the <code>finaliser</code> hyperparameter of the classifier.</p></li><li><p>Instead of calculating the padding <code>p</code>, Flux can infer the required padding in each dimension, which you enable by replacing <code>pad = (p, p)</code> with <code>pad = SamePad()</code>.</p></li></ul><p>We now define the MLJ model.</p><pre><code class="language-julia hljs">ImageClassifier = @load ImageClassifier
clf = ImageClassifier(
    builder=MyConvBuilder(3, 16, 32, 32),
    batch_size=50,
    epochs=10,
    rng=123,
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">ImageClassifier(
  builder = Main.MyConvBuilder(3, 16, 32, 32), 
  finaliser = NNlib.softmax, 
  optimiser = Adam(0.001, (0.9, 0.999), 1.0e-8), 
  loss = Flux.Losses.crossentropy, 
  epochs = 10, 
  batch_size = 50, 
  lambda = 0.0, 
  alpha = 0.0, 
  rng = 123, 
  optimiser_changes_trigger_retraining = false, 
  acceleration = CPU1{Nothing}(nothing))</code></pre><p>You can add Flux options <code>optimiser=...</code> and <code>loss=...</code> in the above constructor call. At present, <code>loss</code> must be a Flux-compatible loss, not an MLJ measure. To run on a GPU, add to the constructor <code>acceleration=CUDALib()</code> and omit <code>rng</code>.</p><p>For illustration purposes, we won&#39;t use all the data here:</p><pre><code class="language-julia hljs">train = 1:500
test = 501:1000</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">501:1000</code></pre><p>Binding the model with data in an MLJ machine:</p><pre><code class="language-julia hljs">mach = machine(clf, images, labels);</code></pre><p>Training for 10 epochs on the first 500 images:</p><pre><code class="language-julia hljs">fit!(mach, rows=train, verbosity=2);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">[ Info: Training machine(ImageClassifier(builder = Main.MyConvBuilder(3, 16, 32, 32), …), …).
[ Info: Loss is 2.28
[ Info: Loss is 2.171
[ Info: Loss is 1.942
[ Info: Loss is 1.505
[ Info: Loss is 0.9922
[ Info: Loss is 0.6912
[ Info: Loss is 0.5584
[ Info: Loss is 0.4542
[ Info: Loss is 0.3809
[ Info: Loss is 0.3272</code></pre><p>Inspecting:</p><pre><code class="language-julia hljs">report(mach)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(training_losses = Float32[2.3174262, 2.280439, 2.1711705, 1.9420795, 1.5045885, 0.99224484, 0.69117606, 0.5583703, 0.45424515, 0.38085267, 0.3271538],)</code></pre><pre><code class="language-julia hljs">chain = fitted_params(mach)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(chain = Chain(Chain(Chain(Conv((3, 3), 1 =&gt; 16, relu, pad=1), MaxPool((2, 2)), Conv((3, 3), 16 =&gt; 32, relu, pad=1), MaxPool((2, 2)), Conv((3, 3), 32 =&gt; 32, relu, pad=1), MaxPool((2, 2)), flatten), Dense(288 =&gt; 10)), softmax),)</code></pre><pre><code class="language-julia hljs">Flux.params(chain)[2]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">16-element Vector{Float32}:
 0.003225543
 0.019304937
 0.062040687
 0.024518687
 0.05317823
 0.069572166
 0.044410173
 0.024950704
 0.015806748
 0.015081032
 0.017513964
 0.02133927
 0.040562775
 0.0018777152
 0.055122323
 0.057923194</code></pre><p>Adding 20 more epochs:</p><pre><code class="language-julia hljs">clf.epochs = clf.epochs + 20
fit!(mach, rows=train);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">[ Info: Updating machine(ImageClassifier(builder = Main.MyConvBuilder(3, 16, 32, 32), …), …).
Optimising neural net:  10%[==&gt;                      ]  ETA: 0:00:07Optimising neural net:  14%[===&gt;                     ]  ETA: 0:00:08Optimising neural net:  19%[====&gt;                    ]  ETA: 0:00:08Optimising neural net:  24%[=====&gt;                   ]  ETA: 0:00:08Optimising neural net:  29%[=======&gt;                 ]  ETA: 0:00:07Optimising neural net:  33%[========&gt;                ]  ETA: 0:00:07Optimising neural net:  38%[=========&gt;               ]  ETA: 0:00:06Optimising neural net:  43%[==========&gt;              ]  ETA: 0:00:06Optimising neural net:  48%[===========&gt;             ]  ETA: 0:00:05Optimising neural net:  52%[=============&gt;           ]  ETA: 0:00:05Optimising neural net:  57%[==============&gt;          ]  ETA: 0:00:05Optimising neural net:  62%[===============&gt;         ]  ETA: 0:00:04Optimising neural net:  67%[================&gt;        ]  ETA: 0:00:04Optimising neural net:  71%[=================&gt;       ]  ETA: 0:00:03Optimising neural net:  76%[===================&gt;     ]  ETA: 0:00:03Optimising neural net:  81%[====================&gt;    ]  ETA: 0:00:02Optimising neural net:  86%[=====================&gt;   ]  ETA: 0:00:02Optimising neural net:  90%[======================&gt;  ]  ETA: 0:00:01Optimising neural net:  95%[=======================&gt; ]  ETA: 0:00:01Optimising neural net: 100%[=========================] Time: 0:00:10</code></pre><p>Computing an out-of-sample estimate of the loss:</p><pre><code class="language-julia hljs">predicted_labels = predict(mach, rows=test);
cross_entropy(predicted_labels, labels[test])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.4883231265583621</code></pre><p>Or to fit and predict, in one line:</p><pre><code class="language-julia hljs">evaluate!(mach,
          resampling=Holdout(fraction_train=0.5),
          measure=cross_entropy,
          rows=1:1000,
          verbosity=0)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">PerformanceEvaluation object with these fields:
  model, measure, operation,
  measurement, per_fold, per_observation,
  fitted_params_per_fold, report_per_fold,
  train_test_rows, resampling, repeats
Extract:
┌──────────────────────┬───────────┬─────────────┐
│ measure              │ operation │ measurement │
├──────────────────────┼───────────┼─────────────┤
│ LogLoss(             │ predict   │ 0.488       │
│   tol = 2.22045e-16) │           │             │
└──────────────────────┴───────────┴─────────────┘
</code></pre><h2 id="Wrapping-the-MLJFlux-model-with-iteration-controls"><a class="docs-heading-anchor" href="#Wrapping-the-MLJFlux-model-with-iteration-controls">Wrapping the MLJFlux model with iteration controls</a><a id="Wrapping-the-MLJFlux-model-with-iteration-controls-1"></a><a class="docs-heading-anchor-permalink" href="#Wrapping-the-MLJFlux-model-with-iteration-controls" title="Permalink"></a></h2><p>Any iterative MLJFlux model can be wrapped in <em>iteration controls</em>, as we demonstrate next. For more on MLJ&#39;s <code>IteratedModel</code> wrapper, see the <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/controlling_iterative_models/">MLJ documentation</a>.</p><p>The &quot;self-iterating&quot; classifier, called <code>iterated_clf</code> below, is for iterating the image classifier defined above until one of the following stopping criterion apply:</p><ul><li><code>Patience(3)</code>: 3 consecutive increases in the loss</li><li><code>InvalidValue()</code>: an out-of-sample loss, or a training loss, is <code>NaN</code>, <code>Inf</code>, or <code>-Inf</code></li><li><code>TimeLimit(t=5/60)</code>: training time has exceeded 5 minutes</li></ul><p>These checks (and other controls) will be applied every two epochs (because of the <code>Step(2)</code> control). Additionally, training a machine bound to <code>iterated_clf</code> will:</p><ul><li>save a snapshot of the machine every three control cycles (every six epochs)</li><li>record traces of the out-of-sample loss and training losses for plotting</li><li>record mean value traces of each Flux parameter for plotting</li></ul><p>For a complete list of controls, see <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/controlling_iterative_models/#Controls-provided">this table</a>.</p><h3 id="Wrapping-the-classifier"><a class="docs-heading-anchor" href="#Wrapping-the-classifier">Wrapping the classifier</a><a id="Wrapping-the-classifier-1"></a><a class="docs-heading-anchor-permalink" href="#Wrapping-the-classifier" title="Permalink"></a></h3><p>Some helpers</p><p>To extract Flux params from an MLJFlux machine</p><pre><code class="language-julia hljs">parameters(mach) = vec.(Flux.params(fitted_params(mach)));</code></pre><p>To store the traces:</p><pre><code class="language-julia hljs">losses = []
training_losses = []
parameter_means = Float32[];
epochs = []</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Any[]</code></pre><p>To update the traces:</p><pre><code class="language-julia hljs">update_loss(loss) = push!(losses, loss)
update_training_loss(losses) = push!(training_losses, losses[end])
update_means(mach) = append!(parameter_means, mean.(parameters(mach)));
update_epochs(epoch) = push!(epochs, epoch)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">update_epochs (generic function with 1 method)</code></pre><p>The controls to apply:</p><pre><code class="language-julia hljs">save_control =
    MLJIteration.skip(Save(joinpath(tempdir(), &quot;mnist.jls&quot;)), predicate=3)

controls=[
    Step(2),
    Patience(3),
    InvalidValue(),
    TimeLimit(5/60),
    save_control,
    WithLossDo(),
    WithLossDo(update_loss),
    WithTrainingLossesDo(update_training_loss),
    Callback(update_means),
    WithIterationsDo(update_epochs),
];</code></pre><p>The &quot;self-iterating&quot; classifier:</p><pre><code class="language-julia hljs">iterated_clf = IteratedModel(
    clf,
    controls=controls,
    resampling=Holdout(fraction_train=0.7),
    measure=log_loss,
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">ProbabilisticIteratedModel(
  model = ImageClassifier(
        builder = Main.MyConvBuilder(3, 16, 32, 32), 
        finaliser = NNlib.softmax, 
        optimiser = Adam(0.001, (0.9, 0.999), 1.0e-8), 
        loss = Flux.Losses.crossentropy, 
        epochs = 30, 
        batch_size = 50, 
        lambda = 0.0, 
        alpha = 0.0, 
        rng = 123, 
        optimiser_changes_trigger_retraining = false, 
        acceleration = CPU1{Nothing}(nothing)), 
  controls = Any[IterationControl.Step(2), EarlyStopping.Patience(3), EarlyStopping.InvalidValue(), EarlyStopping.TimeLimit(Dates.Millisecond(300000)), IterationControl.Skip{MLJIteration.Save{typeof(Serialization.serialize)}, IterationControl.var&quot;#8#9&quot;{Int64}}(MLJIteration.Save{typeof(Serialization.serialize)}(&quot;/tmp/mnist.jls&quot;, Serialization.serialize), IterationControl.var&quot;#8#9&quot;{Int64}(3)), IterationControl.WithLossDo{IterationControl.var&quot;#20#22&quot;}(IterationControl.var&quot;#20#22&quot;(), false, nothing), IterationControl.WithLossDo{typeof(Main.update_loss)}(Main.update_loss, false, nothing), IterationControl.WithTrainingLossesDo{typeof(Main.update_training_loss)}(Main.update_training_loss, false, nothing), IterationControl.Callback{typeof(Main.update_means)}(Main.update_means, false, nothing, false), MLJIteration.WithIterationsDo{typeof(Main.update_epochs)}(Main.update_epochs, false, nothing)], 
  resampling = Holdout(
        fraction_train = 0.7, 
        shuffle = false, 
        rng = Random._GLOBAL_RNG()), 
  measure = LogLoss(tol = 2.22045e-16), 
  weights = nothing, 
  class_weights = nothing, 
  operation = nothing, 
  retrain = false, 
  check_measure = true, 
  iteration_parameter = nothing, 
  cache = true)</code></pre><h3 id="Binding-the-wrapped-model-to-data:"><a class="docs-heading-anchor" href="#Binding-the-wrapped-model-to-data:">Binding the wrapped model to data:</a><a id="Binding-the-wrapped-model-to-data:-1"></a><a class="docs-heading-anchor-permalink" href="#Binding-the-wrapped-model-to-data:" title="Permalink"></a></h3><pre><code class="language-julia hljs">mach = machine(iterated_clf, images, labels);</code></pre><h3 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h3><pre><code class="language-julia hljs">fit!(mach, rows=train);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">[ Info: Training machine(ProbabilisticIteratedModel(model = ImageClassifier(builder = Main.MyConvBuilder(3, 16, 32, 32), …), …), …).
[ Info: No iteration parameter specified. Using `iteration_parameter=:(epochs)`.
[ Info: loss: 2.195050130190149
[ Info: loss: 1.8450074691283658
[ Info: Saving &quot;/tmp/mnist1.jls&quot;.
[ Info: loss: 1.1388123685158849
[ Info: loss: 0.702997545486733
[ Info: loss: 0.5778269559910739
[ Info: Saving &quot;/tmp/mnist2.jls&quot;.
[ Info: loss: 0.5222495075757826
[ Info: loss: 0.49847208228951995
[ Info: loss: 0.4897800580510804
[ Info: Saving &quot;/tmp/mnist3.jls&quot;.
[ Info: loss: 0.4893840844808948
[ Info: loss: 0.49094569068535143
[ Info: loss: 0.49593260647952264
[ Info: Saving &quot;/tmp/mnist4.jls&quot;.
[ Info: loss: 0.5062357308150314
[ Info: final loss: 0.5062357308150314
[ Info: final training loss: 0.059303638
[ Info: Stop triggered by EarlyStopping.Patience(3) stopping criterion.
[ Info: Total of 24 iterations.</code></pre><h3 id="Comparison-of-the-training-and-out-of-sample-losses:"><a class="docs-heading-anchor" href="#Comparison-of-the-training-and-out-of-sample-losses:">Comparison of the training and out-of-sample losses:</a><a id="Comparison-of-the-training-and-out-of-sample-losses:-1"></a><a class="docs-heading-anchor-permalink" href="#Comparison-of-the-training-and-out-of-sample-losses:" title="Permalink"></a></h3><pre><code class="language-julia hljs">plot(
    epochs,
    losses,
    xlab = &quot;epoch&quot;,
    ylab = &quot;cross entropy&quot;,
    label=&quot;out-of-sample&quot;,
)
plot!(epochs, training_losses, label=&quot;training&quot;)

savefig(joinpath(tempdir(), &quot;loss.png&quot;))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">&quot;/tmp/loss.png&quot;</code></pre><h3 id="Evolution-of-weights"><a class="docs-heading-anchor" href="#Evolution-of-weights">Evolution of weights</a><a id="Evolution-of-weights-1"></a><a class="docs-heading-anchor-permalink" href="#Evolution-of-weights" title="Permalink"></a></h3><pre><code class="language-julia hljs">n_epochs =  length(losses)
n_parameters = div(length(parameter_means), n_epochs)
parameter_means2 = reshape(copy(parameter_means), n_parameters, n_epochs)&#39;
plot(
    epochs,
    parameter_means2,
    title=&quot;Flux parameter mean weights&quot;,
    xlab = &quot;epoch&quot;,
)</code></pre><img src="dc63cda3.svg" alt="Example block output"/><p><strong>Note.</strong> The higher the number in the plot legend, the deeper the layer we are **weight-averaging.</p><pre><code class="language-julia hljs">savefig(joinpath(tempdir(), &quot;weights.png&quot;))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">&quot;/tmp/weights.png&quot;</code></pre><h3 id="Retrieving-a-snapshot-for-a-prediction:"><a class="docs-heading-anchor" href="#Retrieving-a-snapshot-for-a-prediction:">Retrieving a snapshot for a prediction:</a><a id="Retrieving-a-snapshot-for-a-prediction:-1"></a><a class="docs-heading-anchor-permalink" href="#Retrieving-a-snapshot-for-a-prediction:" title="Permalink"></a></h3><pre><code class="language-julia hljs">mach2 = machine(joinpath(tempdir(), &quot;mnist3.jls&quot;))
predict_mode(mach2, images[501:503])</code></pre><pre><code class="nohighlight hljs">3-element CategoricalArrays.CategoricalArray{Int64,1,UInt32}:
 7
 9
 5</code></pre><h3 id="Restarting-training"><a class="docs-heading-anchor" href="#Restarting-training">Restarting training</a><a id="Restarting-training-1"></a><a class="docs-heading-anchor-permalink" href="#Restarting-training" title="Permalink"></a></h3><p>Mutating <code>iterated_clf.controls</code> or <code>clf.epochs</code> (which is otherwise ignored) will allow you to restart training from where it left off.</p><pre><code class="language-julia hljs">iterated_clf.controls[2] = Patience(4)
fit!(mach, rows=train)

plot(
    epochs,
    losses,
    xlab = &quot;epoch&quot;,
    ylab = &quot;cross entropy&quot;,
    label=&quot;out-of-sample&quot;,
)
plot!(epochs, training_losses, label=&quot;training&quot;)</code></pre><img src="ef67068c.svg" alt="Example block output"/><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../../common_workflows/architecture_search/notebook/">« Neural Architecture Search</a><a class="docs-footer-nextpage" href="../../spam_detection/notebook/">Spam Detection with RNNs »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Wednesday 13 November 2024 20:06">Wednesday 13 November 2024</span>. Using Julia version 1.10.6.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
