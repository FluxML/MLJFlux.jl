<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Classification · MLJFlux</title><meta name="title" content="Classification · MLJFlux"/><meta property="og:title" content="Classification · MLJFlux"/><meta property="twitter:title" content="Classification · MLJFlux"/><meta name="description" content="Documentation for MLJFlux."/><meta property="og:description" content="Documentation for MLJFlux."/><meta property="twitter:description" content="Documentation for MLJFlux."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,100;0,300;0,400;0,700;0,900;1,100;1,300;1,400;1,700;1,900&amp;family=Montserrat:ital,wght@0,100..900;1,100..900&amp;display=swap" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.gif" alt="MLJFlux logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">MLJFlux</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Introduction</a></li><li><input class="collapse-toggle" id="menuitem-2" type="checkbox" checked/><label class="tocitem" for="menuitem-2"><span class="docs-label">Interface</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Summary/">Summary</a></li><li><a class="tocitem" href="../Builders/">Builders</a></li><li><a class="tocitem" href="../Custom Builders/">Custom Builders</a></li><li class="is-active"><a class="tocitem" href>Classification</a></li><li><a class="tocitem" href="../Regression/">Regression</a></li><li><a class="tocitem" href="../Multitarget Regression/">Multi-Target Regression</a></li><li><a class="tocitem" href="../Image Classification/">Image Classification</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Workflow Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../workflow examples/Incremental Training/incremental/">Incremental Training</a></li><li><a class="tocitem" href="../../workflow examples/Hyperparameter Tuning/tuning/">Hyperparameter Tuning</a></li><li><a class="tocitem" href="../../workflow examples/Basic Neural Architecture Search/tuning/">Neural Architecture Search</a></li><li><a class="tocitem" href="../../workflow examples/Composition/composition/">Model Composition</a></li><li><a class="tocitem" href="../../workflow examples/Comparison/comparison/">Model Comparison</a></li><li><a class="tocitem" href="../../workflow examples/Early Stopping/iteration/">Early Stopping</a></li><li><a class="tocitem" href="../../workflow examples/Live Training/live-training/">Live Training</a></li></ul></li><li><a class="tocitem" href="../../contributing/">Contributing</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Interface</a></li><li class="is-active"><a href>Classification</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Classification</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/FluxML/MLJFlux.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/FluxML/MLJFlux.jl/blob/dev/docs/src/interface/Classification.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJFlux.NeuralNetworkClassifier" href="#MLJFlux.NeuralNetworkClassifier"><code>MLJFlux.NeuralNetworkClassifier</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">NeuralNetworkClassifier</code></pre><p>A model type for constructing a neural network classifier, based on <a href="https://github.com/alan-turing-institute/MLJFlux.jl">MLJFlux.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">NeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux</code></pre><p>Do <code>model = NeuralNetworkClassifier()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>NeuralNetworkClassifier(builder=...)</code>.</p><p><code>NeuralNetworkClassifier</code> is for training a data-dependent Flux.jl neural network for making probabilistic predictions of a <code>Multiclass</code> or <code>OrderedFactor</code> target, given a table of <code>Continuous</code> features. Users provide a recipe for constructing  the network, based on properties of the data that is encountered, by specifying  an appropriate <code>builder</code>. See MLJFlux documentation for more on builders.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, X, y)</code></pre><p>Here:</p><ul><li><p><code>X</code> is either a <code>Matrix</code> or any table of input features (eg, a <code>DataFrame</code>) whose columns are of scitype <code>Continuous</code>; check column scitypes with <code>schema(X)</code>. If <code>X</code> is a <code>Matrix</code>, it is assumed to have columns corresponding to features and rows corresponding to observations.</p></li><li><p><code>y</code> is the target, which can be any <code>AbstractVector</code> whose element scitype is <code>Multiclass</code> or <code>OrderedFactor</code>; check the scitype with <code>scitype(y)</code></p></li></ul><p>Train the machine with <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>builder=MLJFlux.Short()</code>: An MLJFlux builder that constructs a neural network. Possible  <code>builders</code> include: <code>MLJFlux.Linear</code>, <code>MLJFlux.Short</code>, and <code>MLJFlux.MLP</code>. See  MLJFlux.jl documentation for examples of user-defined builders. See also <code>finaliser</code>  below.</p></li><li><p><code>optimiser::Optimisers.Adam()</code>: An Optimisers.jl optimiser. The optimiser performs the updating of the weights of the network. To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at <code>10e-3</code>, and tune using powers of 10 between <code>1</code> and <code>1e-7</code>.</p></li><li><p><code>loss=Flux.crossentropy</code>: The loss function which the network will optimize. Should be a function which can be called in the form <code>loss(yhat, y)</code>.  Possible loss functions are listed in <a href="https://fluxml.ai/Flux.jl/stable/models/losses/">the Flux loss function documentation</a>. For a classification task, the most natural loss functions are:</p><ul><li><p><code>Flux.crossentropy</code>: Standard multiclass classification loss, also known as the log loss.</p></li><li><p><code>Flux.logitcrossentopy</code>: Mathematically equal to crossentropy, but numerically more stable than finalising the outputs with <code>softmax</code> and then calculating crossentropy. You will need to specify <code>finaliser=identity</code> to remove MLJFlux&#39;s default softmax finaliser, and understand that the output of <code>predict</code> is then unnormalized (no longer probabilistic).</p></li><li><p><code>Flux.tversky_loss</code>: Used with imbalanced data to give more weight to false negatives.</p></li><li><p><code>Flux.focal_loss</code>: Used with highly imbalanced data. Weights harder examples more than easier examples.</p></li></ul><p>Currently MLJ measures are not supported values of <code>loss</code>.</p></li><li><p><code>epochs::Int=10</code>: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.</p></li><li><p><code>batch_size::int=1</code>: the batch size to be used for training, representing the number of samples per update of the network weights. Typically, batch size is between 8 and</p><ol><li>Increassing batch size may accelerate training if <code>acceleration=CUDALibs()</code> and a</li></ol><p>GPU is available.</p></li><li><p><code>lambda::Float64=0</code>: The strength of the weight regularization penalty. Can be any value in the range <code>[0, ∞)</code>. Note the history reports unpenalized losses.</p></li><li><p><code>alpha::Float64=0</code>: The L2/L1 mix of regularization, in the range <code>[0, 1]</code>. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.</p></li><li><p><code>rng::Union{AbstractRNG, Int64}</code>: The random number generator or seed used during training. The default is <code>Random.default_rng()</code>.</p></li><li><p><code>optimizer_changes_trigger_retraining::Bool=false</code>: Defines what happens when re-fitting a machine if the associated optimiser has changed. If <code>true</code>, the associated machine will retrain from scratch on <code>fit!</code> call, otherwise it will not.</p></li><li><p><code>acceleration::AbstractResource=CPU1()</code>: Defines on what hardware training is done. For Training on GPU, use <code>CUDALibs()</code>.</p></li><li><p><code>finaliser=Flux.softmax</code>: The final activation function of the neural network (applied after the network defined by <code>builder</code>). Defaults to <code>Flux.softmax</code>.</p></li></ul><p><strong>Operations</strong></p><ul><li><p><code>predict(mach, Xnew)</code>: return predictions of the target given new features <code>Xnew</code>, which should have the same scitype as <code>X</code> above. Predictions are probabilistic but uncalibrated.</p></li><li><p><code>predict_mode(mach, Xnew)</code>: Return the modes of the probabilistic predictions returned above.</p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>chain</code>: The trained &quot;chain&quot; (Flux.jl model), namely the series of layers,  functions, and activations which make up the neural network. This includes  the final layer specified by <code>finaliser</code> (eg, <code>softmax</code>).</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><code>training_losses</code>: A vector of training losses (penalised if <code>lambda != 0</code>) in  historical order, of length <code>epochs + 1</code>.  The first element is the pre-training loss.</li></ul><p><strong>Examples</strong></p><p>In this example we build a classification model using the Iris dataset. This is a very basic example, using a default builder and no standardization.  For a more advanced illustration, see <a href="../Regression/#MLJFlux.NeuralNetworkRegressor"><code>NeuralNetworkRegressor</code></a> or <a href="../Image Classification/#MLJFlux.ImageClassifier"><code>ImageClassifier</code></a>, and examples in the MLJFlux.jl documentation.</p><pre><code class="language-julia hljs">using MLJ
using Flux
import RDatasets</code></pre><p>First, we can load the data:</p><pre><code class="language-julia hljs">iris = RDatasets.dataset(&quot;datasets&quot;, &quot;iris&quot;);
y, X = unpack(iris, ==(:Species), rng=123); # a vector and a table
NeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux
clf = NeuralNetworkClassifier()</code></pre><p>Next, we can train the model:</p><pre><code class="language-julia hljs">mach = machine(clf, X, y)
fit!(mach)</code></pre><p>We can train the model in an incremental fashion, altering the learning rate as we go, provided <code>optimizer_changes_trigger_retraining</code> is <code>false</code> (the default). Here, we also change the number of (total) iterations:</p><pre><code class="language-julia hljs">clf.optimiser.eta = clf.optimiser.eta * 2
clf.epochs = clf.epochs + 5

fit!(mach, verbosity=2) # trains 5 more epochs</code></pre><p>We can inspect the mean training loss using the <code>cross_entropy</code> function:</p><pre><code class="language-julia hljs">training_loss = cross_entropy(predict(mach, X), y)</code></pre><p>And we can access the Flux chain (model) using <code>fitted_params</code>:</p><pre><code class="language-julia hljs">chain = fitted_params(mach).chain</code></pre><p>Finally, we can see how the out-of-sample performance changes over time, using MLJ&#39;s <code>learning_curve</code> function:</p><pre><code class="language-julia hljs">r = range(clf, :epochs, lower=1, upper=200, scale=:log10)
curve = learning_curve(clf, X, y,
                     range=r,
                     resampling=Holdout(fraction_train=0.7),
                     measure=cross_entropy)
using Plots
plot(curve.parameter_values,
     curve.measurements,
     xlab=curve.parameter_name,
     xscale=curve.parameter_scale,
     ylab = &quot;Cross Entropy&quot;)
</code></pre><p>See also <a href="../Image Classification/#MLJFlux.ImageClassifier"><code>ImageClassifier</code></a>, <a href="#MLJFlux.NeuralNetworkBinaryClassifier"><code>NeuralNetworkBinaryClassifier</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/MLJFlux.jl/blob/088e152b38c4b599c49c04a18de94aaa8a1f886f/src/types.jl#L142-L157">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJFlux.NeuralNetworkBinaryClassifier" href="#MLJFlux.NeuralNetworkBinaryClassifier"><code>MLJFlux.NeuralNetworkBinaryClassifier</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">NeuralNetworkBinaryClassifier</code></pre><p>A model type for constructing a neural network binary classifier, based on <a href="unknown">unknown.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">NeuralNetworkBinaryClassifier = @load NeuralNetworkBinaryClassifier pkg=unknown</code></pre><p>Do <code>model = NeuralNetworkBinaryClassifier()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>NeuralNetworkBinaryClassifier(builder=...)</code>.</p><p><code>NeuralNetworkBinaryClassifier</code> is for training a data-dependent Flux.jl neural network for making probabilistic predictions of a binary (<code>Multiclass{2}</code> or <code>OrderedFactor{2}</code>) target, given a table of <code>Continuous</code> features. Users provide a recipe for constructing  the network, based on properties of the data that is encountered, by specifying  an appropriate <code>builder</code>. See MLJFlux documentation for more on builders.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, X, y)</code></pre><p>Here:</p><ul><li><p><code>X</code> is either a <code>Matrix</code> or any table of input features (eg, a <code>DataFrame</code>) whose columns are of scitype <code>Continuous</code>; check column scitypes with <code>schema(X)</code>. If <code>X</code> is a <code>Matrix</code>, it is assumed to have columns corresponding to features and rows corresponding to observations.</p></li><li><p><code>y</code> is the target, which can be any <code>AbstractVector</code> whose element scitype is <code>Multiclass{2}</code> or <code>OrderedFactor{2}</code>; check the scitype with <code>scitype(y)</code></p></li></ul><p>Train the machine with <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>builder=MLJFlux.Short()</code>: An MLJFlux builder that constructs a neural network. Possible  <code>builders</code> include: <code>MLJFlux.Linear</code>, <code>MLJFlux.Short</code>, and <code>MLJFlux.MLP</code>. See  MLJFlux.jl documentation for examples of user-defined builders. See also <code>finaliser</code>  below.</p></li><li><p><code>optimiser::Flux.Adam()</code>: A <code>Flux.Optimise</code> optimiser. The optimiser performs the updating of the weights of the network. For further reference, see <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/">the Flux optimiser documentation</a>. To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at <code>10e-3</code>, and tune using powers of 10 between <code>1</code> and <code>1e-7</code>.</p></li><li><p><code>loss=Flux.binarycrossentropy</code>: The loss function which the network will optimize. Should be a function which can be called in the form <code>loss(yhat, y)</code>.  Possible loss functions are listed in <a href="https://fluxml.ai/Flux.jl/stable/models/losses/">the Flux loss function documentation</a>. For a classification task, the most natural loss functions are:</p><ul><li><p><code>Flux.binarycrossentropy</code>: Standard binary classification loss, also known as the log loss.</p></li><li><p><code>Flux.logitbinarycrossentropy</code>: Mathematically equal to crossentropy, but numerically more stable than finalising the outputs with <code>σ</code> and then calculating crossentropy. You will need to specify <code>finaliser=identity</code> to remove MLJFlux&#39;s default sigmoid finaliser, and understand that the output of <code>predict</code> is then unnormalized (no longer probabilistic).</p></li><li><p><code>Flux.tversky_loss</code>: Used with imbalanced data to give more weight to false negatives.</p></li><li><p><code>Flux.binary_focal_loss</code>: Used with highly imbalanced data. Weights harder examples more than easier examples.</p></li></ul><p>Currently MLJ measures are not supported values of <code>loss</code>.</p></li><li><p><code>epochs::Int=10</code>: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.</p></li><li><p><code>batch_size::int=1</code>: the batch size to be used for training, representing the number of samples per update of the network weights. Typically, batch size is between 8 and</p><ol><li>Increassing batch size may accelerate training if <code>acceleration=CUDALibs()</code> and a</li></ol><p>GPU is available.</p></li><li><p><code>lambda::Float64=0</code>: The strength of the weight regularization penalty. Can be any value in the range <code>[0, ∞)</code>.</p></li><li><p><code>alpha::Float64=0</code>: The L2/L1 mix of regularization, in the range <code>[0, 1]</code>. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.</p></li><li><p><code>rng::Union{AbstractRNG, Int64}</code>: The random number generator or seed used during training.</p></li><li><p><code>optimizer_changes_trigger_retraining::Bool=false</code>: Defines what happens when re-fitting a machine if the associated optimiser has changed. If <code>true</code>, the associated machine will retrain from scratch on <code>fit!</code> call, otherwise it will not.</p></li><li><p><code>acceleration::AbstractResource=CPU1()</code>: Defines on what hardware training is done. For Training on GPU, use <code>CUDALibs()</code>.</p></li><li><p><code>finaliser=Flux.σ</code>: The final activation function of the neural network (applied after the network defined by <code>builder</code>). Defaults to <code>Flux.σ</code>.</p></li></ul><p><strong>Operations</strong></p><ul><li><p><code>predict(mach, Xnew)</code>: return predictions of the target given new features <code>Xnew</code>, which should have the same scitype as <code>X</code> above. Predictions are probabilistic but uncalibrated.</p></li><li><p><code>predict_mode(mach, Xnew)</code>: Return the modes of the probabilistic predictions returned above.</p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>chain</code>: The trained &quot;chain&quot; (Flux.jl model), namely the series of layers,  functions, and activations which make up the neural network. This includes  the final layer specified by <code>finaliser</code> (eg, <code>softmax</code>).</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><code>training_losses</code>: A vector of training losses (penalised if <code>lambda != 0</code>) in  historical order, of length <code>epochs + 1</code>.  The first element is the pre-training loss.</li></ul><p><strong>Examples</strong></p><p>In this example we build a classification model using the Iris dataset. This is a very basic example, using a default builder and no standardization.  For a more advanced illustration, see <a href="../Regression/#MLJFlux.NeuralNetworkRegressor"><code>NeuralNetworkRegressor</code></a> or <a href="../Image Classification/#MLJFlux.ImageClassifier"><code>ImageClassifier</code></a>, and examples in the MLJFlux.jl documentation.</p><pre><code class="language-julia hljs">using MLJ, Flux
import Optimisers
import RDatasets</code></pre><p>First, we can load the data:</p><pre><code class="language-julia hljs">mtcars = RDatasets.dataset(&quot;datasets&quot;, &quot;mtcars&quot;);
y, X = unpack(mtcars, ==(:VS), in([:MPG, :Cyl, :Disp, :HP, :WT, :QSec]));</code></pre><p>Note that <code>y</code> is a vector and <code>X</code> a table.</p><pre><code class="language-julia hljs">y = categorical(y) # classifier takes catogorical input
X_f32 = Float32.(X) # To match floating point type of the neural network layers
NeuralNetworkBinaryClassifier = @load NeuralNetworkBinaryClassifier pkg=MLJFlux
bclf = NeuralNetworkBinaryClassifier()</code></pre><p>Next, we can train the model:</p><pre><code class="language-julia hljs">mach = machine(bclf, X_f32, y)
fit!(mach)</code></pre><p>We can train the model in an incremental fashion, altering the learning rate as we go, provided <code>optimizer_changes_trigger_retraining</code> is <code>false</code> (the default). Here, we also change the number of (total) iterations:</p><pre><code class="language-julia-repl hljs">julia&gt; bclf.optimiser
Adam(0.001, (0.9, 0.999), 1.0e-8)</code></pre><pre><code class="language-julia hljs">bclf.optimiser = Optimisers.Adam(eta = bclf.optimiser.eta * 2)
bclf.epochs = bclf.epochs + 5

fit!(mach, verbosity=2) # trains 5 more epochs</code></pre><p>We can inspect the mean training loss using the <code>cross_entropy</code> function:</p><pre><code class="language-julia hljs">training_loss = cross_entropy(predict(mach, X_f32), y)</code></pre><p>And we can access the Flux chain (model) using <code>fitted_params</code>:</p><pre><code class="language-julia hljs">chain = fitted_params(mach).chain</code></pre><p>Finally, we can see how the out-of-sample performance changes over time, using MLJ&#39;s <code>learning_curve</code> function:</p><pre><code class="language-julia hljs">r = range(bclf, :epochs, lower=1, upper=200, scale=:log10)
curve = learning_curve(
    bclf,
    X_f32,
    y,
    range=r,
    resampling=Holdout(fraction_train=0.7),
    measure=cross_entropy,
)
using Plots
plot(
   curve.parameter_values,
   curve.measurements,
   xlab=curve.parameter_name,
   xscale=curve.parameter_scale,
   ylab = &quot;Cross Entropy&quot;,
)
</code></pre><p>See also <a href="../Image Classification/#MLJFlux.ImageClassifier"><code>ImageClassifier</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/MLJFlux.jl/blob/088e152b38c4b599c49c04a18de94aaa8a1f886f/src/types.jl#L330-L345">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../Custom Builders/">« Custom Builders</a><a class="docs-footer-nextpage" href="../Regression/">Regression »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.4.1 on <span class="colophon-date" title="Monday 10 June 2024 23:13">Monday 10 June 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
