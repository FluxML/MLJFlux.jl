<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Regression · MLJFlux</title><meta name="title" content="Regression · MLJFlux"/><meta property="og:title" content="Regression · MLJFlux"/><meta property="twitter:title" content="Regression · MLJFlux"/><meta name="description" content="Documentation for MLJFlux."/><meta property="og:description" content="Documentation for MLJFlux."/><meta property="twitter:description" content="Documentation for MLJFlux."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,100;0,300;0,400;0,700;0,900;1,100;1,300;1,400;1,700;1,900&amp;family=Montserrat:ital,wght@0,100..900;1,100..900&amp;display=swap" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.gif" alt="MLJFlux logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">MLJFlux</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Introduction</a></li><li><input class="collapse-toggle" id="menuitem-2" type="checkbox" checked/><label class="tocitem" for="menuitem-2"><span class="docs-label">Interface</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../Summary/">Summary</a></li><li><a class="tocitem" href="../Builders/">Builders</a></li><li><a class="tocitem" href="../Custom Builders/">Custom Builders</a></li><li><a class="tocitem" href="../Classification/">Classification</a></li><li class="is-active"><a class="tocitem" href>Regression</a></li><li><a class="tocitem" href="../Multitarget Regression/">Multi-Target Regression</a></li><li><a class="tocitem" href="../Image Classification/">Image Classification</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Common Workflows</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../common_workflows/incremental_training/notebook/">Incremental Training</a></li><li><a class="tocitem" href="../../common_workflows/entity_embeddings/notebook/">Entity Embeddings</a></li><li><a class="tocitem" href="../../common_workflows/hyperparameter_tuning/notebook/">Hyperparameter Tuning</a></li><li><a class="tocitem" href="../../common_workflows/composition/notebook/">Model Composition</a></li><li><a class="tocitem" href="../../common_workflows/comparison/notebook/">Model Comparison</a></li><li><a class="tocitem" href="../../common_workflows/early_stopping/notebook/">Early Stopping</a></li><li><a class="tocitem" href="../../common_workflows/live_training/notebook/">Live Training</a></li><li><a class="tocitem" href="../../common_workflows/architecture_search/notebook/">Neural Architecture Search</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">Extended Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../extended_examples/MNIST/notebook/">MNIST Images</a></li><li><a class="tocitem" href="../../extended_examples/spam_detection/notebook/">Spam Detection with RNNs</a></li></ul></li><li><a class="tocitem" href="../../contributing/">Contributing</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Interface</a></li><li class="is-active"><a href>Regression</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Regression</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/FluxML/MLJFlux.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/FluxML/MLJFlux.jl/blob/dev/docs/src/interface/Regression.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJFlux.NeuralNetworkRegressor" href="#MLJFlux.NeuralNetworkRegressor"><code>MLJFlux.NeuralNetworkRegressor</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">NeuralNetworkRegressor</code></pre><p>A model type for constructing a neural network regressor, based on <a href="https://github.com/alan-turing-institute/MLJFlux.jl">MLJFlux.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">NeuralNetworkRegressor = @load NeuralNetworkRegressor pkg=MLJFlux</code></pre><p>Do <code>model = NeuralNetworkRegressor()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>NeuralNetworkRegressor(builder=...)</code>.</p><p><code>NeuralNetworkRegressor</code> is for training a data-dependent Flux.jl neural network to predict a <code>Continuous</code> target, given a table of <code>Continuous</code> features. Users provide a recipe for constructing the network, based on properties of the data that is encountered, by specifying an appropriate <code>builder</code>. See MLJFlux documentation for more on builders.</p><p>In addition to features with <code>Continuous</code> scientific element type, this model supports categorical features in the input table. If present, such features are embedded into dense vectors by the use of an additional <code>EntityEmbedderLayer</code> layer after the input, as described in Entity Embeddings of Categorical Variables by Cheng Guo, Felix Berkhahn arXiv, 2016.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, X, y)</code></pre><p>Here:</p><ul><li><code>X</code> provides input features and is either: (i) a <code>Matrix</code> with <code>Continuous</code> element scitype (typically <code>Float32</code>); or (ii) a table of input features (eg, a <code>DataFrame</code>) whose columns have <code>Continuous</code>, <code>Multiclass</code> or <code>OrderedFactor</code> element scitype; check column scitypes with <code>schema(X)</code>.  If any <code>Multiclass</code> or <code>OrderedFactor</code> features appear, the constructed network will use an <code>EntityEmbedderLayer</code> layer to transform them into dense vectors. If <code>X</code> is a <code>Matrix</code>, it is assumed that columns correspond to features and rows corresponding to observations.</li></ul><ul><li><code>y</code> is the target, which can be any <code>AbstractVector</code> whose element scitype is <code>Continuous</code>; check the scitype with <code>scitype(y)</code></li></ul><p>Train the machine with <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>builder=MLJFlux.Linear(σ=Flux.relu)</code>: An MLJFlux builder that constructs a neural  network. Possible <code>builders</code> include: <code>MLJFlux.Linear</code>, <code>MLJFlux.Short</code>, and  <code>MLJFlux.MLP</code>. See MLJFlux documentation for more on builders, and the example below  for using the <code>@builder</code> convenience macro.</p></li><li><p><code>optimiser::Optimisers.Adam()</code>: An Optimisers.jl optimiser. The optimiser performs the updating of the weights of the network. To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at <code>10e-3</code>, and tune using powers of <code>10</code> between <code>1</code> and <code>1e-7</code>.</p></li><li><p><code>loss=Flux.mse</code>: The loss function which the network will optimize. Should be a function which can be called in the form <code>loss(yhat, y)</code>.  Possible loss functions are listed in <a href="https://fluxml.ai/Flux.jl/stable/models/losses/">the Flux loss function documentation</a>. For a regression task, natural loss functions are:</p><ul><li><p><code>Flux.mse</code></p></li><li><p><code>Flux.mae</code></p></li><li><p><code>Flux.msle</code></p></li><li><p><code>Flux.huber_loss</code></p></li></ul><p>Currently MLJ measures are not supported as loss functions here.</p></li><li><p><code>epochs::Int=10</code>: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.</p></li><li><p><code>batch_size::int=1</code>: the batch size to be used for training, representing the number of samples per update of the network weights. Typically, batch size is between <code>8</code> and <code>512</code>. Increasing batch size may accelerate training if <code>acceleration=CUDALibs()</code> and a GPU is available.</p></li><li><p><code>lambda::Float64=0</code>: The strength of the weight regularization penalty. Can be any value in the range <code>[0, ∞)</code>. Note the history reports unpenalized losses.</p></li><li><p><code>alpha::Float64=0</code>: The L2/L1 mix of regularization, in the range <code>[0, 1]</code>. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.</p></li><li><p><code>rng::Union{AbstractRNG, Int64}</code>: The random number generator or seed used during training. The default is <code>Random.default_rng()</code>.</p></li><li><p><code>optimizer_changes_trigger_retraining::Bool=false</code>: Defines what happens when re-fitting a machine if the associated optimiser has changed. If <code>true</code>, the associated machine will retrain from scratch on <code>fit!</code> call, otherwise it will not.</p></li><li><p><code>acceleration::AbstractResource=CPU1()</code>: Defines on what hardware training is done. For Training on GPU, use <code>CUDALibs()</code>.</p></li><li><p><code>embedding_dims</code>: a <code>Dict</code> whose keys are names of categorical features, given as symbols, and whose values are numbers representing the desired dimensionality of the entity embeddings of such features: an integer value of <code>7</code>, say, sets the embedding dimensionality to <code>7</code>; a float value of <code>0.5</code>, say, sets the embedding dimensionality to <code>ceil(0.5 * c)</code>, where <code>c</code> is the number of feature levels.  Unspecified feature dimensionality defaults to <code>min(c - 1, 10)</code>.</p></li></ul><p><strong>Operations</strong></p><ul><li><p><code>predict(mach, Xnew)</code>: return predictions of the target given new features <code>Xnew</code>, which should have the same scitype as <code>X</code> above.</p></li><li><p><code>transform(mach, Xnew)</code>: Assuming <code>Xnew</code> has the same schema as <code>X</code>, transform the categorical features of <code>Xnew</code> into dense <code>Continuous</code> vectors using the <code>MLJFlux.EntityEmbedderLayer</code> layer present in the network. Does nothing in case the model was trained on an input <code>X</code> that lacks categorical features.</p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>chain</code>: The trained &quot;chain&quot; (Flux.jl model), namely the series of layers, functions,  and activations which make up the neural network.</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><code>training_losses</code>: A vector of training losses (penalized if <code>lambda != 0</code>) in  historical order, of length <code>epochs + 1</code>.  The first element is the pre-training loss.</li></ul><p><strong>Examples</strong></p><p>In this example we build a regression model for the Boston house price dataset.</p><pre><code class="language-julia hljs">using MLJ
import MLJFlux
using Flux
import Optimisers</code></pre><p>First, we load in the data: The <code>:MEDV</code> column becomes the target vector <code>y</code>, and all remaining columns go into a table <code>X</code>, with the exception of <code>:CHAS</code>:</p><pre><code class="language-julia hljs">data = OpenML.load(531); # Loads from https://www.openml.org/d/531
y, X = unpack(data, ==(:MEDV), !=(:CHAS); rng=123);

scitype(y)
schema(X)</code></pre><p>Since MLJFlux models do not handle ordered factors, we&#39;ll treat <code>:RAD</code> as <code>Continuous</code>:</p><pre><code class="language-julia hljs">X = coerce(X, :RAD=&gt;Continuous)</code></pre><p>Splitting off a test set:</p><pre><code class="language-julia hljs">(X, Xtest), (y, ytest) = partition((X, y), 0.7, multi=true);</code></pre><p>Next, we can define a <code>builder</code>, making use of a convenience macro to do so.  In the following <code>@builder</code> call, <code>n_in</code> is a proxy for the number input features (which will be known at <code>fit!</code> time) and <code>rng</code> is a proxy for a RNG (which will be passed from the <code>rng</code> field of <code>model</code> defined below). We also have the parameter <code>n_out</code> which is the number of output features. As we are doing single target regression, the value passed will always be <code>1</code>, but the builder we define will also work for <a href="../Multitarget Regression/#MLJFlux.MultitargetNeuralNetworkRegressor"><code>MultitargetNeuralNetworkRegressor</code></a>.</p><pre><code class="language-julia hljs">builder = MLJFlux.@builder begin
    init=Flux.glorot_uniform(rng)
    Chain(
        Dense(n_in, 64, relu, init=init),
        Dense(64, 32, relu, init=init),
        Dense(32, n_out, init=init),
    )
end</code></pre><p>Instantiating a model:</p><pre><code class="language-julia hljs">NeuralNetworkRegressor = @load NeuralNetworkRegressor pkg=MLJFlux
model = NeuralNetworkRegressor(
    builder=builder,
    rng=123,
    epochs=20
)</code></pre><p>We arrange for standardization of the the target by wrapping our model in <code>TransformedTargetModel</code>, and standardization of the features by inserting the wrapped model in a pipeline:</p><pre><code class="language-julia hljs">pipe = Standardizer |&gt; TransformedTargetModel(model, transformer=Standardizer)</code></pre><p>If we fit with a high verbosity (&gt;1), we will see the losses during training. We can also see the losses in the output of <code>report(mach)</code>.</p><pre><code class="language-julia hljs">mach = machine(pipe, X, y)
fit!(mach, verbosity=2)

# first element initial loss, 2:end per epoch training losses
report(mach).transformed_target_model_deterministic.model.training_losses</code></pre><p><strong>Experimenting with learning rate</strong></p><p>We can visually compare how the learning rate affects the predictions:</p><pre><code class="language-julia hljs">using Plots

rates = rates = [5e-5, 1e-4, 0.005, 0.001, 0.05]
plt=plot()

foreach(rates) do η
  pipe.transformed_target_model_deterministic.model.optimiser = Optimisers.Adam(η)
  fit!(mach, force=true, verbosity=0)
  losses =
      report(mach).transformed_target_model_deterministic.model.training_losses[3:end]
  plot!(1:length(losses), losses, label=η)
end

plt

pipe.transformed_target_model_deterministic.model.optimiser.eta = Optimisers.Adam(0.0001)</code></pre><p>With the learning rate fixed, we compute a CV estimate of the performance (using all data bound to <code>mach</code>) and compare this with performance on the test set:</p><pre><code class="language-julia hljs"># CV estimate, based on `(X, y)`:
evaluate!(mach, resampling=CV(nfolds=5), measure=l2)

# loss for `(Xtest, test)`:
fit!(mach) # train on `(X, y)`
yhat = predict(mach, Xtest)
l2(yhat, ytest)</code></pre><p>These losses, for the pipeline model, refer to the target on the original, unstandardized, scale.</p><p>For implementing stopping criterion and other iteration controls, refer to examples linked from the MLJFlux documentation.</p><p>See also <a href="../Multitarget Regression/#MLJFlux.MultitargetNeuralNetworkRegressor"><code>MultitargetNeuralNetworkRegressor</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/MLJFlux.jl/blob/22c2e332de9af6ea2e7391131276fcf0e14a77ad/src/types.jl#L880-L918">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../Classification/">« Classification</a><a class="docs-footer-nextpage" href="../Multitarget Regression/">Multi-Target Regression »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.1 on <span class="colophon-date" title="Wednesday 12 March 2025 21:58">Wednesday 12 March 2025</span>. Using Julia version 1.10.9.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
