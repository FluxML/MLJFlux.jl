<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Summary · MLJFlux</title><meta name="title" content="Summary · MLJFlux"/><meta property="og:title" content="Summary · MLJFlux"/><meta property="twitter:title" content="Summary · MLJFlux"/><meta name="description" content="Documentation for MLJFlux."/><meta property="og:description" content="Documentation for MLJFlux."/><meta property="twitter:description" content="Documentation for MLJFlux."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,100;0,300;0,400;0,700;0,900;1,100;1,300;1,400;1,700;1,900&amp;family=Montserrat:ital,wght@0,100..900;1,100..900&amp;display=swap" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.gif" alt="MLJFlux logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">MLJFlux</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Introduction</a></li><li><input class="collapse-toggle" id="menuitem-2" type="checkbox" checked/><label class="tocitem" for="menuitem-2"><span class="docs-label">Interface</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>Summary</a><ul class="internal"><li><a class="tocitem" href="#Models"><span>Models</span></a></li><li><a class="tocitem" href="#Model-Hyperparameters."><span>Model Hyperparameters.</span></a></li><li><a class="tocitem" href="#Built-in-builders"><span>Built-in builders</span></a></li></ul></li><li><a class="tocitem" href="../Builders/">Builders</a></li><li><a class="tocitem" href="../Custom Builders/">Custom Builders</a></li><li><a class="tocitem" href="../Classification/">Classification</a></li><li><a class="tocitem" href="../Regression/">Regression</a></li><li><a class="tocitem" href="../Multitarget Regression/">Multi-Target Regression</a></li><li><a class="tocitem" href="../Image Classification/">Image Classification</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Workflow Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../workflow examples/Incremental Training/incremental/">Incremental Training</a></li><li><a class="tocitem" href="../../workflow examples/Hyperparameter Tuning/tuning/">Hyperparameter Tuning</a></li><li><a class="tocitem" href="../../workflow examples/Basic Neural Architecture Search/tuning/">Neural Architecture Search</a></li><li><a class="tocitem" href="../../workflow examples/Composition/composition/">Model Composition</a></li><li><a class="tocitem" href="../../workflow examples/Comparison/comparison/">Model Comparison</a></li><li><a class="tocitem" href="../../workflow examples/Early Stopping/iteration/">Early Stopping</a></li><li><a class="tocitem" href="../../workflow examples/Live Training/live-training/">Live Training</a></li></ul></li><li><a class="tocitem" href="../../contributing/">Contributing</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Interface</a></li><li class="is-active"><a href>Summary</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Summary</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/FluxML/MLJFlux.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/FluxML/MLJFlux.jl/blob/dev/docs/src/interface/Summary.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h2 id="Models"><a class="docs-heading-anchor" href="#Models">Models</a><a id="Models-1"></a><a class="docs-heading-anchor-permalink" href="#Models" title="Permalink"></a></h2><p>MLJFlux provides four model types, for use with input features <code>X</code> and targets <code>y</code> of the <a href="https://alan-turing-institute.github.io/MLJScientificTypes.jl/dev/">scientific type</a> indicated in the table below. The parameters <code>n_in</code>, <code>n_out</code> and <code>n_channels</code> refer to information passed to the builder, as described under <a href="defining-a-new-builder">Defining a new builder</a> below.</p><table><tr><th style="text-align: right">Model Type</th><th style="text-align: right">Prediction type</th><th style="text-align: right"><code>scitype(X) &lt;: _</code></th><th style="text-align: right"><code>scitype(y) &lt;: _</code></th></tr><tr><td style="text-align: right"><code>NeuralNetworkRegressor</code></td><td style="text-align: right"><code>Deterministic</code></td><td style="text-align: right"><code>Table(Continuous)</code> with <code>n_in</code> columns</td><td style="text-align: right"><code>AbstractVector{&lt;:Continuous)</code> (<code>n_out = 1</code>)</td></tr><tr><td style="text-align: right"><code>MultitargetNeuralNetworkRegressor</code></td><td style="text-align: right"><code>Deterministic</code></td><td style="text-align: right"><code>Table(Continuous)</code> with <code>n_in</code> columns</td><td style="text-align: right"><code>&lt;: Table(Continuous)</code> with <code>n_out</code> columns</td></tr><tr><td style="text-align: right"><code>NeuralNetworkClassifier</code></td><td style="text-align: right"><code>Probabilistic</code></td><td style="text-align: right"><code>&lt;:Table(Continuous)</code> with <code>n_in</code> columns</td><td style="text-align: right"><code>AbstractVector{&lt;:Finite}</code> with <code>n_out</code> classes</td></tr><tr><td style="text-align: right"><code>NeuralNetworkBinaryClassifier</code></td><td style="text-align: right"><code>Probabilistic</code></td><td style="text-align: right"><code>&lt;:Table(Continuous)</code> with <code>n_in</code> columns</td><td style="text-align: right"><code>AbstractVector{&lt;:Finite{2}}</code> (<code>n_out = 2</code>)</td></tr><tr><td style="text-align: right"><code>ImageClassifier</code></td><td style="text-align: right"><code>Probabilistic</code></td><td style="text-align: right"><code>AbstractVector(&lt;:Image{W,H})</code> with <code>n_in = (W, H)</code></td><td style="text-align: right"><code>AbstractVector{&lt;:Finite}</code> with <code>n_out</code> classes</td></tr></table><details><summary><b>See definition of "model"</b></summary><p>In MLJ a <em>model</em> is a mutable struct storing hyper-parameters for some learning algorithm indicated by the model name, and that&#39;s all. In particular, an MLJ model does not store learned parameters.</p><div class="admonition is-warning"><header class="admonition-header">Difference in Definition</header><div class="admonition-body"><p>In Flux the term &quot;model&quot; has another meaning. However, as all Flux &quot;models&quot; used in MLJFLux are <code>Flux.Chain</code> objects, we call them <em>chains</em>, and restrict use of &quot;model&quot; to models in the MLJ sense.</p></div></div></details><details open><summary><b>Dealing with non-tabular input</b></summary><p>Any <code>AbstractMatrix{&lt;:AbstractFloat}</code> object <code>Xmat</code> can be forced to have scitype <code>Table(Continuous)</code> by replacing it with <code>X = MLJ.table(Xmat)</code>. Furthermore, this wrapping, and subsequent unwrapping under the hood, will compile to a no-op. At present this includes support for sparse matrix data, but the implementation has not been optimized for sparse data at this time and so should be used with caution.</p><p>Instructions for coercing common image formats into some <code>AbstractVector{&lt;:Image}</code> are <a href="https://juliaai.github.io/ScientificTypes.jl/dev/#Type-coercion-for-image-data">here</a>.</p></details><details closed><summary><b>Fitting and warm restarts</b></summary><p>MLJ machines cache state enabling the &quot;warm restart&quot; of model training, as demonstrated in the incremental training example. In the case of MLJFlux models, <code>fit!(mach)</code> will use a warm restart if:</p><ul><li><p>only <code>model.epochs</code> has changed since the last call; or</p></li><li><p>only <code>model.epochs</code> or <code>model.optimiser</code> have changed since the last call and <code>model.optimiser_changes_trigger_retraining == false</code> (the default) (the &quot;state&quot; part of the optimiser is ignored in this comparison). This allows one to dynamically modify learning rates, for example.</p></li></ul><p>Here <code>model=mach.model</code> is the associated MLJ model.</p><p>The warm restart feature makes it possible to apply early stopping criteria, as defined in <a href="https://github.com/ablaom/EarlyStopping.jl">EarlyStopping.jl</a>. For an example, see <a href="/examples/mnist/">/examples/mnist/</a>. (Eventually, this will be handled by an MLJ model wrapper for controlling arbitrary iterative models.)</p></details><h2 id="Model-Hyperparameters."><a class="docs-heading-anchor" href="#Model-Hyperparameters.">Model Hyperparameters.</a><a id="Model-Hyperparameters.-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Hyperparameters." title="Permalink"></a></h2><p>All models share the following hyper-parameters:</p><table><tr><th style="text-align: right">Hyper-parameter</th><th style="text-align: right">Description</th><th style="text-align: right">Default</th></tr><tr><td style="text-align: right"><code>builder</code></td><td style="text-align: right">Default builder for models.</td><td style="text-align: right"><code>MLJFlux.Linear(σ=Flux.relu)</code> (regressors) or <code>MLJFlux.Short(n_hidden=0, dropout=0.5, σ=Flux.σ)</code> (classifiers)</td></tr><tr><td style="text-align: right"><code>optimiser</code></td><td style="text-align: right">The optimiser to use for training.</td><td style="text-align: right"><code>Flux.ADAM()</code></td></tr><tr><td style="text-align: right"><code>loss</code></td><td style="text-align: right">The loss function used for training.</td><td style="text-align: right"><code>Flux.mse</code> (regressors) and <code>Flux.crossentropy</code> (classifiers)</td></tr><tr><td style="text-align: right"><code>n_epochs</code></td><td style="text-align: right">Number of epochs to train for.</td><td style="text-align: right"><code>10</code></td></tr><tr><td style="text-align: right"><code>batch_size</code></td><td style="text-align: right">The batch size for the data.</td><td style="text-align: right"><code>1</code></td></tr><tr><td style="text-align: right"><code>lambda</code></td><td style="text-align: right">The regularization strength. Range = [0, ∞).</td><td style="text-align: right"><code>0</code></td></tr><tr><td style="text-align: right"><code>alpha</code></td><td style="text-align: right">The L2/L1 mix of regularization. Range = [0, 1].</td><td style="text-align: right"><code>0</code></td></tr><tr><td style="text-align: right"><code>rng</code></td><td style="text-align: right">The random number generator (RNG) passed to builders, for weight initialization, for example. Can be any <code>AbstractRNG</code> or the seed (integer) for a <code>MersenneTwister</code> that is reset on every cold restart of model (machine) training.</td><td style="text-align: right"><code>GLOBAL_RNG</code></td></tr><tr><td style="text-align: right"><code>acceleration</code></td><td style="text-align: right">Use <code>CUDALibs()</code> for training on GPU; default is <code>CPU1()</code>.</td><td style="text-align: right"><code>CPU1()</code></td></tr><tr><td style="text-align: right"><code>optimiser_changes_trigger_retraining</code></td><td style="text-align: right">True if fitting an associated machine should trigger retraining from scratch whenever the optimiser changes.</td><td style="text-align: right"><code>false</code></td></tr></table><p>The classifiers have an additional hyperparameter <code>finaliser</code> (default = <code>Flux.softmax</code>) which is the operation applied to the unnormalized output of the final layer to obtain probabilities (outputs summing to one). Default = <code>Flux.softmax</code>. It should return a vector of the same length as its input.</p><div class="admonition is-info"><header class="admonition-header">Loss Functions</header><div class="admonition-body"><p>Currently, the loss function specified by <code>loss=...</code> is applied internally by Flux and needs to conform to the Flux API. You cannot, for example, supply one of MLJ&#39;s probabilistic loss functions, such as <code>MLJ.cross_entropy</code> to one of the classifier constructors. </p></div></div><p>That said, you can only use MLJ loss functions or metrics in evaluation meta-algorithms (such as cross validation) and they will work even if the underlying model comes from <code>MLJFlux</code>.</p><details closed><summary><b>More on accelerated training with GPUs</b></summary><p>As in the table, when instantiating a model for training on a GPU, specify <code>acceleration=CUDALibs()</code>, as in</p><pre><code class="language-julia hljs">using MLJ
ImageClassifier = @load ImageClassifier
model = ImageClassifier(epochs=10, acceleration=CUDALibs())
mach = machine(model, X, y) |&gt; fit!</code></pre><p>In this example, the data <code>X, y</code> is copied onto the GPU under the hood on the call to <code>fit!</code> and cached for use in any warm restart (see above). The Flux chain used in training is always copied back to the CPU at then conclusion of <code>fit!</code>, and made available as <code>fitted_params(mach)</code>.</p></details><h2 id="Built-in-builders"><a class="docs-heading-anchor" href="#Built-in-builders">Built-in builders</a><a id="Built-in-builders-1"></a><a class="docs-heading-anchor-permalink" href="#Built-in-builders" title="Permalink"></a></h2><p>As for the <code>builder</code> argument, the following builders are provided out-of-the-box:</p><table><tr><th style="text-align: left">Builder</th><th style="text-align: left">Description</th></tr><tr><td style="text-align: left"><code>MLJFlux.MLP(hidden=(10,))</code></td><td style="text-align: left">General multi-layer perceptron</td></tr><tr><td style="text-align: left"><code>MLJFlux.Short(n_hidden=0, dropout=0.5, σ=sigmoid)</code></td><td style="text-align: left">Fully connected network with one hidden layer and dropout</td></tr><tr><td style="text-align: left"><code>MLJFlux.Linear(σ=relu)</code></td><td style="text-align: left">Vanilla linear network with no hidden layers and activation function <code>σ</code></td></tr></table><p>See the following sections to learn more about the interface for the builders and models.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">« Introduction</a><a class="docs-footer-nextpage" href="../Builders/">Builders »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.4.1 on <span class="colophon-date" title="Monday 10 June 2024 23:13">Monday 10 June 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
