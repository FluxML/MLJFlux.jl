var documenterSearchIndex = {"docs":
[{"location":"contributing/#Adding-new-models-to-MLJFlux","page":"Contributing","title":"Adding new models to MLJFlux","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"This section assumes familiarity with the MLJ model API","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"If one subtypes a new model type as either MLJFlux.MLJFluxProbabilistic or MLJFlux.MLJFluxDeterministic, then instead of defining new methods for MLJModelInterface.fit and MLJModelInterface.update one can make use of fallbacks by implementing the lower level methods shape, build, and fitresult. See the classifier source code for an example.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"One still needs to implement a new predict method.","category":"page"},{"location":"interface/Multitarget Regression/","page":"Multi-Target Regression","title":"Multi-Target Regression","text":"MLJFlux.MultitargetNeuralNetworkRegressor","category":"page"},{"location":"interface/Multitarget Regression/#MLJFlux.MultitargetNeuralNetworkRegressor","page":"Multi-Target Regression","title":"MLJFlux.MultitargetNeuralNetworkRegressor","text":"MultitargetNeuralNetworkRegressor\n\nA model type for constructing a multitarget neural network regressor, based on MLJFlux.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nMultitargetNeuralNetworkRegressor = @load MultitargetNeuralNetworkRegressor pkg=MLJFlux\n\nDo model = MultitargetNeuralNetworkRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in MultitargetNeuralNetworkRegressor(builder=...).\n\nMultitargetNeuralNetworkRegressor is for training a data-dependent Flux.jl neural network to predict a multi-valued Continuous target, represented as a table, given a table of Continuous features. Users provide a recipe for constructing the network, based on properties of the data that is encountered, by specifying an appropriate builder. See MLJFlux documentation for more on builders.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX is either a Matrix or any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check column scitypes with schema(X). If X is a Matrix, it is assumed to have columns corresponding to features and rows corresponding to observations.\ny is the target, which can be any table or matrix of output targets whose element scitype is Continuous; check column scitypes with schema(y). If y is a Matrix, it is assumed to have columns corresponding to variables and rows corresponding to observations.\n\nHyper-parameters\n\nbuilder=MLJFlux.Linear(σ=Flux.relu): An MLJFlux builder that constructs a neural network. Possible builders include: Linear, Short, and MLP. See MLJFlux documentation for more on builders, and the example below for using the @builder convenience macro.\noptimiser::Flux.Adam(): A Flux.Optimise optimiser. The optimiser performs the updating of the weights of the network. For further reference, see the Flux optimiser documentation. To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at 10e-3, and tune using powers of 10 between 1 and 1e-7.\nloss=Flux.mse: The loss function which the network will optimize. Should be a function which can be called in the form loss(yhat, y).  Possible loss functions are listed in the Flux loss function documentation. For a regression task, natural loss functions are:\nFlux.mse\nFlux.mae\nFlux.msle\nFlux.huber_loss\nCurrently MLJ measures are not supported as loss functions here.\nepochs::Int=10: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.\nbatch_size::int=1: the batch size to be used for training, representing the number of samples per update of the network weights. Typically, batch size is between 8 and\nIncreassing batch size may accelerate training if acceleration=CUDALibs() and a\nGPU is available.\nlambda::Float64=0: The strength of the weight regularization penalty. Can be any value in the range [0, ∞).\nalpha::Float64=0: The L2/L1 mix of regularization, in the range [0, 1]. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.\nrng::Union{AbstractRNG, Int64}: The random number generator or seed used during training.\noptimizer_changes_trigger_retraining::Bool=false: Defines what happens when re-fitting a machine if the associated optimiser has changed. If true, the associated machine will retrain from scratch on fit! call, otherwise it will not.\nacceleration::AbstractResource=CPU1(): Defines on what hardware training is done. For Training on GPU, use CUDALibs().\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given new features Xnew having the same scitype as X above. Predictions are deterministic.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nchain: The trained \"chain\" (Flux.jl model), namely the series of layers,  functions, and activations  which make up the neural network.\n\nReport\n\nThe fields of report(mach) are:\n\ntraining_losses: A vector of training losses (penalised if lambda != 0) in  historical order, of length epochs + 1.  The first element is the pre-training loss.\n\nExamples\n\nIn this example we apply a multi-target regression model to synthetic data:\n\nusing MLJ\nimport MLJFlux\nusing Flux\n\nFirst, we generate some synthetic data (needs MLJBase 0.20.16 or higher):\n\nX, y = make_regression(100, 9; n_targets = 2) # both tables\nschema(y)\nschema(X)\n\nSplitting off a test set:\n\n(X, Xtest), (y, ytest) = partition((X, y), 0.7, multi=true);\n\nNext, we can define a builder, making use of a convenience macro to do so.  In the following @builder call, n_in is a proxy for the number input features and n_out the number of target variables (both known at fit! time), while rng is a proxy for a RNG (which will be passed from the rng field of model defined below).\n\nbuilder = MLJFlux.@builder begin\n    init=Flux.glorot_uniform(rng)\n    Chain(\n        Dense(n_in, 64, relu, init=init),\n        Dense(64, 32, relu, init=init),\n        Dense(32, n_out, init=init),\n    )\nend\n\nInstantiating the regression model:\n\nMultitargetNeuralNetworkRegressor = @load MultitargetNeuralNetworkRegressor\nmodel = MultitargetNeuralNetworkRegressor(builder=builder, rng=123, epochs=20)\n\nWe will arrange for standardization of the the target by wrapping our model in  TransformedTargetModel, and standardization of the features by inserting the wrapped  model in a pipeline:\n\npipe = Standardizer |> TransformedTargetModel(model, target=Standardizer)\n\nIf we fit with a high verbosity (>1), we will see the losses during training. We can also see the losses in the output of report(mach)\n\nmach = machine(pipe, X, y)\nfit!(mach, verbosity=2)\n\n# first element initial loss, 2:end per epoch training losses\nreport(mach).transformed_target_model_deterministic.model.training_losses\n\nFor experimenting with learning rate, see the NeuralNetworkRegressor example.\n\npipe.transformed_target_model_deterministic.model.optimiser.eta = 0.0001\n\nWith the learning rate fixed, we can now compute a CV estimate of the performance (using all data bound to mach) and compare this with performance on the test set:\n\n# custom MLJ loss:\nmulti_loss(yhat, y) = l2(MLJ.matrix(yhat), MLJ.matrix(y)) |> mean\n\n# CV estimate, based on `(X, y)`:\nevaluate!(mach, resampling=CV(nfolds=5), measure=multi_loss)\n\n# loss for `(Xtest, test)`:\nfit!(mach) # trains on all data `(X, y)`\nyhat = predict(mach, Xtest)\nmulti_loss(yhat, ytest)\n\nSee also NeuralNetworkRegressor\n\n\n\n\n\n","category":"type"},{"location":"workflow examples/Incremental Training/#Incremental-training","page":"Incremental Training","title":"Incremental training","text":"","category":"section"},{"location":"workflow examples/Incremental Training/","page":"Incremental Training","title":"Incremental Training","text":"import Random.seed!; seed!(123)\nmach = machine(clf, X, y)\nfit!(mach)\n\njulia> training_loss = cross_entropy(predict(mach, X), y) |> mean\n0.9064070459118777\n\n# Increasing learning rate and adding iterations:\nclf.optimiser.eta = clf.optimiser.eta * 2\nclf.epochs = clf.epochs + 5\n\njulia> fit!(mach, verbosity=2)\n[ Info: Updating Machine{NeuralNetworkClassifier{Short,…},…} @804.\n[ Info: Loss is 0.8686\n[ Info: Loss is 0.8228\n[ Info: Loss is 0.7706\n[ Info: Loss is 0.7565\n[ Info: Loss is 0.7347\nMachine{NeuralNetworkClassifier{Short,…},…} @804 trained 2 times; caches data\n  args:\n\t1:  Source @985 ⏎ `Table{AbstractVector{Continuous}}`\n\t2:  Source @367 ⏎ `AbstractVector{Multiclass{3}}`\n\njulia> training_loss = cross_entropy(predict(mach, X), y) |> mean\n0.7347092796453824","category":"page"},{"location":"workflow examples/Incremental Training/#Accessing-the-Flux-chain-(model)","page":"Incremental Training","title":"Accessing the Flux chain (model)","text":"","category":"section"},{"location":"workflow examples/Incremental Training/","page":"Incremental Training","title":"Incremental Training","text":"julia> fitted_params(mach).chain\nChain(Chain(Dense(4, 3, σ), Flux.Dropout{Float64}(0.5, false), Dense(3, 3)), softmax)","category":"page"},{"location":"workflow examples/Incremental Training/#Evolution-of-out-of-sample-performance","page":"Incremental Training","title":"Evolution of out-of-sample performance","text":"","category":"section"},{"location":"workflow examples/Incremental Training/","page":"Incremental Training","title":"Incremental Training","text":"r = range(clf, :epochs, lower=1, upper=200, scale=:log10)\ncurve = learning_curve(\n    clf, X, y;\n    range=r,\n    resampling=Holdout(fraction_train=0.7),\n    measure=cross_entropy,\n    )\nusing Plots\nplot(\n    curve.parameter_values,\n    curve.measurements,\n    xlab=curve.parameter_name,\n    xscale=curve.parameter_scale,\n    ylab = \"Cross Entropy\",\n    )\n","category":"page"},{"location":"workflow examples/Incremental Training/","page":"Incremental Training","title":"Incremental Training","text":"(Image: )","category":"page"},{"location":"workflow examples/Incremental Training/","page":"Incremental Training","title":"Incremental Training","text":"This is still work-in-progress. Expect more workflow examples and tutorials soon.","category":"page"},{"location":"interface/Regression/","page":"Regression","title":"Regression","text":"MLJFlux.NeuralNetworkRegressor","category":"page"},{"location":"interface/Regression/#MLJFlux.NeuralNetworkRegressor","page":"Regression","title":"MLJFlux.NeuralNetworkRegressor","text":"NeuralNetworkRegressor\n\nA model type for constructing a neural network regressor, based on MLJFlux.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nNeuralNetworkRegressor = @load NeuralNetworkRegressor pkg=MLJFlux\n\nDo model = NeuralNetworkRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in NeuralNetworkRegressor(builder=...).\n\nNeuralNetworkRegressor is for training a data-dependent Flux.jl neural network to predict a Continuous target, given a table of Continuous features. Users provide a recipe for constructing the network, based on properties of the data that is encountered, by specifying an appropriate builder. See MLJFlux documentation for more on builders.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX is either a Matrix or any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check column scitypes with schema(X). If X is a Matrix, it is assumed to have columns corresponding to features and rows corresponding to observations.\ny is the target, which can be any AbstractVector whose element scitype is Continuous; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).\n\nHyper-parameters\n\nbuilder=MLJFlux.Linear(σ=Flux.relu): An MLJFlux builder that constructs a neural  network. Possible builders include: MLJFlux.Linear, MLJFlux.Short, and  MLJFlux.MLP. See MLJFlux documentation for more on builders, and the example below  for using the @builder convenience macro.\noptimiser::Flux.Adam(): A Flux.Optimise optimiser. The optimiser performs the updating of the weights of the network. For further reference, see the Flux optimiser documentation. To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at 10e-3, and tune using powers of 10 between 1 and 1e-7.\nloss=Flux.mse: The loss function which the network will optimize. Should be a function which can be called in the form loss(yhat, y).  Possible loss functions are listed in the Flux loss function documentation. For a regression task, natural loss functions are:\nFlux.mse\nFlux.mae\nFlux.msle\nFlux.huber_loss\nCurrently MLJ measures are not supported as loss functions here.\nepochs::Int=10: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.\nbatch_size::int=1: the batch size to be used for training, representing the number of samples per update of the network weights. Typically, batch size is between 8 and\nIncreasing batch size may accelerate training if acceleration=CUDALibs() and a\nGPU is available.\nlambda::Float64=0: The strength of the weight regularization penalty. Can be any value in the range [0, ∞).\nalpha::Float64=0: The L2/L1 mix of regularization, in the range [0, 1]. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.\nrng::Union{AbstractRNG, Int64}: The random number generator or seed used during training.\noptimizer_changes_trigger_retraining::Bool=false: Defines what happens when re-fitting a machine if the associated optimiser has changed. If true, the associated machine will retrain from scratch on fit! call, otherwise it will not.\nacceleration::AbstractResource=CPU1(): Defines on what hardware training is done. For Training on GPU, use CUDALibs().\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given new features Xnew, which should have the same scitype as X above.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nchain: The trained \"chain\" (Flux.jl model), namely the series of layers, functions,  and activations which make up the neural network.\n\nReport\n\nThe fields of report(mach) are:\n\ntraining_losses: A vector of training losses (penalized if lambda != 0) in  historical order, of length epochs + 1.  The first element is the pre-training loss.\n\nExamples\n\nIn this example we build a regression model for the Boston house price dataset.\n\nusing MLJ\nimport MLJFlux\nusing Flux\n\nFirst, we load in the data: The :MEDV column becomes the target vector y, and all remaining columns go into a table X, with the exception of :CHAS:\n\ndata = OpenML.load(531); # Loads from https://www.openml.org/d/531\ny, X = unpack(data, ==(:MEDV), !=(:CHAS); rng=123);\n\nscitype(y)\nschema(X)\n\nSince MLJFlux models do not handle ordered factors, we'll treat :RAD as Continuous:\n\nX = coerce(X, :RAD=>Continuous)\n\nSplitting off a test set:\n\n(X, Xtest), (y, ytest) = partition((X, y), 0.7, multi=true);\n\nNext, we can define a builder, making use of a convenience macro to do so.  In the following @builder call, n_in is a proxy for the number input features (which will be known at fit! time) and rng is a proxy for a RNG (which will be passed from the rng field of model defined below). We also have the parameter n_out which is the number of output features. As we are doing single target regression, the value passed will always be 1, but the builder we define will also work for MultitargetNeuralRegressor.\n\nbuilder = MLJFlux.@builder begin\n    init=Flux.glorot_uniform(rng)\n    Chain(\n        Dense(n_in, 64, relu, init=init),\n        Dense(64, 32, relu, init=init),\n        Dense(32, n_out, init=init),\n    )\nend\n\nInstantiating a model:\n\nNeuralNetworkRegressor = @load NeuralNetworkRegressor pkg=MLJFlux\nmodel = NeuralNetworkRegressor(\n    builder=builder,\n    rng=123,\n    epochs=20\n)\n\nWe arrange for standardization of the the target by wrapping our model in TransformedTargetModel, and standardization of the features by inserting the wrapped model in a pipeline:\n\npipe = Standardizer |> TransformedTargetModel(model, target=Standardizer)\n\nIf we fit with a high verbosity (>1), we will see the losses during training. We can also see the losses in the output of report(mach).\n\nmach = machine(pipe, X, y)\nfit!(mach, verbosity=2)\n\n# first element initial loss, 2:end per epoch training losses\nreport(mach).transformed_target_model_deterministic.model.training_losses\n\nExperimenting with learning rate\n\nWe can visually compare how the learning rate affects the predictions:\n\nusing Plots\n\nrates = rates = [5e-5, 1e-4, 0.005, 0.001, 0.05]\nplt=plot()\n\nforeach(rates) do η\n  pipe.transformed_target_model_deterministic.model.optimiser.eta = η\n  fit!(mach, force=true, verbosity=0)\n  losses =\n      report(mach).transformed_target_model_deterministic.model.training_losses[3:end]\n  plot!(1:length(losses), losses, label=η)\nend\n\nplt\n\npipe.transformed_target_model_deterministic.model.optimiser.eta = 0.0001\n\nWith the learning rate fixed, we compute a CV estimate of the performance (using all data bound to mach) and compare this with performance on the test set:\n\n# CV estimate, based on `(X, y)`:\nevaluate!(mach, resampling=CV(nfolds=5), measure=l2)\n\n# loss for `(Xtest, test)`:\nfit!(mach) # train on `(X, y)`\nyhat = predict(mach, Xtest)\nl2(yhat, ytest)  |> mean\n\nThese losses, for the pipeline model, refer to the target on the original, unstandardized, scale.\n\nFor implementing stopping criterion and other iteration controls, refer to examples linked from the MLJFlux documentation.\n\nSee also MultitargetNeuralNetworkRegressor\n\n\n\n\n\n","category":"type"},{"location":"interface/Builders/","page":"Builders","title":"Builders","text":"MLJFlux.Linear","category":"page"},{"location":"interface/Builders/#MLJFlux.Linear","page":"Builders","title":"MLJFlux.Linear","text":"Linear(; σ=Flux.relu, rng=Random.GLOBAL_RNG)\n\nMLJFlux builder that constructs a fully connected two layer network with activation function σ. The number of input and output nodes is determined from the data. The bias and coefficients are initialized using Flux.glorot_uniform(rng). If rng is an integer, it is instead used as the seed for a MersenneTwister.\n\n\n\n\n\n","category":"type"},{"location":"interface/Builders/","page":"Builders","title":"Builders","text":"MLJFlux.Short","category":"page"},{"location":"interface/Builders/#MLJFlux.Short","page":"Builders","title":"MLJFlux.Short","text":"Short(; n_hidden=0, dropout=0.5, σ=Flux.sigmoid, rng=GLOBAL_RNG)\n\nMLJFlux builder that constructs a full-connected three-layer network using n_hidden nodes in the hidden layer and the specified dropout (defaulting to 0.5). An activation function σ is applied between the hidden and final layers. If n_hidden=0 (the default) then n_hidden is the geometric mean of the number of input and output nodes.  The number of input and output nodes is determined from the data.\n\nThe each layer is initialized using Flux.glorot_uniform(rng). If rng is an integer, it is instead used as the seed for a MersenneTwister.\n\n\n\n\n\n","category":"type"},{"location":"interface/Builders/","page":"Builders","title":"Builders","text":"MLJFlux.MLP","category":"page"},{"location":"interface/Builders/#MLJFlux.MLP","page":"Builders","title":"MLJFlux.MLP","text":"MLP(; hidden=(100,), σ=Flux.relu, rng=GLOBAL_RNG)\n\nMLJFlux builder that constructs a Multi-layer perceptron network. The ith element of hidden represents the number of neurons in the ith hidden layer. An activation function σ is applied between each layer.\n\nThe each layer is initialized using Flux.glorot_uniform(rng). If rng is an integer, it is instead used as the seed for a MersenneTwister.\n\n\n\n\n\n","category":"type"},{"location":"interface/Builders/","page":"Builders","title":"Builders","text":"MLJFlux.@builder","category":"page"},{"location":"interface/Builders/#MLJFlux.@builder","page":"Builders","title":"MLJFlux.@builder","text":"@builder neural_net\n\nCreates a builder for neural_net. The variables rng, n_in, n_out and n_channels can be used to create builders for any random number generator rng, input and output sizes n_in and n_out and number of input channels n_channels.\n\nExamples\n\njulia> import MLJFlux: @builder;\n\njulia> nn = NeuralNetworkRegressor(builder = @builder(Chain(Dense(n_in, 64, relu),\n                                                            Dense(64, 32, relu),\n                                                            Dense(32, n_out))));\n\njulia> conv_builder = @builder begin\n           front = Chain(Conv((3, 3), n_channels => 16), Flux.flatten)\n           d = Flux.outputsize(front, (n_in..., n_channels, 1)) |> first\n           Chain(front, Dense(d, n_out));\n       end\n\njulia> conv_nn = NeuralNetworkRegressor(builder = conv_builder);\n\n\n\n\n\n","category":"macro"},{"location":"interface/Image Classification/","page":"Image Classification","title":"Image Classification","text":"MLJFlux.ImageClassifier","category":"page"},{"location":"interface/Image Classification/#MLJFlux.ImageClassifier","page":"Image Classification","title":"MLJFlux.ImageClassifier","text":"ImageClassifier\n\nA model type for constructing a image classifier, based on MLJFlux.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nImageClassifier = @load ImageClassifier pkg=MLJFlux\n\nDo model = ImageClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ImageClassifier(builder=...).\n\nImageClassifier classifies images using a neural network adapted to the type of images provided (color or gray scale). Predictions are probabilistic. Users provide a recipe for constructing the network, based on properties of the image encountered, by specifying an appropriate builder. See MLJFlux documentation for more on builders.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX is any AbstractVector of images with ColorImage or GrayImage scitype; check  the scitype with scitype(X) and refer to ScientificTypes.jl documentation on coercing  typical image formats into an appropriate type.\ny is the target, which can be any AbstractVector whose element  scitype is Multiclass; check the scitype with scitype(y).\n\nTrain the machine with fit!(mach, rows=...).\n\nHyper-parameters\n\nbuilder: An MLJFlux builder that constructs the neural network.  The fallback builds a  depth-16 VGG architecture adapted to the image size and number of target classes, with  no batch normalization; see the Metalhead.jl documentation for details. See the example  below for a user-specified builder. A convenience macro @builder is also  available. See also finaliser below.\noptimiser::Flux.Adam(): A Flux.Optimise optimiser. The optimiser performs the updating of the weights of the network. For further reference, see the Flux optimiser documentation. To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at 10e-3, and tune using powers of 10 between 1 and 1e-7.\nloss=Flux.crossentropy: The loss function which the network will optimize. Should be a function which can be called in the form loss(yhat, y).  Possible loss functions are listed in the Flux loss function documentation. For a classification task, the most natural loss functions are:\nFlux.crossentropy: Standard multiclass classification loss, also known as the log loss.\nFlux.logitcrossentopy: Mathematically equal to crossentropy, but numerically more stable than finalising the outputs with softmax and then calculating crossentropy. You will need to specify finaliser=identity to remove MLJFlux's default softmax finaliser, and understand that the output of predict is then unnormalized (no longer probabilistic).\nFlux.tversky_loss: Used with imbalanced data to give more weight to false negatives.\nFlux.focal_loss: Used with highly imbalanced data. Weights harder examples more than easier examples.\nCurrently MLJ measures are not supported values of loss.\nepochs::Int=10: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.\nbatch_size::int=1: the batch size to be used for training, representing the number of samples per update of the network weights. Typically, batch size is between 8 and\nIncreassing batch size may accelerate training if acceleration=CUDALibs() and a\nGPU is available.\nlambda::Float64=0: The strength of the weight regularization penalty. Can be any value in the range [0, ∞).\nalpha::Float64=0: The L2/L1 mix of regularization, in the range [0, 1]. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.\nrng::Union{AbstractRNG, Int64}: The random number generator or seed used during training.\noptimizer_changes_trigger_retraining::Bool=false: Defines what happens when re-fitting a machine if the associated optimiser has changed. If true, the associated machine will retrain from scratch on fit! call, otherwise it will not.\nacceleration::AbstractResource=CPU1(): Defines on what hardware training is done. For Training on GPU, use CUDALibs().\nfinaliser=Flux.softmax: The final activation function of the neural network (applied after the network defined by builder). Defaults to Flux.softmax.\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given new features Xnew, which should have the same scitype as X above. Predictions are probabilistic but uncalibrated.\npredict_mode(mach, Xnew): Return the modes of the probabilistic predictions returned above.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nchain: The trained \"chain\" (Flux.jl model), namely the series of layers,  functions, and activations  which make up the neural network. This includes  the final layer specified by finaliser (eg, softmax).\n\nReport\n\nThe fields of report(mach) are:\n\ntraining_losses: A vector of training losses (penalised if lambda != 0) in  historical order, of length epochs + 1.  The first element is the pre-training loss.\n\nExamples\n\nIn this example we use MLJFlux and a custom builder to classify the MNIST image dataset.\n\nusing MLJ\nusing Flux\nimport MLJFlux\nimport MLJIteration # for `skip` control\n\nFirst we want to download the MNIST dataset, and unpack into images and labels:\n\nimport MLDatasets: MNIST\ndata = MNIST(split=:train)\nimages, labels = data.features, data.targets\n\nIn MLJ, integers cannot be used for encoding categorical data, so we must coerce them into the Multiclass scitype:\n\nlabels = coerce(labels, Multiclass);\n\nAbove images is a single array but MLJFlux requires the images to be a vector of individual image arrays:\n\nimages = coerce(images, GrayImage);\nimages[1]\n\nWe start by defining a suitable builder object. This is a recipe for building the neural network. Our builder will work for images of any (constant) size, whether they be color or black and white (ie, single or multi-channel).  The architecture always consists of six alternating convolution and max-pool layers, and a final dense layer; the filter size and the number of channels after each convolution layer is customizable.\n\nimport MLJFlux\n\nstruct MyConvBuilder\n    filter_size::Int\n    channels1::Int\n    channels2::Int\n    channels3::Int\nend\n\nmake2d(x::AbstractArray) = reshape(x, :, size(x)[end])\n\nfunction MLJFlux.build(b::MyConvBuilder, rng, n_in, n_out, n_channels)\n    k, c1, c2, c3 = b.filter_size, b.channels1, b.channels2, b.channels3\n    mod(k, 2) == 1 || error(\"`filter_size` must be odd. \")\n    p = div(k - 1, 2) # padding to preserve image size\n    init = Flux.glorot_uniform(rng)\n    front = Chain(\n        Conv((k, k), n_channels => c1, pad=(p, p), relu, init=init),\n        MaxPool((2, 2)),\n        Conv((k, k), c1 => c2, pad=(p, p), relu, init=init),\n        MaxPool((2, 2)),\n        Conv((k, k), c2 => c3, pad=(p, p), relu, init=init),\n        MaxPool((2 ,2)),\n        make2d)\n    d = Flux.outputsize(front, (n_in..., n_channels, 1)) |> first\n    return Chain(front, Dense(d, n_out, init=init))\nend\n\nIt is important to note that in our build function, there is no final softmax. This is applied by default in all MLJFlux classifiers (override this using the finaliser hyperparameter).\n\nNow that our builder is defined, we can instantiate the actual MLJFlux model. If you have a GPU, you can substitute in acceleration=CUDALibs() below to speed up training.\n\nImageClassifier = @load ImageClassifier pkg=MLJFlux\nclf = ImageClassifier(builder=MyConvBuilder(3, 16, 32, 32),\n                      batch_size=50,\n                      epochs=10,\n                      rng=123)\n\nYou can add Flux options such as optimiser and loss in the snippet above. Currently, loss must be a flux-compatible loss, and not an MLJ measure.\n\nNext, we can bind the model with the data in a machine, and train using the first 500 images:\n\nmach = machine(clf, images, labels);\nfit!(mach, rows=1:500, verbosity=2);\nreport(mach)\nchain = fitted_params(mach)\nFlux.params(chain)[2]\n\nWe can tack on 20 more epochs by modifying the epochs field, and iteratively fit some more:\n\nclf.epochs = clf.epochs + 20\nfit!(mach, rows=1:500, verbosity=2);\n\nWe can also make predictions and calculate an out-of-sample loss estimate, using any MLJ measure (loss/score):\n\npredicted_labels = predict(mach, rows=501:1000);\ncross_entropy(predicted_labels, labels[501:1000]) |> mean\n\nThe preceding fit!/predict/evaluate workflow can be alternatively executed as follows:\n\nevaluate!(mach,\n          resampling=Holdout(fraction_train=0.5),\n          measure=cross_entropy,\n          rows=1:1000,\n          verbosity=0)\n\nSee also NeuralNetworkClassifier.\n\n\n\n\n\n","category":"type"},{"location":"interface/Summary/#Models","page":"Summary","title":"Models","text":"","category":"section"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"MLJFlux provides four model types, for use with input features X and targets y of the scientific type indicated in the table below. The parameters n_in, n_out and n_channels refer to information passed to the builder, as described under Defining a new builder below.","category":"page"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"Model Type Prediction type scitype(X) <: _ scitype(y) <: _\nNeuralNetworkRegressor Deterministic Table(Continuous) with n_in columns AbstractVector{<:Continuous) (n_out = 1)\nMultitargetNeuralNetworkRegressor Deterministic Table(Continuous) with n_in columns <: Table(Continuous) with n_out columns\nNeuralNetworkClassifier Probabilistic <:Table(Continuous) with n_in columns AbstractVector{<:Finite} with n_out classes\nImageClassifier Probabilistic AbstractVector(<:Image{W,H}) with n_in = (W, H) AbstractVector{<:Finite} with n_out classes","category":"page"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"<details><summary><b>See definition of \"model\"</b></summary>","category":"page"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"In MLJ a model is a mutable struct storing hyper-parameters for some learning algorithm indicated by the model name, and that's all. In particular, an MLJ model does not store learned parameters.","category":"page"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"warning: Difference in Definition\nIn Flux the term \"model\" has another meaning. However, as all Flux \"models\" used in MLJFLux are Flux.Chain objects, we call them chains, and restrict use of \"model\" to models in the MLJ sense.","category":"page"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"</details>","category":"page"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"<details open><summary><b>Dealing with non-tabular input</b></summary>","category":"page"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"Any AbstractMatrix{<:AbstractFloat} object Xmat can be forced to have scitype Table(Continuous) by replacing it with X = MLJ.table(Xmat). Furthermore, this wrapping, and subsequent unwrapping under the hood, will compile to a no-op. At present this includes support for sparse matrix data, but the implementation has not been optimized for sparse data at this time and so should be used with caution.","category":"page"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"Instructions for coercing common image formats into some AbstractVector{<:Image} are here.","category":"page"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"</details>","category":"page"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"<details closed><summary><b>Fitting and warm restarts</b></summary>","category":"page"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"MLJ machines cache state enabling the \"warm restart\" of model training, as demonstrated in the incremental training example. In the case of MLJFlux models, fit!(mach) will use a warm restart if:","category":"page"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"only model.epochs has changed since the last call; or\nonly model.epochs or model.optimiser have changed since the last call and model.optimiser_changes_trigger_retraining == false (the default) (the \"state\" part of the optimiser is ignored in this comparison). This allows one to dynamically modify learning rates, for example.","category":"page"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"Here model=mach.model is the associated MLJ model.","category":"page"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"The warm restart feature makes it possible to apply early stopping criteria, as defined in EarlyStopping.jl. For an example, see /examples/mnist/. (Eventually, this will be handled by an MLJ model wrapper for controlling arbitrary iterative models.)","category":"page"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"</details>","category":"page"},{"location":"interface/Summary/#Model-Hyperparameters.","page":"Summary","title":"Model Hyperparameters.","text":"","category":"section"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"All models share the following hyper-parameters:","category":"page"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"Hyper-parameter Description Default\nbuilder Default builder for models. MLJFlux.Linear(σ=Flux.relu) (regressors) or MLJFlux.Short(n_hidden=0, dropout=0.5, σ=Flux.σ) (classifiers)\noptimiser The optimiser to use for training. Flux.ADAM()\nloss The loss function used for training. Flux.mse (regressors) and Flux.crossentropy (classifiers)\nn_epochs Number of epochs to train for. 10\nbatch_size The batch size for the data. 1\nlambda The regularization strength. Range = [0, ∞). 0\nalpha The L2/L1 mix of regularization. Range = [0, 1]. 0\nrng The random number generator (RNG) passed to builders, for weight initialization, for example. Can be any AbstractRNG or the seed (integer) for a MersenneTwister that is reset on every cold restart of model (machine) training. GLOBAL_RNG\nacceleration Use CUDALibs() for training on GPU; default is CPU1(). CPU1()\noptimiser_changes_trigger_retraining True if fitting an associated machine should trigger retraining from scratch whenever the optimiser changes. false","category":"page"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"The classifiers have an additional hyperparameter finaliser (default = Flux.softmax) which is the operation applied to the unnormalized output of the final layer to obtain probabilities (outputs summing to one). Default = Flux.softmax. It should return a vector of the same length as its input.","category":"page"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"note: Loss Functions\nCurrently, the loss function specified by loss=... is applied internally by Flux and needs to conform to the Flux API. You cannot, for example, supply one of MLJ's probabilistic loss functions, such as MLJ.cross_entropy to one of the classifier constructors. ","category":"page"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"That said, you can only use MLJ loss functions or metrics in evaluation meta-algorithms (such as cross validation) and they will work even if the underlying model comes from MLJFlux.","category":"page"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"<details closed><summary><b>More on accelerated training with GPUs</b></summary>","category":"page"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"As in the table, when instantiating a model for training on a GPU, specify acceleration=CUDALibs(), as in","category":"page"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"using MLJ\nImageClassifier = @load ImageClassifier\nmodel = ImageClassifier(epochs=10, acceleration=CUDALibs())\nmach = machine(model, X, y) |> fit!","category":"page"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"In this example, the data X, y is copied onto the GPU under the hood on the call to fit! and cached for use in any warm restart (see above). The Flux chain used in training is always copied back to the CPU at then conclusion of fit!, and made available as fitted_params(mach).","category":"page"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"</details>","category":"page"},{"location":"interface/Summary/#Built-in-builders","page":"Summary","title":"Built-in builders","text":"","category":"section"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"As for the builder argument, the following builders are provided out-of-the-box:","category":"page"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"Builder Description\nMLJFlux.MLP(hidden=(10,)) General multi-layer perceptron\nMLJFlux.Short(n_hidden=0, dropout=0.5, σ=sigmoid) Fully connected network with one hidden layer and dropout\nMLJFlux.Linear(σ=relu) Vanilla linear network with no hidden layers and activation function σ","category":"page"},{"location":"interface/Summary/","page":"Summary","title":"Summary","text":"See the following sections to learn more about the interface for the builders and models.","category":"page"},{"location":"interface/Classification/","page":"Classification","title":"Classification","text":"MLJFlux.NeuralNetworkClassifier","category":"page"},{"location":"interface/Classification/#MLJFlux.NeuralNetworkClassifier","page":"Classification","title":"MLJFlux.NeuralNetworkClassifier","text":"NeuralNetworkClassifier\n\nA model type for constructing a neural network classifier, based on MLJFlux.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nNeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux\n\nDo model = NeuralNetworkClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in NeuralNetworkClassifier(builder=...).\n\nNeuralNetworkClassifier is for training a data-dependent Flux.jl neural network for making probabilistic predictions of a Multiclass or OrderedFactor target, given a table of Continuous features. Users provide a recipe for constructing  the network, based on properties of the data that is encountered, by specifying  an appropriate builder. See MLJFlux documentation for more on builders.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX is either a Matrix or any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check column scitypes with schema(X). If X is a Matrix, it is assumed to have columns corresponding to features and rows corresponding to observations.\ny is the target, which can be any AbstractVector whose element scitype is Multiclass or OrderedFactor; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).\n\nHyper-parameters\n\nbuilder=MLJFlux.Short(): An MLJFlux builder that constructs a neural network. Possible  builders include: MLJFlux.Linear, MLJFlux.Short, and MLJFlux.MLP. See  MLJFlux.jl documentation for examples of user-defined builders. See also finaliser  below.\noptimiser::Flux.Adam(): A Flux.Optimise optimiser. The optimiser performs the updating of the weights of the network. For further reference, see the Flux optimiser documentation. To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at 10e-3, and tune using powers of 10 between 1 and 1e-7.\nloss=Flux.crossentropy: The loss function which the network will optimize. Should be a function which can be called in the form loss(yhat, y).  Possible loss functions are listed in the Flux loss function documentation. For a classification task, the most natural loss functions are:\nFlux.crossentropy: Standard multiclass classification loss, also known as the log loss.\nFlux.logitcrossentopy: Mathematically equal to crossentropy, but numerically more stable than finalising the outputs with softmax and then calculating crossentropy. You will need to specify finaliser=identity to remove MLJFlux's default softmax finaliser, and understand that the output of predict is then unnormalized (no longer probabilistic).\nFlux.tversky_loss: Used with imbalanced data to give more weight to false negatives.\nFlux.focal_loss: Used with highly imbalanced data. Weights harder examples more than easier examples.\nCurrently MLJ measures are not supported values of loss.\nepochs::Int=10: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.\nbatch_size::int=1: the batch size to be used for training, representing the number of samples per update of the network weights. Typically, batch size is between 8 and\nIncreassing batch size may accelerate training if acceleration=CUDALibs() and a\nGPU is available.\nlambda::Float64=0: The strength of the weight regularization penalty. Can be any value in the range [0, ∞).\nalpha::Float64=0: The L2/L1 mix of regularization, in the range [0, 1]. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.\nrng::Union{AbstractRNG, Int64}: The random number generator or seed used during training.\noptimizer_changes_trigger_retraining::Bool=false: Defines what happens when re-fitting a machine if the associated optimiser has changed. If true, the associated machine will retrain from scratch on fit! call, otherwise it will not.\nacceleration::AbstractResource=CPU1(): Defines on what hardware training is done. For Training on GPU, use CUDALibs().\nfinaliser=Flux.softmax: The final activation function of the neural network (applied after the network defined by builder). Defaults to Flux.softmax.\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given new features Xnew, which should have the same scitype as X above. Predictions are probabilistic but uncalibrated.\npredict_mode(mach, Xnew): Return the modes of the probabilistic predictions returned above.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nchain: The trained \"chain\" (Flux.jl model), namely the series of layers,  functions, and activations which make up the neural network. This includes  the final layer specified by finaliser (eg, softmax).\n\nReport\n\nThe fields of report(mach) are:\n\ntraining_losses: A vector of training losses (penalised if lambda != 0) in  historical order, of length epochs + 1.  The first element is the pre-training loss.\n\nExamples\n\nIn this example we build a classification model using the Iris dataset. This is a very basic example, using a default builder and no standardization.  For a more advanced illustration, see NeuralNetworkRegressor or ImageClassifier, and examples in the MLJFlux.jl documentation.\n\nusing MLJ\nusing Flux\nimport RDatasets\n\nFirst, we can load the data:\n\niris = RDatasets.dataset(\"datasets\", \"iris\");\ny, X = unpack(iris, ==(:Species), rng=123); # a vector and a table\nNeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux\nclf = NeuralNetworkClassifier()\n\nNext, we can train the model:\n\nmach = machine(clf, X, y)\nfit!(mach)\n\nWe can train the model in an incremental fashion, altering the learning rate as we go, provided optimizer_changes_trigger_retraining is false (the default). Here, we also change the number of (total) iterations:\n\nclf.optimiser.eta = clf.optimiser.eta * 2\nclf.epochs = clf.epochs + 5\n\nfit!(mach, verbosity=2) # trains 5 more epochs\n\nWe can inspect the mean training loss using the cross_entropy function:\n\ntraining_loss = cross_entropy(predict(mach, X), y) |> mean\n\nAnd we can access the Flux chain (model) using fitted_params:\n\nchain = fitted_params(mach).chain\n\nFinally, we can see how the out-of-sample performance changes over time, using MLJ's learning_curve function:\n\nr = range(clf, :epochs, lower=1, upper=200, scale=:log10)\ncurve = learning_curve(clf, X, y,\n                     range=r,\n                     resampling=Holdout(fraction_train=0.7),\n                     measure=cross_entropy)\nusing Plots\nplot(curve.parameter_values,\n     curve.measurements,\n     xlab=curve.parameter_name,\n     xscale=curve.parameter_scale,\n     ylab = \"Cross Entropy\")\n\n\nSee also ImageClassifier.\n\n\n\n\n\n","category":"type"},{"location":"#MLJFlux.jl","page":"Introduction","title":"MLJFlux.jl","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"A Julia package integrating deep learning Flux models with MLJ.","category":"page"},{"location":"#Objectives","page":"Introduction","title":"Objectives","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Provide a user-friendly and high-level interface to fundamental Flux deep learning models while still being extensible by supporting custom models written with Flux\nMake building deep learning models more convenient to users already familiar with the MLJ workflow\nMake it easier to apply machine learning techniques provided by MLJ, including: out-of-sample performance evaluation, hyper-parameter optimization, iteration control, and more, to deep learning models","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"note: MLJFlux Coverage\nMLJFlux support is focused on fundamental and widely used deep learning models.  Sophisticated architectures or techniques such as online learning, reinforcement learning, and adversarial networks are currently beyond its scope. ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Also note that MLJFlux is limited to training models only when all training data fits into memory, though it still supports automatic batching of data.","category":"page"},{"location":"#Installation","page":"Introduction","title":"Installation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"import Pkg\nPkg.activate(\"my_environment\", shared=true)\nPkg.add([\"MLJ\", \"MLJFlux\", \"Flux\"])","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"You only need Flux if you need to build a custom architecture or experiment with different optimizers, loss functions and activations.","category":"page"},{"location":"#Quick-Start","page":"Introduction","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"using MLJ, Flux, MLJFlux\nimport RDatasets\n\n# 1. Load Data\niris = RDatasets.dataset(\"datasets\", \"iris\");\ny, X = unpack(iris, ==(:Species), colname -> true, rng=123);\n\n# 2. Load and instantiate model\nNeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=\"MLJFlux\"\nclf = NeuralNetworkClassifier(\n    builder=MLJFlux.MLP(; hidden=(5,4), σ=Flux.relu),\n    optimiser=Flux.ADAM(0.01),\n    batch_size=8,\n    epochs=100, \n    acceleration=CUDALibs()         # For GPU support\n    )\n\n# 3. Wrap it in a machine \nmach = machine(clf, X, y)\n\n# 4. Evaluate the model\ncv=CV(nfolds=5)\nevaluate!(mach, resampling=cv, measure=accuracy) ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"As you can see we were able to use MLJ functionality (i.e., cross validation) with a Flux deep learning model. All arguments provided also have defaults.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Notice that we were also able to define the neural network in a high-level fashion by only specifying the number of neurons in each hidden layer and the activation function. Meanwhile, MLJFlux was able to infer the input and output layer as well as use a suitable default for the loss function and output activation given the classification task. Notice as well that we did not need to implement a training or prediction loop as in Flux.","category":"page"},{"location":"#Basic-idea","page":"Introduction","title":"Basic idea","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"As in the example above, any MLJFlux model has a builder hyperparameter, an object encoding instructions for creating a neural network given the data that the model eventually sees (e.g., the number of classes in a classification problem). While each MLJ model has a simple default builder, users may need to define custom builders to get optimal results, and this will require familiarity with the Flux API for defining a neural network chain.","category":"page"},{"location":"#Flux-or-MLJFlux?","page":"Introduction","title":"Flux or MLJFlux?","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Flux is a deep learning framework in Julia that comes with everything you need to build deep learning models (i.e., GPU support, automatic differentiation, layers, activations, losses, optimizers, etc.). MLJFlux wraps models built with Flux which provides a more high-level interface for building and training such models. More importantly, it empowers Flux models by extending their support to many common machine learning workflows that are possible via MLJ such as:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Estimating performance of your model using a holdout set or other resampling strategy (e.g., cross-validation) as measured by one or more metrics (e.g., loss functions) that may not have been used in training\nOptimizing hyper-parameters such as a regularization parameter (e.g., dropout) or a width/height/nchannnels of convolution layer\nCompose with other models such as introducing data pre-processing steps (e.g., missing data imputation) into a pipeline. It might make sense to include non-deep learning models in this pipeline. Other kinds of model composition could include blending predictions of a deep learner with some other kind of model (as in “model stacking”). Models composed with MLJ can be also tuned as a single unit.\nControlling iteration by adding an early stopping criterion based on an out-of-sample estimate of the loss, dynamically changing the learning rate (eg, cyclic learning rates), periodically save snapshots of the model, generate live plots of sample weights to judge training progress (as in tensor board)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Comparing your model with a non-deep learning models","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"A comparable project, FastAI/FluxTraining, also provides a high-level interface for interacting with Flux models and supports a set of features that may overlap with (but not include all of) those supported by MLJFlux.","category":"page"},{"location":"full tutorials/MNIST/#Image-Classification-Example","page":"-","title":"Image Classification Example","text":"","category":"section"},{"location":"full tutorials/MNIST/","page":"-","title":"-","text":"An expanded version of this example, with early stopping and snapshots, is available here.","category":"page"},{"location":"full tutorials/MNIST/","page":"-","title":"-","text":"We define a builder that builds a chain with six alternating convolution and max-pool layers, and a final dense layer, which we apply to the MNIST image dataset.","category":"page"},{"location":"full tutorials/MNIST/","page":"-","title":"-","text":"First we define a generic builder (working for any image size, color or gray):","category":"page"},{"location":"full tutorials/MNIST/","page":"-","title":"-","text":"using MLJ\nusing Flux\nusing MLDatasets\n\n# helper function\nfunction flatten(x::AbstractArray)\n\treturn reshape(x, :, size(x)[end])\nend\n\nimport MLJFlux\nmutable struct MyConvBuilder\n\tfilter_size::Int\n\tchannels1::Int\n\tchannels2::Int\n\tchannels3::Int\nend\n\nfunction MLJFlux.build(b::MyConvBuilder, rng, n_in, n_out, n_channels)\n\n\tk, c1, c2, c3 = b.filter_size, b.channels1, b.channels2, b.channels3\n\n\tmod(k, 2) == 1 || error(\"`filter_size` must be odd. \")\n\n\t# padding to preserve image size on convolution:\n\tp = div(k - 1, 2)\n\n\tfront = Chain(\n            Conv((k, k), n_channels => c1, pad=(p, p), relu),\n            MaxPool((2, 2)),\n            Conv((k, k), c1 => c2, pad=(p, p), relu),\n            MaxPool((2, 2)),\n            Conv((k, k), c2 => c3, pad=(p, p), relu),\n            MaxPool((2 ,2)),\n           flatten,\n           )\n\td = Flux.outputsize(front, (n_in..., n_channels, 1)) |> first\n\treturn Chain(front, Dense(d, n_out))\nend","category":"page"},{"location":"full tutorials/MNIST/","page":"-","title":"-","text":"Next, we load some of the MNIST data and check scientific types conform to those is the table above:","category":"page"},{"location":"full tutorials/MNIST/","page":"-","title":"-","text":"N = 500\nXraw, yraw = MNIST(split=:train)[:];\nXraw = Xraw[:,:,1:N];\nyraw = yraw[1:N];\n\nscitype(Xraw)","category":"page"},{"location":"full tutorials/MNIST/","page":"-","title":"-","text":"scitype(yraw)","category":"page"},{"location":"full tutorials/MNIST/","page":"-","title":"-","text":"Inputs should have element scitype GrayImage:","category":"page"},{"location":"full tutorials/MNIST/","page":"-","title":"-","text":"X = coerce(Xraw, GrayImage);","category":"page"},{"location":"full tutorials/MNIST/","page":"-","title":"-","text":"For classifiers, target must have element scitype <: Finite:","category":"page"},{"location":"full tutorials/MNIST/","page":"-","title":"-","text":"y = coerce(yraw, Multiclass);","category":"page"},{"location":"full tutorials/MNIST/","page":"-","title":"-","text":"Instantiating an image classifier model:","category":"page"},{"location":"full tutorials/MNIST/","page":"-","title":"-","text":"ImageClassifier = @load ImageClassifier\nclf = ImageClassifier(\n    builder=MyConvBuilder(3, 16, 32, 32),\n    epochs=10,\n    loss=Flux.crossentropy,\n    )","category":"page"},{"location":"full tutorials/MNIST/","page":"-","title":"-","text":"And evaluating the accuracy of the model on a 30% holdout set:","category":"page"},{"location":"full tutorials/MNIST/","page":"-","title":"-","text":"mach = machine(clf, X, y)\n\nevaluate!(\n    mach,\n    resampling=Holdout(rng=123, fraction_train=0.7),\n    measure=misclassification_rate,\n    )","category":"page"},{"location":"interface/Custom Builders/#Defining-Custom-Builders","page":"Custom Builders","title":"Defining Custom Builders","text":"","category":"section"},{"location":"interface/Custom Builders/","page":"Custom Builders","title":"Custom Builders","text":"Following is an example defining a new builder for creating a simple fully-connected neural network with two hidden layers, with n1 nodes in the first hidden layer, and n2 nodes in the second, for use in any of the first three models in Table 1. The definition includes one mutable struct and one method:","category":"page"},{"location":"interface/Custom Builders/","page":"Custom Builders","title":"Custom Builders","text":"mutable struct MyBuilder <: MLJFlux.Builder\n\tn1 :: Int\n\tn2 :: Int\nend\n\nfunction MLJFlux.build(nn::MyBuilder, rng, n_in, n_out)\n\tinit = Flux.glorot_uniform(rng)\n        return Chain(\n            Dense(n_in, nn.n1, init=init),\n            Dense(nn.n1, nn.n2, init=init),\n            Dense(nn.n2, n_out, init=init),\n            )\nend","category":"page"},{"location":"interface/Custom Builders/","page":"Custom Builders","title":"Custom Builders","text":"Note here that n_in and n_out depend on the size of the data (see Table 1.","category":"page"},{"location":"interface/Custom Builders/","page":"Custom Builders","title":"Custom Builders","text":"For a concrete image classification example, see the Image Classification Example.","category":"page"},{"location":"interface/Custom Builders/","page":"Custom Builders","title":"Custom Builders","text":"More generally, defining a new builder means defining a new struct sub-typing MLJFlux.Builder and defining a new MLJFlux.build method with one of these signatures:","category":"page"},{"location":"interface/Custom Builders/","page":"Custom Builders","title":"Custom Builders","text":"MLJFlux.build(builder::MyBuilder, rng, n_in, n_out)\nMLJFlux.build(builder::MyBuilder, rng, n_in, n_out, n_channels) # for use with `ImageClassifier`","category":"page"},{"location":"interface/Custom Builders/","page":"Custom Builders","title":"Custom Builders","text":"This method must return a Flux.Chain instance, chain, subject to the following conditions:","category":"page"},{"location":"interface/Custom Builders/","page":"Custom Builders","title":"Custom Builders","text":"chain(x) must make sense:\nfor any x <: Array{<:AbstractFloat, 2} of size (n_in, batch_size) where batch_size is any integer (for use with one of the first three model types); or\nfor any x <: Array{<:Float32, 4} of size (W, H, n_channels, batch_size), where (W, H) = n_in, n_channels is 1 or 3, and batch_size is any integer (for use with ImageClassifier)\nThe object returned by chain(x) must be an AbstractFloat vector of length n_out.","category":"page"},{"location":"interface/Custom Builders/","page":"Custom Builders","title":"Custom Builders","text":"Alternatively, use MLJFlux.@builder(neural_net) to automatically create a builder for any valid Flux chain expression neural_net, where the symbols n_in, n_out, n_channels and rng can appear literally, with the interpretations explained above. For example,","category":"page"},{"location":"interface/Custom Builders/","page":"Custom Builders","title":"Custom Builders","text":"builder = MLJFlux.@builder Chain(Dense(n_in, 128), Dense(128, n_out, tanh))","category":"page"}]
}
