var documenterSearchIndex = {"docs":
[{"location":"common_workflows/hyperparameter_tuning/notebook/#Hyperparameter-Tuning-with-MLJFlux","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning with MLJFlux","text":"This demonstration is available as a Jupyter notebook or julia script here.\n\nIn this workflow example we learn how to tune different hyperparameters of MLJFlux models with emphasis on training hyperparameters.\n\nThis script tested using Julia 1.10","category":"section"},{"location":"common_workflows/hyperparameter_tuning/notebook/#Basic-Imports","page":"Hyperparameter Tuning","title":"Basic Imports","text":"using MLJ               # Has MLJFlux models\nusing Flux              # For more flexibility\nusing Plots             # To plot tuning results\nimport Optimisers       # native Flux.jl optimisers no longer supported\nusing StableRNGs        # for reproducibility across Julia versions\n\nstable_rng() = StableRNGs.StableRNG(123)","category":"section"},{"location":"common_workflows/hyperparameter_tuning/notebook/#Loading-and-Splitting-the-Data","page":"Hyperparameter Tuning","title":"Loading and Splitting the Data","text":"iris = load_iris() # a named-tuple of vectors\ny, X = unpack(iris, ==(:target), rng=stable_rng())\nX = fmap(column-> Float32.(column), X) # Flux prefers Float32 data","category":"section"},{"location":"common_workflows/hyperparameter_tuning/notebook/#Instantiating-the-model","page":"Hyperparameter Tuning","title":"Instantiating the model","text":"Now let's construct our model. This follows a similar setup the one followed in the Quick Start.\n\nNeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux\nclf = NeuralNetworkClassifier(\n    builder=MLJFlux.MLP(; hidden=(5,4), σ=Flux.relu),\n    optimiser=Optimisers.Adam(0.01),\n    batch_size=8,\n    epochs=10,\n    rng=stable_rng(),\n)","category":"section"},{"location":"common_workflows/hyperparameter_tuning/notebook/#Hyperparameter-Tuning-Example","page":"Hyperparameter Tuning","title":"Hyperparameter Tuning Example","text":"Let's tune the batch size and the learning rate. We will use grid search and 5-fold cross-validation.\n\nWe start by defining the hyperparameter ranges\n\nr1 = range(clf, :batch_size, lower=1, upper=64)\netas = [10^x for x in range(-4, stop=0, length=4)]\noptimisers = [Optimisers.Adam(eta) for eta in etas]\nr2 = range(clf, :optimiser, values=optimisers)\n\nThen passing the ranges along with the model and other arguments to the TunedModel constructor.\n\ntuned_model = TunedModel(\n    model=clf,\n    tuning=Grid(goal=25),\n    resampling=CV(nfolds=5, rng=stable_rng()),\n    range=[r1, r2],\n    measure=cross_entropy,\n);\nnothing #hide\n\nThen wrapping our tuned model in a machine and fitting it.\n\nmach = machine(tuned_model, X, y);\nfit!(mach, verbosity=0);\nnothing #hide\n\nLet's check out the best performing model:\n\nfitted_params(mach).best_model","category":"section"},{"location":"common_workflows/hyperparameter_tuning/notebook/#Learning-Curves","page":"Hyperparameter Tuning","title":"Learning Curves","text":"With learning curves, it's possible to center our focus on the effects of a single hyperparameter of the model\n\nFirst define the range and wrap it in a learning curve\n\nr = range(clf, :epochs, lower=1, upper=200, scale=:log10)\ncurve = learning_curve(\n    clf,\n    X,\n    y,\n    range=r,\n    resampling=CV(nfolds=4, rng=stable_rng()),\n    measure=cross_entropy,\n)\n\nThen plot the curve\n\nplot(\n    curve.parameter_values,\n    curve.measurements,\n    xlab=curve.parameter_name,\n    xscale=curve.parameter_scale,\n    ylab = \"Cross Entropy\",\n)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"common_workflows/entity_embeddings/README/#Contents","page":"Contents","title":"Contents","text":"file description\nnotebook.ipynb Juptyer notebook (executed)\nnotebook.unexecuted.ipynb Jupyter notebook (unexecuted)\nnotebook.md static markdown (included in MLJFlux.jl docs)\nnotebook.jl executable Julia script annotated with comments\ngenerate.jl maintainers only: execute to generate MD and IPYNB from JL","category":"section"},{"location":"common_workflows/entity_embeddings/README/#Important","page":"Contents","title":"Important","text":"Scripts or notebooks in this folder cannot be reliably executed without the accompanying Manifest.toml and Project.toml files. Relative to the directory of this README.md, you can try looking for these files at ../../../docs/","category":"section"},{"location":"common_workflows/comparison/notebook/#Model-Comparison-with-MLJFlux","page":"Model Comparison","title":"Model Comparison with MLJFlux","text":"This demonstration is available as a Jupyter notebook or julia script here.\n\nIn this workflow example, we see how we can compare different machine learning models with a neural network from MLJFlux.\n\nThis script tested using Julia 1.10","category":"section"},{"location":"common_workflows/comparison/notebook/#Basic-Imports","page":"Model Comparison","title":"Basic Imports","text":"using MLJ               # Has MLJFlux models\nusing Flux              # For more flexibility\nusing DataFrames        # To visualize hyperparameter search results\nimport Optimisers       # native Flux.jl optimisers no longer supported\nusing Measurements       # to get ± functionality\nimport CategoricalArrays.unwrap\nusing StableRNGs        # for reproducibility across Julia versions\n\nstable_rng() = StableRNG(123)","category":"section"},{"location":"common_workflows/comparison/notebook/#Loading-and-Splitting-the-Data","page":"Model Comparison","title":"Loading and Splitting the Data","text":"iris = load_iris() # a named-tuple of vectors\ny, X = unpack(iris, ==(:target), rng=stable_rng())","category":"section"},{"location":"common_workflows/comparison/notebook/#Instantiating-the-models-Now-let's-construct-our-model.-This-follows-a-similar-setup","page":"Model Comparison","title":"Instantiating the models Now let's construct our model. This follows a similar setup","text":"to the one followed in the Quick Start.\n\nNeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux\n\nclf1 = NeuralNetworkClassifier(\n    builder=MLJFlux.MLP(; hidden=(5,4), σ=Flux.relu),\n    optimiser=Optimisers.Adam(0.01),\n    batch_size=8,\n    epochs=50,\n    rng=stable_rng(),\n    )\n\nLet's as well load and construct three other classical machine learning models:\n\nBayesianLDA = @load BayesianLDA pkg=MultivariateStats\nclf2 = BayesianLDA()\nRandomForestClassifier = @load RandomForestClassifier pkg=DecisionTree\nclf3 = RandomForestClassifier()\nXGBoostClassifier = @load XGBoostClassifier pkg=XGBoost\nclf4 = XGBoostClassifier();\nnothing #hide","category":"section"},{"location":"common_workflows/comparison/notebook/#Wrapping-One-of-the-Models-in-a-TunedModel","page":"Model Comparison","title":"Wrapping One of the Models in a TunedModel","text":"Instead of just comparing with four models with the default/given hyperparameters, we will give XGBoostClassifier an unfair advantage By wrapping it in a TunedModel that considers the best learning rate η for the model.\n\nr1 = range(clf4, :eta, lower=0.01, upper=0.5, scale=:log10)\ntuned_model_xg = TunedModel(\n    model=clf4,\n    ranges=[r1],\n    tuning=Grid(resolution=10),\n    resampling=CV(nfolds=5, rng=stable_rng()),\n    measure=cross_entropy,\n);\nnothing #hide\n\nOf course, one can wrap each of the four in a TunedModel if they are interested in comparing the models over a large set of their hyperparameters.","category":"section"},{"location":"common_workflows/comparison/notebook/#Comparing-the-models","page":"Model Comparison","title":"Comparing the models","text":"We simply pass the four models to the models argument of the TunedModel construct\n\ntuned_model = TunedModel(\n    models=[clf1, clf2, clf3, tuned_model_xg],\n    tuning=Explicit(),\n    resampling=CV(nfolds=2, rng=stable_rng()),\n    repeats=5,\n    measure=cross_entropy,\n);\nnothing #hide\n\nNotice here we are using 5 x 2 Monte Carlo cross-validation.\n\nThen wrapping our tuned model in a machine and fitting it.\n\nmach = machine(tuned_model, X, y);\nfit!(mach, verbosity=0);\nnothing #hide\n\nNow let's see the history for more details on the performance for each of the models\n\nhistory = report(mach).history\nhistory_df = DataFrame(\n    mlp = [x.model for x in history],\n    measurement = [\n        x.evaluation.measurement[1] ±\n            x.evaluation.uncertainty_radius_95[1] for x in history\n                ],\n)\nsort!(history_df, [order(:measurement)])\n\nThis is Occam's razor in practice.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"contributing/#Adding-new-models-to-MLJFlux","page":"Contributing","title":"Adding new models to MLJFlux","text":"This section assumes familiarity with the MLJ model API\n\nIf one subtypes a new model type as either MLJFlux.MLJFluxProbabilistic or MLJFlux.MLJFluxDeterministic, then instead of defining new methods for MLJModelInterface.fit and MLJModelInterface.update one can make use of fallbacks by implementing the lower level methods shape, build, and fitresult. See the classifier source code for an example.\n\nOne still needs to implement a new predict method.","category":"section"},{"location":"interface/entity_embedding/","page":"Entity Embdedding","title":"Entity Embdedding","text":"Most MLJFlux models support categorical features by learning enitity embeddings. By wrapping such an MLJFlux model using EntityEmbedder, the learned embeddings can be used in MLJ pipelines as transformer elements. In particular, these embeddings can be used for supervised models that are not neural networks and require features to be Continuous. See the example below.","category":"section"},{"location":"interface/entity_embedding/#MLJFlux.EntityEmbedder","page":"Entity Embdedding","title":"MLJFlux.EntityEmbedder","text":"EntityEmbedder(; model=supervised_mljflux_model)\n\nWrapper for a MLJFlux supervised model, to convert it to a transformer. Such transformers are still presented a target variable in training, but they behave as transformers in MLJ pipelines. They are entity embedding transformers, in the sense of the article, \"Entity Embeddings of Categorical Variables\" by Cheng Guo, Felix Berkhahn.\n\nTraining data\n\nIn MLJ (or MLJBase) bind an instance unsupervised model to data with\n\nmach = machine(embed_model, X, y)\n\nHere:\n\nembed_model is an instance of EntityEmbedder, which wraps a supervised MLJFlux model, model, which must be an instance of one of these: MLJFlux.NeuralNetworkClassifier, NeuralNetworkBinaryClassifier, MLJFlux.NeuralNetworkRegressor,MLJFlux.MultitargetNeuralNetworkRegressor.\nX is any table of input features supported by the model being wrapped. Features to be transformed must have element scitype Multiclass or OrderedFactor. Use schema(X) to check scitypes.\ny is the target, which can be any AbstractVector supported by the model being wrapped.\n\nTrain the machine using fit!(mach).\n\nExamples\n\nIn the following example we wrap a NeuralNetworkClassifier as an EntityEmbedder, so that it can be used to supply continuously encoded features to a nearest neighbor model, which does not support categorical features.\n\nSimple Example\n\nusing MLJ\n\n# Setup some data\nN = 400\nX = (\n  a = rand(Float32, N),\n  b = categorical(rand(\"abcde\", N)),\n  c = categorical(rand(\"ABCDEFGHIJ\", N), ordered = true),\n)\n\ny = categorical(rand(\"YN\", N));\n\n# Initiate model\nEntityEmbedder = @load EntityEmbedder pkg=MLJFlux\n\n# Flux model to do learn the entity embeddings:\nNeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux\n\n# Instantiate the models:\nclf = NeuralNetworkClassifier(embedding_dims=Dict(:b => 2, :c => 3))\nemb = EntityEmbedder(clf)\n\n# Train and transform the data using the embedder:\nmach = machine(emb, X, y)\nfit!(mach)\nXnew = transform(mach, X)\n\n# Compare schemas before and after transformation\nschema(X)\nschema(Xnew)\n\nUsing with Downstream Models (Pipeline)\n\nusing MLJ\n\n# Setup some data\nN = 400\nX = (\n  a = rand(Float32, N),\n  b = categorical(rand(\"abcde\", N)),\n  c = categorical(rand(\"ABCDEFGHIJ\", N), ordered = true),\n)\n\ny = categorical(rand(\"YN\", N));\n\n# Initiate model\nEntityEmbedder = @load EntityEmbedder pkg=MLJFlux\n\n# Flux model to do learn the entity embeddings:\nNeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux\n\n# Other supervised model type, requiring `Continuous` features:\nKNNClassifier = @load KNNClassifier pkg=NearestNeighborModels\n\n# Instantiate the models:\nclf = NeuralNetworkClassifier(embedding_dims=Dict(:b => 2, :c => 3))\nemb = EntityEmbedder(clf)\n\n# Now construct the pipeline:\npipe = emb |> KNNClassifier()\n\n# And train it to make predictions:\nmach = machine(pipe, X, y)\nfit!(mach)\npredict(mach, X)[1:3]\n\nIt is to be emphasized that the NeuralNertworkClassifier is only being used to learn entity embeddings, not to make predictions, which here are made by KNNClassifier().\n\nSee also NeuralNetworkClassifier, NeuralNetworkRegressor\n\n\n\n\n\n","category":"type"},{"location":"common_workflows/entity_embeddings/notebook/#Entity-Embeddings-with-MLJFlux","page":"Entity Embeddings","title":"Entity Embeddings with MLJFlux","text":"This demonstration is available as a Jupyter notebook or julia script here.\n\nEntity embedding is newer deep learning approach for categorical encoding introduced in 2016 by Cheng Guo and Felix Berkhahn.  It employs a set of embedding layers to map each categorical feature into a dense continuous vector in a similar fashion to how they are employed in NLP architectures.\n\nIn MLJFlux, the NeuralNetworkClassifier, NeuralNetworkRegressor, and the MultitargetNeuralNetworkRegressorcan be trained and evaluated with heterogenous data (i.e., containing categorical features) because they have a built-in entity embedding layer.  Moreover, they offer atransform` method which encodes the categorical features with the learned embeddings. Such embeddings can then be used as features in downstream machine learning models.\n\nIn this notebook, we will explore how to use entity embeddings in MLJFlux models.\n\nThis script tested using Julia 1.10","category":"section"},{"location":"common_workflows/entity_embeddings/notebook/#Basic-Imports","page":"Entity Embeddings","title":"Basic Imports","text":"using MLJ\nusing Flux\nusing Optimisers\nusing CategoricalArrays\nusing DataFrames\nusing Random\nusing Tables\nusing ProgressMeter\nusing Plots\nusing ScientificTypes\nusing StableRNGs        # for reproducibility across Julia versions\n\nstable_rng() = StableRNGs.StableRNG(246)\n\nGenerate some data\n\nX, y = make_blobs(1000, 2; centers=2, as_table=true, rng=stable_rng())\nX = DataFrame(X);\nnothing #hide\n\nVisualize it\n\nX_class0 = X[y .== 1, :]\nX_class1 = X[y .== 2, :]\n\np = plot()\n\nscatter!(p, X_class0[!, 1], X_class0[!, 2], markercolor=:blue, label=\"Class 0\")\nscatter!(p, X_class1[!, 1], X_class1[!, 2], markercolor=:red, label=\"Class 1\")\n\ntitle!(p, \"Classes in Different Colors\")\nxlabel!(\"Feature 1\")\nylabel!(\"Feature 2\")\n\nplot(p)\n\nLet's write a function that creates categorical features C1 and C2 from x1 and x2 in a meaningful way:\n\nrng = stable_rng()\ngenerate_C1(x1) = (x1 > mean(X.x1) ) ? rand(rng, ['A', 'B'])  : rand(rng, ['C', 'D'])\ngenerate_C2(x2) = (x2 > mean(X.x2) ) ? rand(rng, ['X', 'Y'])  : rand(rng, ['Z'])\n\nGenerate C1 and C2 columns\n\nX[!, :C1] = [generate_C1(x) for x in X[!, :x1]];\nX[!, :C2] = [generate_C2(x) for x in X[!, :x2]];\nX[!, :R3] = rand(1000);  # A random continuous column.\nnothing #hide\n\nForm final dataset using categorical and continuous columns\n\nX = X[!, [:C1, :C2, :R3]];\nnothing #hide\n\nIt's also necessary to cast the categorical columns to the correct scientific type as the embedding layer will have an effect on the model if and only if categorical columns exist.\n\nX = coerce(X, :C1=>Multiclass, :C2=>Multiclass);\nnothing #hide\n\nSplit the data\n\n(X_train, X_test), (y_train, y_test) = partition(\n        (X, y),\n        0.8,\n        multi = true,\n        shuffle = true,\n        stratify = y,\n        rng = stable_rng(),\n);\nnothing #hide","category":"section"},{"location":"common_workflows/entity_embeddings/notebook/#Build-MLJFlux-Model","page":"Entity Embeddings","title":"Build MLJFlux Model","text":"NeuralNetworkClassifier = @load NeuralNetworkClassifier pkg = MLJFlux\n\n\nclf = MLJFlux.NeuralNetworkBinaryClassifier(\n    builder = MLJFlux.Short(n_hidden = 5),\n    optimiser = Optimisers.Adam(0.01),\n    batch_size = 2,\n    epochs = 100,\n    acceleration = CPU1(), # use `CUDALibs()` on a GPU\n    embedding_dims =  Dict(:C1 => 2, :C2 => 2,),\n);\nnothing #hide\n\nNotice that we specified to embed each of the columns to 2D columns. By default, it uses min(numfeats - 1, 10) for the new dimensionality of any categorical feature.","category":"section"},{"location":"common_workflows/entity_embeddings/notebook/#Train-and-evaluate","page":"Entity Embeddings","title":"Train and evaluate","text":"mach = machine(clf, X_train, y_train)\n\nfit!(mach, verbosity = 0)\n\nGet predictions on the training data\n\ny_pred = predict_mode(mach, X_test)\nbalanced_accuracy(y_pred, y_test)\n\nNotice how the model has learnt to almost perfectly distinguish the classes and all the information has been in the categorical variables.","category":"section"},{"location":"common_workflows/entity_embeddings/notebook/#Visualize-the-embedding-space","page":"Entity Embeddings","title":"Visualize the embedding space","text":"mapping_matrices = MLJFlux.get_embedding_matrices(\n                fitted_params(mach).chain,\n                [1, 2],             # feature indices\n                [:C1, :C2],         # feature names (to assign to the indices)\n            )\n\nC1_basis = mapping_matrices[:C1]\nC2_basis = mapping_matrices[:C2]\n\np1 = scatter(C1_basis[1, :], C1_basis[2, :],\n             title = \"C1 Basis Columns\",\n             xlabel = \"Column 1\",\n             ylabel = \"Column 2\",\n             xlim = (-5, 5),\n             ylim = (-5, 5),\n             label = nothing,\n)\n\np2 = scatter(C2_basis[1, :], C2_basis[2, :],\n             title = \"C2 Basis Columns\",\n             xlabel = \"Column 1\",\n             ylabel = \"Column 2\",\n             xlim = (-1.2, 1.0),\n             ylim = (-1.5, 0.25),\n             label = nothing,\n)\n\nc1_cats = ['A', 'B', 'C', 'D']\nfor (i, col) in enumerate(eachcol(C1_basis))\n    annotate!(p1, col[1] + 0.1, col[2] + 0.1, text(c1_cats[i], :black, 8))\nend\n\nc2_cats = ['X', 'Y', 'Z']\nfor (i, col) in enumerate(eachcol(C2_basis))\n    annotate!(p2, col[1] + 0.1, col[2] + 0.1, text(string(c2_cats[i]), :black, 8))\nend\n\nplot(p1, p2, layout = (1, 2), size = (1000, 400))\n\nAs we can see, categories that were generated in a similar pattern were assigned similar vectors. In a dataset, where some columns have high cardinality, it's expected that some of the categories will exhibit similar patterns.","category":"section"},{"location":"common_workflows/entity_embeddings/notebook/#Transform-(embed)-data","page":"Entity Embeddings","title":"Transform (embed) data","text":"X_tr = MLJ.transform(mach, X);\nfirst(X_tr, 5)\n\nThis will transform each categorical value into its corresponding embedding vector. Continuous value will remain intact.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"common_workflows/composition/README/#Contents","page":"Contents","title":"Contents","text":"file description\nnotebook.ipynb Juptyer notebook (executed)\nnotebook.unexecuted.ipynb Jupyter notebook (unexecuted)\nnotebook.md static markdown (included in MLJFlux.jl docs)\nnotebook.jl executable Julia script annotated with comments\ngenerate.jl maintainers only: execute to generate MD and IPYNB from JL","category":"section"},{"location":"common_workflows/composition/README/#Important","page":"Contents","title":"Important","text":"Scripts or notebooks in this folder cannot be reliably executed without the accompanying Manifest.toml and Project.toml files. Relative to the directory of this README.md, you can try looking for these files at ../../../docs/","category":"section"},{"location":"common_workflows/incremental_training/notebook/#Incremental-Training-with-MLJFlux","page":"Incremental Training","title":"Incremental Training with MLJFlux","text":"This demonstration is available as a Jupyter notebook or julia script here.\n\nIn this workflow example we explore how to incrementally train MLJFlux models.\n\nJulia version is assumed to be 1.10.*","category":"section"},{"location":"common_workflows/incremental_training/notebook/#Basic-Imports","page":"Incremental Training","title":"Basic Imports","text":"using MLJ               # Has MLJFlux models\nusing Flux              # For more flexibility\nimport Optimisers       # native Flux.jl optimisers no longer supported\nusing StableRNGs        # for reproducibility across Julia versions\n\nstable_rng() = StableRNGs.StableRNG(123)","category":"section"},{"location":"common_workflows/incremental_training/notebook/#Loading-and-Splitting-the-Data","page":"Incremental Training","title":"Loading and Splitting the Data","text":"iris = load_iris() # a named-tuple of vectors\ny, X = unpack(iris, ==(:target), rng=stable_rng())\nX = fmap(column-> Float32.(column), X) # Flux prefers Float32 data\n(X_train, X_test), (y_train, y_test) = partition(\n    (X, y), 0.8,\n    multi = true,\n    shuffle = true,\n    rng=stable_rng(),\n);\nnothing #hide","category":"section"},{"location":"common_workflows/incremental_training/notebook/#Instantiating-the-model","page":"Incremental Training","title":"Instantiating the model","text":"Now let's construct our model. This follows a similar setup to the one followed in the Quick Start.\n\nNeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux\nclf = NeuralNetworkClassifier(\n    builder=MLJFlux.MLP(; hidden=(5,4), σ=Flux.relu),\n    optimiser=Optimisers.Adam(0.01),\n    batch_size=8,\n    epochs=10,\n    rng=stable_rng(),\n)","category":"section"},{"location":"common_workflows/incremental_training/notebook/#Initial-round-of-training","page":"Incremental Training","title":"Initial round of training","text":"Now let's train the model. Calling fit! will automatically train it for 100 epochs as specified above.\n\nmach = machine(clf, X_train, y_train)\nfit!(mach, verbosity=0)\n\nLet's evaluate the training loss and validation accuracy\n\ntraining_loss = cross_entropy(predict(mach, X_train), y_train)\n\nval_acc = accuracy(predict_mode(mach, X_test), y_test)\n\nPoor performance it seems.","category":"section"},{"location":"common_workflows/incremental_training/notebook/#Incremental-Training","page":"Incremental Training","title":"Incremental Training","text":"Now let's train it for another 30 epochs at half the original learning rate. All we need to do is changes these hyperparameters and call fit again. It won't reset the model parameters before training.\n\nclf.optimiser = Optimisers.Adam(clf.optimiser.eta/2)\nclf.epochs = clf.epochs + 30\nfit!(mach, verbosity=2);\nnothing #hide\n\nLet's evaluate the training loss and validation accuracy\n\ntraining_loss = cross_entropy(predict(mach, X_train), y_train)\n\ntraining_acc = accuracy(predict_mode(mach, X_test), y_test)\n\nThat's much better. If we are rather interested in resetting the model parameters before fitting, we can do fit(mach, force=true).\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"extended_examples/MNIST/notebook/#Using-MLJ-to-classifiy-the-MNIST-image-dataset","page":"MNIST Images","title":"Using MLJ to classifiy the MNIST image dataset","text":"This tutorial is available as a Jupyter notebook or julia script here.\n\nThis script tested using Julia 1.10\n\nusing MLJ\nusing Flux\nimport MLJFlux\nimport MLUtils\nimport MLJIteration # for `skip`\nusing StableRNGs\n\nstable_rng() = StableRNG(123)\n\nIf running on a GPU, you will also need to import CUDA and import cuDNN.\n\nusing Plots\ngr(size=(600, 300*(sqrt(5)-1)));\nnothing #hide","category":"section"},{"location":"extended_examples/MNIST/notebook/#Basic-training","page":"MNIST Images","title":"Basic training","text":"Downloading the MNIST image dataset:\n\nimport MLDatasets: MNIST\n\nENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true\nimages, labels = MNIST(split=:train)[:];\nnothing #hide\n\nIn MLJ, integers cannot be used for encoding categorical data, so we must force the labels to have the Multiclass scientific type. For more on this, see Working with Categorical Data.\n\nlabels = coerce(labels, Multiclass);\nimages = coerce(images, GrayImage);\nnothing #hide\n\nChecking scientific types:\n\n@assert scitype(images) <: AbstractVector{<:Image}\n@assert scitype(labels) <: AbstractVector{<:Finite}\n\nLooks good.\n\nFor general instructions on coercing image data, see Type coercion for image data\n\nimages[1]\n\nWe start by defining a suitable Builder object. This is a recipe for building the neural network. Our builder will work for images of any (constant) size, whether they be color or black and white (ie, single or multi-channel).  The architecture always consists of six alternating convolution and max-pool layers, and a final dense layer; the filter size and the number of channels after each convolution layer is customisable.\n\nimport MLJFlux\nstruct MyConvBuilder\n    filter_size::Int\n    channels1::Int\n    channels2::Int\n    channels3::Int\nend\n\nfunction MLJFlux.build(b::MyConvBuilder, rng, n_in, n_out, n_channels)\n    k, c1, c2, c3 = b.filter_size, b.channels1, b.channels2, b.channels3\n    mod(k, 2) == 1 || error(\"`filter_size` must be odd. \")\n    p = div(k - 1, 2) # padding to preserve image size\n    init = Flux.glorot_uniform(rng)\n    front = Chain(\n        Conv((k, k), n_channels => c1, pad=(p, p), relu, init=init),\n        MaxPool((2, 2)),\n        Conv((k, k), c1 => c2, pad=(p, p), relu, init=init),\n        MaxPool((2, 2)),\n        Conv((k, k), c2 => c3, pad=(p, p), relu, init=init),\n        MaxPool((2 ,2)),\n        MLUtils.flatten)\n    d = Flux.outputsize(front, (n_in..., n_channels, 1)) |> first\n    return Chain(front, Dense(d, n_out, init=init))\nend\n\nNotes.\n\nThere is no final softmax here, as this is applied by default in all MLJFLux classifiers. Customisation of this behaviour is controlled using using the finaliser hyperparameter of the classifier.\nInstead of calculating the padding p, Flux can infer the required padding in each dimension, which you enable by replacing pad = (p, p) with pad = SamePad().\n\nWe now define the MLJ model.\n\nImageClassifier = @load ImageClassifier pkg=MLJFlux\nclf = ImageClassifier(\n    builder=MyConvBuilder(3, 16, 32, 32),\n    batch_size=50,\n    epochs=10,\n    rng=stable_rng(),\n)\n\nYou can add Flux options optimiser=... and loss=... in the above constructor call. At present, loss must be a Flux-compatible loss, not an MLJ measure. To run on a GPU, add to the constructor acceleration=CUDALib() and omit rng.\n\nFor illustration purposes, we won't use all the data here:\n\ntrain = 1:500\ntest = 501:1000\n\nBinding the model with data in an MLJ machine:\n\nmach = machine(clf, images, labels);\nnothing #hide\n\nTraining for 10 epochs on the first 500 images:\n\nfit!(mach, rows=train, verbosity=2);\nnothing #hide\n\nInspecting:\n\nreport(mach)\n\nchain = fitted_params(mach)\n\nFlux.params(chain)[2]\n\nAdding 20 more epochs:\n\nclf.epochs = clf.epochs + 20\nfit!(mach, rows=train, verbosity=0);\nnothing #hide\n\nComputing an out-of-sample estimate of the loss:\n\npredicted_labels = predict(mach, rows=test);\ncross_entropy(predicted_labels, labels[test])\n\nOr to fit and predict, in one line:\n\nevaluate!(\n    mach,\n    resampling=Holdout(fraction_train=0.5),\n    measure=cross_entropy,\n    rows=1:1000,\n    verbosity=0,\n)\n\n(We could also have specified resampling=[(train, test),] and dropped the rows specifi ation.)","category":"section"},{"location":"extended_examples/MNIST/notebook/#Wrapping-the-MLJFlux-model-with-iteration-controls","page":"MNIST Images","title":"Wrapping the MLJFlux model with iteration controls","text":"Any iterative MLJFlux model can be wrapped in iteration controls, as we demonstrate next. For more on MLJ's IteratedModel wrapper, see the MLJ documentation.\n\nThe \"self-iterating\" classifier, called iterated_clf below, is for iterating the image classifier defined above until one of the following stopping criterion apply:\n\nPatience(3): 3 consecutive increases in the loss\nInvalidValue(): an out-of-sample loss, or a training loss, is NaN, Inf, or -Inf\nTimeLimit(t=5/60): training time has exceeded 5 minutes\n\nThese checks (and other controls) will be applied every two epochs (because of the Step(2) control). Additionally, training a machine bound to iterated_clf will:\n\nsave a snapshot of the machine every three control cycles (every six epochs)\nrecord traces of the out-of-sample loss and training losses for plotting\nrecord mean value traces of each Flux parameter for plotting\n\nFor a complete list of controls, see this table.","category":"section"},{"location":"extended_examples/MNIST/notebook/#Wrapping-the-classifier","page":"MNIST Images","title":"Wrapping the classifier","text":"Some helpers\n\nTo extract Flux params from an MLJFlux machine\n\nparameters(mach) = vec.(Flux.trainables(fitted_params(mach)))\n\nTo store the traces:\n\nlosses = []\ntraining_losses = []\nparameter_means = Float32[];\nepochs = []\n\nTo update the traces:\n\nupdate_loss(loss) = push!(losses, loss)\nupdate_training_loss(losses) = push!(training_losses, losses[end])\nupdate_means(mach) = append!(parameter_means, mean.(parameters(mach)));\nupdate_epochs(epoch) = push!(epochs, epoch)\n\nThe controls to apply:\n\nsave_control =\n    MLJIteration.skip(Save(joinpath(tempdir(), \"mnist.jls\")), predicate=3)\n\ncontrols=[\n    Step(2),\n    Patience(3),\n    InvalidValue(),\n    TimeLimit(5/60),\n    save_control,\n    WithLossDo(),\n    WithLossDo(update_loss),\n    WithTrainingLossesDo(update_training_loss),\n    Callback(update_means),\n    WithIterationsDo(update_epochs),\n];\nnothing #hide\n\nThe \"self-iterating\" classifier:\n\niterated_clf = IteratedModel(\n    clf,\n    controls=controls,\n    resampling=Holdout(fraction_train=0.7),\n    measure=log_loss,\n)","category":"section"},{"location":"extended_examples/MNIST/notebook/#Binding-the-wrapped-model-to-data:","page":"MNIST Images","title":"Binding the wrapped model to data:","text":"mach = machine(iterated_clf, images, labels);\nnothing #hide","category":"section"},{"location":"extended_examples/MNIST/notebook/#Training","page":"MNIST Images","title":"Training","text":"fit!(mach, rows=train);\nnothing #hide","category":"section"},{"location":"extended_examples/MNIST/notebook/#Comparison-of-the-training-and-out-of-sample-losses:","page":"MNIST Images","title":"Comparison of the training and out-of-sample losses:","text":"plot(\n    epochs,\n    losses,\n    xlab = \"epoch\",\n    ylab = \"cross entropy\",\n    label=\"out-of-sample\",\n)\nplot!(epochs, training_losses, label=\"training\")\n\nsavefig(joinpath(tempdir(), \"loss.png\"))","category":"section"},{"location":"extended_examples/MNIST/notebook/#Evolution-of-weights","page":"MNIST Images","title":"Evolution of weights","text":"n_epochs =  length(losses)\nn_parameters = div(length(parameter_means), n_epochs)\nparameter_means2 = reshape(copy(parameter_means), n_parameters, n_epochs)'\nplot(\n    epochs,\n    parameter_means2,\n    title=\"Flux parameter mean weights\",\n    xlab = \"epoch\",\n)\n\nNote. The higher the number in the plot legend, the deeper the layer we are **weight-averaging.\n\nsavefig(joinpath(tempdir(), \"weights.png\"))","category":"section"},{"location":"extended_examples/MNIST/notebook/#Retrieving-a-snapshot-for-a-prediction:","page":"MNIST Images","title":"Retrieving a snapshot for a prediction:","text":"mach2 = machine(joinpath(tempdir(), \"mnist3.jls\"))\npredict_mode(mach2, images[501:503])","category":"section"},{"location":"extended_examples/MNIST/notebook/#Restarting-training","page":"MNIST Images","title":"Restarting training","text":"Mutating iterated_clf.controls or clf.epochs (which is otherwise ignored) will allow you to restart training from where it left off.\n\niterated_clf.controls[2] = Patience(4)\nfit!(mach, rows=train)\n\nplot(\n    epochs,\n    losses,\n    xlab = \"epoch\",\n    ylab = \"cross entropy\",\n    label=\"out-of-sample\",\n)\nplot!(epochs, training_losses, label=\"training\")\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"interface/Multitarget Regression/#MLJFlux.MultitargetNeuralNetworkRegressor","page":"Multi-Target Regression","title":"MLJFlux.MultitargetNeuralNetworkRegressor","text":"MultitargetNeuralNetworkRegressor\n\nA model type for constructing a multitarget neural network regressor, based on MLJFlux.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nMultitargetNeuralNetworkRegressor = @load MultitargetNeuralNetworkRegressor pkg=MLJFlux\n\nDo model = MultitargetNeuralNetworkRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in MultitargetNeuralNetworkRegressor(builder=...).\n\nMultitargetNeuralNetworkRegressor is for training a data-dependent Flux.jl neural network to predict a multi-valued Continuous target, represented as a table, given a table of Continuous features. Users provide a recipe for constructing the network, based on properties of the data that is encountered, by specifying an appropriate builder. See MLJFlux documentation for more on builders.\n\nIn addition to features with Continuous scientific element type, this model supports categorical features in the input table. If present, such features are embedded into dense vectors by the use of an additional EntityEmbedderLayer layer after the input, as described in Entity Embeddings of Categorical Variables by Cheng Guo, Felix Berkhahn arXiv, 2016.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX provides input features and is either: (i) a Matrix with Continuous element scitype (typically Float32); or (ii) a table of input features (eg, a DataFrame) whose columns have Continuous, Multiclass or OrderedFactor element scitype; check column scitypes with schema(X).  If any Multiclass or OrderedFactor features appear, the constructed network will use an EntityEmbedderLayer layer to transform them into dense vectors. If X is a Matrix, it is assumed that columns correspond to features and rows corresponding to observations.\n\ny is the target, which can be any table or matrix of output targets whose element scitype is Continuous; check column scitypes with schema(y). If y is a Matrix, it is assumed to have columns corresponding to variables and rows corresponding to observations.\n\nHyper-parameters\n\nbuilder=MLJFlux.Linear(σ=Flux.relu): An MLJFlux builder that constructs a neural network. Possible builders include: Linear, Short, and MLP. See MLJFlux documentation for more on builders, and the example below for using the @builder convenience macro.\noptimiser::Optimisers.Adam(): An Optimisers.jl optimiser. The optimiser performs the updating of the weights of the network. To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at 10e-3, and tune using powers of 10 between 1 and 1e-7.\nloss=Flux.mse: The loss function which the network will optimize. Should be a function which can be called in the form loss(yhat, y).  Possible loss functions are listed in the Flux loss function documentation. For a regression task, natural loss functions are:\nFlux.mse\nFlux.mae\nFlux.msle\nFlux.huber_loss\nCurrently MLJ measures are not supported as loss functions here.\nepochs::Int=10: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.\nbatch_size::int=1: the batch size to be used for training, representing the number of samples per update of the network weights. Typically, batch size is between 8 and 512. Increassing batch size may accelerate training if acceleration=CUDALibs() and a GPU is available.\nlambda::Float64=0: The strength of the weight regularization penalty. Can be any value in the range [0, ∞). Note the history reports unpenalized losses.\nalpha::Float64=0: The L2/L1 mix of regularization, in the range [0, 1]. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.\nrng::Union{AbstractRNG, Int64}: The random number generator or seed used during training. The default is Random.default_rng().\noptimizer_changes_trigger_retraining::Bool=false: Defines what happens when re-fitting a machine if the associated optimiser has changed. If true, the associated machine will retrain from scratch on fit! call, otherwise it will not.\nacceleration::AbstractResource=CPU1(): Defines on what hardware training is done. For Training on GPU, use CUDALibs().\nembedding_dims: a Dict whose keys are names of categorical features, given as symbols, and whose values are numbers representing the desired dimensionality of the entity embeddings of such features: an integer value of 7, say, sets the embedding dimensionality to 7; a float value of 0.5, say, sets the embedding dimensionality to ceil(0.5 * c), where c is the number of feature levels.  Unspecified feature dimensionality defaults to min(c - 1, 10).\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given new features Xnew having the same scitype as X above. Predictions are deterministic.\ntransform(mach, Xnew): Assuming Xnew has the same schema as X, transform the categorical features of Xnew into dense Continuous vectors using the MLJFlux.EntityEmbedderLayer layer present in the network. Does nothing in case the model was trained on an input X that lacks categorical features.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nchain: The trained \"chain\" (Flux.jl model), namely the series of layers,  functions, and activations  which make up the neural network.\n\nReport\n\nThe fields of report(mach) are:\n\ntraining_losses: A vector of training losses (penalised if lambda != 0) in  historical order, of length epochs + 1.  The first element is the pre-training loss.\n\nExamples\n\nIn this example we apply a multi-target regression model to synthetic data:\n\nusing MLJ\nimport MLJFlux\nusing Flux\nimport Optimisers\n\nFirst, we generate some synthetic data (needs MLJBase 0.20.16 or higher):\n\nX, y = make_regression(100, 9; n_targets = 2) # both tables\nschema(y)\nschema(X)\n\nSplitting off a test set:\n\n(X, Xtest), (y, ytest) = partition((X, y), 0.7, multi=true);\n\nNext, we can define a builder, making use of a convenience macro to do so.  In the following @builder call, n_in is a proxy for the number input features and n_out the number of target variables (both known at fit! time), while rng is a proxy for a RNG (which will be passed from the rng field of model defined below).\n\nbuilder = MLJFlux.@builder begin\n    init=Flux.glorot_uniform(rng)\n    Chain(\n        Dense(n_in, 64, relu, init=init),\n        Dense(64, 32, relu, init=init),\n        Dense(32, n_out, init=init),\n    )\nend\n\nInstantiating the regression model:\n\nMultitargetNeuralNetworkRegressor = @load MultitargetNeuralNetworkRegressor\nmodel = MultitargetNeuralNetworkRegressor(builder=builder, rng=123, epochs=20)\n\nWe will arrange for standardization of the the target by wrapping our model in  TransformedTargetModel, and standardization of the features by inserting the wrapped  model in a pipeline:\n\npipe = Standardizer |> TransformedTargetModel(model, transformer=Standardizer)\n\nIf we fit with a high verbosity (>1), we will see the losses during training. We can also see the losses in the output of report(mach)\n\nmach = machine(pipe, X, y)\nfit!(mach, verbosity=2)\n\n# first element initial loss, 2:end per epoch training losses\nreport(mach).transformed_target_model_deterministic.model.training_losses\n\nFor experimenting with learning rate, see the NeuralNetworkRegressor example.\n\npipe.transformed_target_model_deterministic.model.optimiser = Optimisers.Adam(0.0001)\n\nWith the learning rate fixed, we can now compute a CV estimate of the performance (using all data bound to mach) and compare this with performance on the test set:\n\n\n# CV estimate, based on `(X, y)`:\nevaluate!(mach, resampling=CV(nfolds=5), measure=multitarget_l2)\n\n# loss for `(Xtest, test)`:\nfit!(mach) # trains on all data `(X, y)`\nyhat = predict(mach, Xtest)\nmultitarget_l2(yhat, ytest)\n\nSee also NeuralNetworkRegressor\n\n\n\n\n\n","category":"type"},{"location":"common_workflows/early_stopping/notebook/#Early-Stopping-with-MLJ","page":"Early Stopping","title":"Early Stopping with MLJ","text":"This demonstration is available as a Jupyter notebook or julia script here.\n\nIn this workflow example, we learn how MLJFlux enables us to easily use early stopping when training MLJFlux models.\n\nThis script tested using Julia 1.10","category":"section"},{"location":"common_workflows/early_stopping/notebook/#Basic-Imports","page":"Early Stopping","title":"Basic Imports","text":"using MLJ               # Has MLJFlux models\nusing Flux              # For more flexibility\nusing Plots             # To visualize training\nimport Optimisers       # native Flux.jl optimisers no longer supported\nusing StableRNGs        # for reproducibility across Julia versions\n\nstable_rng() = StableRNGs.StableRNG(123)","category":"section"},{"location":"common_workflows/early_stopping/notebook/#Loading-and-Splitting-the-Data","page":"Early Stopping","title":"Loading and Splitting the Data","text":"iris = load_iris() # a named-tuple of vectors\ny, X = unpack(iris, ==(:target), rng=stable_rng())\nX = fmap(column-> Float32.(column), X) # Flux prefers Float32 data","category":"section"},{"location":"common_workflows/early_stopping/notebook/#Instantiating-the-model-Now-let's-construct-our-model.-This-follows-a-similar-setup","page":"Early Stopping","title":"Instantiating the model Now let's construct our model. This follows a similar setup","text":"to the one followed in the Quick Start.\n\nNeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux\n\nclf = NeuralNetworkClassifier(\n    builder=MLJFlux.MLP(; hidden=(5,4), σ=Flux.relu),\n    optimiser=Optimisers.Adam(0.01),\n    batch_size=8,\n    epochs=50,\n    rng=stable_rng(),\n)","category":"section"},{"location":"common_workflows/early_stopping/notebook/#Wrapping-it-in-an-IteratedModel","page":"Early Stopping","title":"Wrapping it in an IteratedModel","text":"Let's start by defining the condition that can cause the model to early stop.\n\nstop_conditions = [\n    Step(1),            # Repeatedly train for one iteration\n    NumberLimit(100),   # Don't train for more than 100 iterations\n    Patience(5),        # Stop after 5 iterations of disimprovement in validation loss\n    NumberSinceBest(9), # Or if the best loss occurred 9 iterations ago\n    TimeLimit(30/60),   # Or if 30 minutes passed\n]\n\nWe can also define callbacks. Here we want to store the validation loss for each iteration\n\nvalidation_losses = []\ncallbacks = [\n    WithLossDo(loss->push!(validation_losses, loss)),\n]\n\nConstruct the iterated model and pass to it the stop_conditions and the callbacks:\n\niterated_model = IteratedModel(\n    model=clf,\n    resampling=Holdout(fraction_train=0.7); # loss and stopping are based on out-of-sample\n    measures=log_loss,\n    iteration_parameter=:(epochs),\n    controls=vcat(stop_conditions, callbacks),\n    retrain=false            # no need to retrain on all data at the end\n);\nnothing #hide\n\nYou can see more advanced stopping conditions as well as how to involve callbacks in the documentation","category":"section"},{"location":"common_workflows/early_stopping/notebook/#Training-with-Early-Stopping","page":"Early Stopping","title":"Training with Early Stopping","text":"At this point, all we need is to fit the model and iteration controls will be automatically handled\n\nmach = machine(iterated_model, X, y)\nfit!(mach)\n# We can get the training losses like so\ntraining_losses = report(mach).model_report.training_losses;\nnothing #hide","category":"section"},{"location":"common_workflows/early_stopping/notebook/#Results","page":"Early Stopping","title":"Results","text":"We can see that the model converged after 100 iterations.\n\nplot(training_losses, label=\"Training Loss\", linewidth=2)\nplot!(validation_losses, label=\"Validation Loss\", linewidth=2, size=(800,400))\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"common_workflows/early_stopping/README/#Contents","page":"Contents","title":"Contents","text":"file description\nnotebook.ipynb Juptyer notebook (executed)\nnotebook.unexecuted.ipynb Jupyter notebook (unexecuted)\nnotebook.md static markdown (included in MLJFlux.jl docs)\nnotebook.jl executable Julia script annotated with comments\ngenerate.jl maintainers only: execute to generate MD and IPYNB from JL","category":"section"},{"location":"common_workflows/early_stopping/README/#Important","page":"Contents","title":"Important","text":"Scripts or notebooks in this folder cannot be reliably executed without the accompanying Manifest.toml and Project.toml files. Relative to the directory of this README.md, you can try looking for these files at ../../../docs/","category":"section"},{"location":"common_workflows/live_training/README/#Contents","page":"Contents","title":"Contents","text":"file description\nnotebook.unexecuted.ipynb Jupyter notebook (unexecuted)\nnotebook.md static markdown (included in MLJFlux.jl docs)\nnotebook.jl executable Julia script annotated with comments\ngenerate.jl maintainers only: execute to generate MD and IPYNB from JL","category":"section"},{"location":"common_workflows/live_training/README/#Important","page":"Contents","title":"Important","text":"Scripts or notebooks in this folder cannot be reliably executed without the accompanying Manifest.toml and Project.toml files. Relative to the directory of this README.md, you can try looking for these files at ../../../docs/","category":"section"},{"location":"common_workflows/hyperparameter_tuning/README/#Contents","page":"Contents","title":"Contents","text":"file description\nnotebook.ipynb Juptyer notebook (executed)\nnotebook.unexecuted.ipynb Jupyter notebook (unexecuted)\nnotebook.md static markdown (included in MLJFlux.jl docs)\nnotebook.jl executable Julia script annotated with comments\ngenerate.jl maintainers only: execute to generate MD and IPYNB from JL\n ","category":"section"},{"location":"common_workflows/hyperparameter_tuning/README/#Important","page":"Contents","title":"Important","text":"Scripts or notebooks in this folder cannot be reliably executed without the accompanying Manifest.toml and Project.toml files. Relative to the directory of this README.md, you can try looking for these files at ../../../docs/","category":"section"},{"location":"extended_examples/MNIST/README/#Contents","page":"Contents","title":"Contents","text":"file description\nnotebook.unexecuted.ipynb Jupyter notebook (unexecuted)\nnotebook.md static markdown (included in MLJFlux.jl docs)\nnotebook.jl executable Julia script annotated with comments\ngenerate.jl maintainers only: execute to generate MD and IPYNB from JL\n ","category":"section"},{"location":"extended_examples/MNIST/README/#Important","page":"Contents","title":"Important","text":"Scripts or notebooks in this folder cannot be reliably executed without the accompanying Manifest.toml and Project.toml files. Relative to the directory of this README.md, you can try looking for these files at ../../../docs/","category":"section"},{"location":"common_workflows/composition/notebook/#Model-Composition-with-MLJFlux","page":"Model Composition","title":"Model Composition with MLJFlux","text":"This demonstration is available as a Jupyter notebook or julia script here.\n\nIn this workflow example, we see how MLJFlux enables composing MLJ models with MLJFlux models. We will assume a class imbalance setting and wrap an oversampler with a deep learning model from MLJFlux.\n\nThis script tested using Julia 1.10","category":"section"},{"location":"common_workflows/composition/notebook/#Basic-Imports","page":"Model Composition","title":"Basic Imports","text":"using MLJ               # Has MLJFlux models\nusing Flux              # For more flexibility\nimport Random           # To create imbalance\nimport Imbalance        # To solve the imbalance\nimport Optimisers       # native Flux.jl optimisers no longer supported\nusing StableRNGs        # for reproducibility across Julia versions\nimport CategoricalArrays.unwrap\n\nstable_rng() = StableRNGs.StableRNG(123)","category":"section"},{"location":"common_workflows/composition/notebook/#Loading-and-Splitting-the-Data","page":"Model Composition","title":"Loading and Splitting the Data","text":"iris = load_iris() # a named-tuple of vectors\ny, X = unpack(iris, ==(:target), rng=stable_rng())\nX = fmap(column-> Float32.(column), X) # Flux prefers Float32 data\n\nThe iris dataset has a target with uniformly distributed values, \"versicolor\", \"setosa\", and \"virginica\". To manufacture an unbalanced dataset, we'll combine the first two into a single classs, \"colosa\":\n\ny = coerce(\n        map(y) do species\n            species == \"virginica\" ? unwrap(species) : \"colosa\"\n        end,\n        Multiclass,\n);\nImbalance.checkbalance(y)","category":"section"},{"location":"common_workflows/composition/notebook/#Instantiating-the-model","page":"Model Composition","title":"Instantiating the model","text":"Let's load BorderlineSMOTE1 to oversample the data and Standardizer to standardize it.\n\nBorderlineSMOTE1 = @load BorderlineSMOTE1 pkg=Imbalance verbosity=0\nNeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux\n\nWe didn't need to load Standardizer because it is a local model for MLJ (see localmodels())\n\nclf = NeuralNetworkClassifier(\n    builder=MLJFlux.MLP(; hidden=(5,4), σ=Flux.relu),\n    optimiser=Optimisers.Adam(0.01),\n    batch_size=8,\n    epochs=50,\n    rng=stable_rng(),\n)\n\nFirst we wrap the oversampler with the neural network via the BalancedModel construct. This comes from MLJBalancing And allows combining resampling methods with MLJ models in a sequential pipeline.\n\noversampler = BorderlineSMOTE1(k=5, ratios=1.0, rng=stable_rng())\nbalanced_model = BalancedModel(model=clf, balancer1=oversampler)\nstandarizer = Standardizer()\n\nNow let's compose the balanced model with a standardizer.\n\npipeline = standarizer |> balanced_model\n\nBy this, any training data will be standardized then oversampled then passed to the model. Meanwhile, for inference, the standardizer will automatically use the training set's mean and std and the oversampler will be play no role.","category":"section"},{"location":"common_workflows/composition/notebook/#Training-the-Composed-Model","page":"Model Composition","title":"Training the Composed Model","text":"The pipeline model can be evaluated like any other model:\n\nmach = machine(pipeline, X, y)\nfit!(mach)\ncv=CV(nfolds=5)\nevaluate!(mach, resampling=cv, measure=accuracy)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"interface/Regression/#MLJFlux.NeuralNetworkRegressor","page":"Regression","title":"MLJFlux.NeuralNetworkRegressor","text":"NeuralNetworkRegressor\n\nA model type for constructing a neural network regressor, based on MLJFlux.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nNeuralNetworkRegressor = @load NeuralNetworkRegressor pkg=MLJFlux\n\nDo model = NeuralNetworkRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in NeuralNetworkRegressor(builder=...).\n\nNeuralNetworkRegressor is for training a data-dependent Flux.jl neural network to predict a Continuous target, given a table of Continuous features. Users provide a recipe for constructing the network, based on properties of the data that is encountered, by specifying an appropriate builder. See MLJFlux documentation for more on builders.\n\nIn addition to features with Continuous scientific element type, this model supports categorical features in the input table. If present, such features are embedded into dense vectors by the use of an additional EntityEmbedderLayer layer after the input, as described in Entity Embeddings of Categorical Variables by Cheng Guo, Felix Berkhahn arXiv, 2016.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX provides input features and is either: (i) a Matrix with Continuous element scitype (typically Float32); or (ii) a table of input features (eg, a DataFrame) whose columns have Continuous, Multiclass or OrderedFactor element scitype; check column scitypes with schema(X).  If any Multiclass or OrderedFactor features appear, the constructed network will use an EntityEmbedderLayer layer to transform them into dense vectors. If X is a Matrix, it is assumed that columns correspond to features and rows corresponding to observations.\n\ny is the target, which can be any AbstractVector whose element scitype is Continuous; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).\n\nHyper-parameters\n\nbuilder=MLJFlux.Linear(σ=Flux.relu): An MLJFlux builder that constructs a neural  network. Possible builders include: MLJFlux.Linear, MLJFlux.Short, and  MLJFlux.MLP. See MLJFlux documentation for more on builders, and the example below  for using the @builder convenience macro.\noptimiser::Optimisers.Adam(): An Optimisers.jl optimiser. The optimiser performs the updating of the weights of the network. To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at 10e-3, and tune using powers of 10 between 1 and 1e-7.\nloss=Flux.mse: The loss function which the network will optimize. Should be a function which can be called in the form loss(yhat, y).  Possible loss functions are listed in the Flux loss function documentation. For a regression task, natural loss functions are:\nFlux.mse\nFlux.mae\nFlux.msle\nFlux.huber_loss\nCurrently MLJ measures are not supported as loss functions here.\nepochs::Int=10: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.\nbatch_size::int=1: the batch size to be used for training, representing the number of samples per update of the network weights. Typically, batch size is between 8 and 512. Increasing batch size may accelerate training if acceleration=CUDALibs() and a GPU is available.\nlambda::Float64=0: The strength of the weight regularization penalty. Can be any value in the range [0, ∞). Note the history reports unpenalized losses.\nalpha::Float64=0: The L2/L1 mix of regularization, in the range [0, 1]. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.\nrng::Union{AbstractRNG, Int64}: The random number generator or seed used during training. The default is Random.default_rng().\noptimizer_changes_trigger_retraining::Bool=false: Defines what happens when re-fitting a machine if the associated optimiser has changed. If true, the associated machine will retrain from scratch on fit! call, otherwise it will not.\nacceleration::AbstractResource=CPU1(): Defines on what hardware training is done. For Training on GPU, use CUDALibs().\nembedding_dims: a Dict whose keys are names of categorical features, given as symbols, and whose values are numbers representing the desired dimensionality of the entity embeddings of such features: an integer value of 7, say, sets the embedding dimensionality to 7; a float value of 0.5, say, sets the embedding dimensionality to ceil(0.5 * c), where c is the number of feature levels.  Unspecified feature dimensionality defaults to min(c - 1, 10).\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given new features Xnew, which should have the same scitype as X above.\ntransform(mach, Xnew): Assuming Xnew has the same schema as X, transform the categorical features of Xnew into dense Continuous vectors using the MLJFlux.EntityEmbedderLayer layer present in the network. Does nothing in case the model was trained on an input X that lacks categorical features.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nchain: The trained \"chain\" (Flux.jl model), namely the series of layers, functions,  and activations which make up the neural network.\n\nReport\n\nThe fields of report(mach) are:\n\ntraining_losses: A vector of training losses (penalized if lambda != 0) in  historical order, of length epochs + 1.  The first element is the pre-training loss.\n\nExamples\n\nIn this example we build a regression model for the Boston house price dataset.\n\nusing MLJ\nimport MLJFlux\nusing Flux\nimport Optimisers\n\nFirst, we load in the data: The :MEDV column becomes the target vector y, and all remaining columns go into a table X, with the exception of :CHAS:\n\ndata = OpenML.load(531); # Loads from https://www.openml.org/d/531\ny, X = unpack(data, ==(:MEDV), !=(:CHAS); rng=123);\n\nscitype(y)\nschema(X)\n\nSince MLJFlux models do not handle ordered factors, we'll treat :RAD as Continuous:\n\nX = coerce(X, :RAD=>Continuous)\n\nSplitting off a test set:\n\n(X, Xtest), (y, ytest) = partition((X, y), 0.7, multi=true);\n\nNext, we can define a builder, making use of a convenience macro to do so.  In the following @builder call, n_in is a proxy for the number input features (which will be known at fit! time) and rng is a proxy for a RNG (which will be passed from the rng field of model defined below). We also have the parameter n_out which is the number of output features. As we are doing single target regression, the value passed will always be 1, but the builder we define will also work for MultitargetNeuralNetworkRegressor.\n\nbuilder = MLJFlux.@builder begin\n    init=Flux.glorot_uniform(rng)\n    Chain(\n        Dense(n_in, 64, relu, init=init),\n        Dense(64, 32, relu, init=init),\n        Dense(32, n_out, init=init),\n    )\nend\n\nInstantiating a model:\n\nNeuralNetworkRegressor = @load NeuralNetworkRegressor pkg=MLJFlux\nmodel = NeuralNetworkRegressor(\n    builder=builder,\n    rng=123,\n    epochs=20\n)\n\nWe arrange for standardization of the the target by wrapping our model in TransformedTargetModel, and standardization of the features by inserting the wrapped model in a pipeline:\n\npipe = Standardizer |> TransformedTargetModel(model, transformer=Standardizer)\n\nIf we fit with a high verbosity (>1), we will see the losses during training. We can also see the losses in the output of report(mach).\n\nmach = machine(pipe, X, y)\nfit!(mach, verbosity=2)\n\n# first element initial loss, 2:end per epoch training losses\nreport(mach).transformed_target_model_deterministic.model.training_losses\n\nExperimenting with learning rate\n\nWe can visually compare how the learning rate affects the predictions:\n\nusing Plots\n\nrates = rates = [5e-5, 1e-4, 0.005, 0.001, 0.05]\nplt=plot()\n\nforeach(rates) do η\n  pipe.transformed_target_model_deterministic.model.optimiser = Optimisers.Adam(η)\n  fit!(mach, force=true, verbosity=0)\n  losses =\n      report(mach).transformed_target_model_deterministic.model.training_losses[3:end]\n  plot!(1:length(losses), losses, label=η)\nend\n\nplt\n\npipe.transformed_target_model_deterministic.model.optimiser.eta = Optimisers.Adam(0.0001)\n\nWith the learning rate fixed, we compute a CV estimate of the performance (using all data bound to mach) and compare this with performance on the test set:\n\n# CV estimate, based on `(X, y)`:\nevaluate!(mach, resampling=CV(nfolds=5), measure=l2)\n\n# loss for `(Xtest, test)`:\nfit!(mach) # train on `(X, y)`\nyhat = predict(mach, Xtest)\nl2(yhat, ytest)\n\nThese losses, for the pipeline model, refer to the target on the original, unstandardized, scale.\n\nFor implementing stopping criterion and other iteration controls, refer to examples linked from the MLJFlux documentation.\n\nSee also MultitargetNeuralNetworkRegressor\n\n\n\n\n\n","category":"type"},{"location":"common_workflows/comparison/README/#Contents","page":"Contents","title":"Contents","text":"file description\nnotebook.ipynb Juptyer notebook (executed)\nnotebook.unexecuted.ipynb Jupyter notebook (unexecuted)\nnotebook.md static markdown (included in MLJFlux.jl docs)\nnotebook.jl executable Julia script annotated with comments\ngenerate.jl maintainers only: execute to generate MD and IPYNB from JL","category":"section"},{"location":"common_workflows/comparison/README/#Important","page":"Contents","title":"Important","text":"Scripts or notebooks in this folder cannot be reliably executed without the accompanying Manifest.toml and Project.toml files. Relative to the directory of this README.md, you can try looking for these files at ../../../docs/","category":"section"},{"location":"interface/Builders/#MLJFlux.Linear","page":"Builders","title":"MLJFlux.Linear","text":"Linear(; σ=Flux.relu)\n\nMLJFlux builder that constructs a fully connected two layer network with activation function σ. The number of input and output nodes is determined from the data. Weights are initialized using Flux.glorot_uniform(rng), where rng is inferred from the rng field of the MLJFlux model.\n\n\n\n\n\n","category":"type"},{"location":"interface/Builders/#MLJFlux.Short","page":"Builders","title":"MLJFlux.Short","text":"Short(; n_hidden=0, dropout=0.5, σ=Flux.sigmoid)\n\nMLJFlux builder that constructs a full-connected three-layer network using n_hidden nodes in the hidden layer and the specified dropout (defaulting to 0.5). An activation function σ is applied between the hidden and final layers. If n_hidden=0 (the default) then n_hidden is the geometric mean of the number of input and output nodes.  The number of input and output nodes is determined from the data.\n\nEach layer is initialized using Flux.glorot_uniform(rng), where rng is inferred from the rng field of the MLJFlux model.\n\n\n\n\n\n","category":"type"},{"location":"interface/Builders/#MLJFlux.MLP","page":"Builders","title":"MLJFlux.MLP","text":"MLP(; hidden=(100,), σ=Flux.relu)\n\nMLJFlux builder that constructs a Multi-layer perceptron network. The ith element of hidden represents the number of neurons in the ith hidden layer. An activation function σ is applied between each layer.\n\nEach layer is initialized using Flux.glorot_uniform(rng), where rng is inferred from the rng field of the MLJFlux model.\n\n\n\n\n\n","category":"type"},{"location":"interface/Builders/#MLJFlux.@builder","page":"Builders","title":"MLJFlux.@builder","text":"@builder neural_net\n\nCreates a builder for neural_net. The variables rng, n_in, n_out and n_channels can be used to create builders for any random number generator rng, input and output sizes n_in and n_out and number of input channels n_channels.\n\nExamples\n\njulia> import MLJFlux: @builder;\n\njulia> nn = NeuralNetworkRegressor(builder = @builder(Chain(Dense(n_in, 64, relu),\n                                                            Dense(64, 32, relu),\n                                                            Dense(32, n_out))));\n\njulia> conv_builder = @builder begin\n           front = Chain(Conv((3, 3), n_channels => 16), Flux.flatten)\n           d = Flux.outputsize(front, (n_in..., n_channels, 1)) |> first\n           Chain(front, Dense(d, n_out));\n       end\n\njulia> conv_nn = NeuralNetworkRegressor(builder = conv_builder);\n\n\n\n\n\n","category":"macro"},{"location":"common_workflows/architecture_search/notebook/#Neural-Architecture-Search-with-MLJFlux","page":"Neural Architecture Search","title":"Neural Architecture Search with MLJFlux","text":"This demonstration is available as a Jupyter notebook or julia script here.\n\nNeural Architecture Search (NAS) is an instance of hyperparameter tuning concerned with tuning model hyperparameters defining the architecture itself. Although it's typically performed with sophisticated search algorithms for efficiency, in this example we will be using a simple random search.\n\nThis script tested using Julia 1.10","category":"section"},{"location":"common_workflows/architecture_search/notebook/#Basic-Imports","page":"Neural Architecture Search","title":"Basic Imports","text":"using MLJ               # Has MLJFlux models\nusing Flux              # For more flexibility\nusing DataFrames        # To view tuning results in a table\nimport Optimisers       # native Flux.jl optimisers no longer supported\nusing StableRNGs        # for reproducibility across Julia versions\n\nstable_rng() = StableRNGs.StableRNG(123)","category":"section"},{"location":"common_workflows/architecture_search/notebook/#Loading-and-Splitting-the-Data","page":"Neural Architecture Search","title":"Loading and Splitting the Data","text":"iris = load_iris() # a named-tuple of vectors\ny, X = unpack(iris, ==(:target), rng=stable_rng())\nX = fmap(column-> Float32.(column), X) # Flux prefers Float32 data","category":"section"},{"location":"common_workflows/architecture_search/notebook/#Instantiating-the-model","page":"Neural Architecture Search","title":"Instantiating the model","text":"Now let's construct our model. This follows a similar setup the one followed in the Quick Start.\n\nNeuralNetworkClassifier = @load NeuralNetworkClassifier pkg = \"MLJFlux\"\nclf = NeuralNetworkClassifier(\n    builder = MLJFlux.MLP(; hidden = (1, 1, 1), σ = Flux.relu),\n    optimiser = Optimisers.ADAM(0.01),\n    batch_size = 8,\n    epochs = 10,\n    rng = stable_rng(),\n)","category":"section"},{"location":"common_workflows/architecture_search/notebook/#Generating-Network-Architectures","page":"Neural Architecture Search","title":"Generating Network Architectures","text":"We know that the MLP builder takes a tuple of the form (z_1 z_2  z_k) to define a network with k hidden layers and where the ith layer has z_i neurons. We will proceed by defining a function that can generate all possible networks with a specific number of hidden layers, a minimum and maximum number of neurons per layer and increments to consider for the number of neurons.\n\nfunction generate_networks(\n    ;min_neurons::Int,\n    max_neurons::Int,\n    neuron_step::Int,\n    num_layers::Int,\n    )\n    # Define the range of neurons\n    neuron_range = min_neurons:neuron_step:max_neurons\n\n    # Empty list to store the network configurations\n    networks = Vector{Tuple{Vararg{Int, num_layers}}}()\n\n    # Recursive helper function to generate all combinations of tuples\n    function generate_tuple(current_layers, remaining_layers)\n        if remaining_layers > 0\n            for n in neuron_range\n                # current_layers =[] then current_layers=[(min_neurons)],\n                # [(min_neurons+neuron_step)], [(min_neurons+2*neuron_step)],...\n                # for each of these we call generate_layers again which appends\n                # the n combinations for each one of them\n                generate_tuple(vcat(current_layers, [n]), remaining_layers - 1)\n            end\n        else\n            # in the base case, no more layers to \"recurse on\"\n            # and we just append the current_layers as a tuple\n            push!(networks, tuple(current_layers...))\n        end\n    end\n\n    # Generate networks for the given number of layers\n    generate_tuple([], num_layers)\n\n    return networks\nend\n\nNow let's generate an array of all possible neural networks with three hidden layers and number of neurons per layer ∈ [1,64] with a step of 4\n\nnetworks_space =\n    generate_networks(\n        min_neurons = 1,\n        max_neurons = 64,\n        neuron_step = 4,\n        num_layers = 3,\n    )\n\nnetworks_space[1:5]","category":"section"},{"location":"common_workflows/architecture_search/notebook/#Wrapping-the-Model-for-Tuning","page":"Neural Architecture Search","title":"Wrapping the Model for Tuning","text":"Let's use this array to define the range of hyperparameters and pass it along with the model to the TunedModel constructor.\n\nr1 = range(clf, :(builder.hidden), values = networks_space)\n\ntuned_clf = TunedModel(\n    model = clf,\n    tuning = RandomSearch(),\n    resampling = CV(nfolds = 4, rng = 42),\n    range = [r1],\n    measure = cross_entropy,\n    n = 100,             # searching over 100 random samples are enough\n);\nnothing #hide","category":"section"},{"location":"common_workflows/architecture_search/notebook/#Performing-the-Search","page":"Neural Architecture Search","title":"Performing the Search","text":"Similar to the last workflow example, all we need now is to fit our model and the search will take place automatically:\n\nmach = machine(tuned_clf, X, y);\nfit!(mach, verbosity = 0);\nfitted_params(mach).best_model","category":"section"},{"location":"common_workflows/architecture_search/notebook/#Analyzing-the-Search-Results","page":"Neural Architecture Search","title":"Analyzing the Search Results","text":"Let's analyze the search results by converting the history array to a dataframe and viewing it:\n\nhistory = report(mach).history;\nhistory_df = DataFrame(\n    mlp = [x[:model].builder for x in history],\n    measurement = [x[:measurement][1] for x in history],\n)\nfirst(sort!(history_df, [order(:measurement)]), 10)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"interface/Image Classification/#MLJFlux.ImageClassifier","page":"Image Classification","title":"MLJFlux.ImageClassifier","text":"ImageClassifier\n\nA model type for constructing a image classifier, based on MLJFlux.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nImageClassifier = @load ImageClassifier pkg=MLJFlux\n\nDo model = ImageClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ImageClassifier(builder=...).\n\nImageClassifier classifies images using a neural network adapted to the type of images provided (color or gray scale). Predictions are probabilistic. Users provide a recipe for constructing the network, based on properties of the image encountered, by specifying an appropriate builder. See MLJFlux documentation for more on builders.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX is any AbstractVector of images with ColorImage or GrayImage scitype; check  the scitype with scitype(X) and refer to ScientificTypes.jl documentation on coercing  typical image formats into an appropriate type.\ny is the target, which can be any AbstractVector whose element  scitype is Multiclass; check the scitype with scitype(y).\n\nTrain the machine with fit!(mach, rows=...).\n\nHyper-parameters\n\nbuilder: An MLJFlux builder that constructs the neural network.  The fallback builds a  depth-16 VGG architecture adapted to the image size and number of target classes, with  no batch normalization; see the Metalhead.jl documentation for details. See the example  below for a user-specified builder. A convenience macro @builder is also  available. See also finaliser below.\noptimiser::Optimisers.Adam(): An Optimisers.jl optimiser. The optimiser performs the updating of the weights of the network. To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at 10e-3, and tune using powers of 10 between 1 and 1e-7.\nloss=Flux.crossentropy: The loss function which the network will optimize. Should be a function which can be called in the form loss(yhat, y).  Possible loss functions are listed in the Flux loss function documentation. For a classification task, the most natural loss functions are:\nFlux.crossentropy: Standard multiclass classification loss, also known as the log loss.\nFlux.logitcrossentopy: Mathematically equal to crossentropy, but numerically more stable than finalising the outputs with softmax and then calculating crossentropy. You will need to specify finaliser=identity to remove MLJFlux's default softmax finaliser, and understand that the output of predict is then unnormalized (no longer probabilistic).\nFlux.tversky_loss: Used with imbalanced data to give more weight to false negatives.\nFlux.focal_loss: Used with highly imbalanced data. Weights harder examples more than easier examples.\nCurrently MLJ measures are not supported values of loss.\nepochs::Int=10: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.\nbatch_size::int=1: the batch size to be used for training, representing the number of samples per update of the network weights. Typically, batch size is between 8 and\nIncreassing batch size may accelerate training if acceleration=CUDALibs() and a\nGPU is available.\nlambda::Float64=0: The strength of the weight regularization penalty. Can be any value in the range [0, ∞). Note the history reports unpenalized losses.\nalpha::Float64=0: The L2/L1 mix of regularization, in the range [0, 1]. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.\nrng::Union{AbstractRNG, Int64}: The random number generator or seed used during training. The default is Random.default_rng().\noptimizer_changes_trigger_retraining::Bool=false: Defines what happens when re-fitting a machine if the associated optimiser has changed. If true, the associated machine will retrain from scratch on fit! call, otherwise it will not.\nacceleration::AbstractResource=CPU1(): Defines on what hardware training is done. For Training on GPU, use CUDALibs().\nfinaliser=Flux.softmax: The final activation function of the neural network (applied after the network defined by builder). Defaults to Flux.softmax.\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given new features Xnew, which should have the same scitype as X above. Predictions are probabilistic but uncalibrated.\npredict_mode(mach, Xnew): Return the modes of the probabilistic predictions returned above.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nchain: The trained \"chain\" (Flux.jl model), namely the series of layers,  functions, and activations  which make up the neural network. This includes  the final layer specified by finaliser (eg, softmax).\n\nReport\n\nThe fields of report(mach) are:\n\ntraining_losses: A vector of training losses (penalised if lambda != 0) in  historical order, of length epochs + 1.  The first element is the pre-training loss.\n\nExamples\n\nIn this example we use MLJFlux and a custom builder to classify the MNIST image dataset.\n\nusing MLJ\nusing Flux\nimport MLJFlux\nimport Optimisers\nimport MLJIteration # for `skip` control\n\nFirst we want to download the MNIST dataset, and unpack into images and labels:\n\nimport MLDatasets: MNIST\ndata = MNIST(split=:train)\nimages, labels = data.features, data.targets\n\nIn MLJ, integers cannot be used for encoding categorical data, so we must coerce them into the Multiclass scitype:\n\nlabels = coerce(labels, Multiclass);\n\nAbove images is a single array but MLJFlux requires the images to be a vector of individual image arrays:\n\nimages = coerce(images, GrayImage);\nimages[1]\n\nWe start by defining a suitable builder object. This is a recipe for building the neural network. Our builder will work for images of any (constant) size, whether they be color or black and white (ie, single or multi-channel).  The architecture always consists of six alternating convolution and max-pool layers, and a final dense layer; the filter size and the number of channels after each convolution layer is customizable.\n\nimport MLJFlux\n\nstruct MyConvBuilder\n    filter_size::Int\n    channels1::Int\n    channels2::Int\n    channels3::Int\nend\n\nmake2d(x::AbstractArray) = reshape(x, :, size(x)[end])\n\nfunction MLJFlux.build(b::MyConvBuilder, rng, n_in, n_out, n_channels)\n    k, c1, c2, c3 = b.filter_size, b.channels1, b.channels2, b.channels3\n    mod(k, 2) == 1 || error(\"`filter_size` must be odd. \")\n    p = div(k - 1, 2) # padding to preserve image size\n    init = Flux.glorot_uniform(rng)\n    front = Chain(\n        Conv((k, k), n_channels => c1, pad=(p, p), relu, init=init),\n        MaxPool((2, 2)),\n        Conv((k, k), c1 => c2, pad=(p, p), relu, init=init),\n        MaxPool((2, 2)),\n        Conv((k, k), c2 => c3, pad=(p, p), relu, init=init),\n        MaxPool((2 ,2)),\n        make2d)\n    d = Flux.outputsize(front, (n_in..., n_channels, 1)) |> first\n    return Chain(front, Dense(d, n_out, init=init))\nend\n\nIt is important to note that in our build function, there is no final softmax. This is applied by default in all MLJFlux classifiers (override this using the finaliser hyperparameter).\n\nNow that our builder is defined, we can instantiate the actual MLJFlux model. If you have a GPU, you can substitute in acceleration=CUDALibs() below to speed up training.\n\nImageClassifier = @load ImageClassifier pkg=MLJFlux\nclf = ImageClassifier(builder=MyConvBuilder(3, 16, 32, 32),\n                      batch_size=50,\n                      epochs=10,\n                      rng=123)\n\nYou can add Flux options such as optimiser and loss in the snippet above. Currently, loss must be a flux-compatible loss, and not an MLJ measure.\n\nNext, we can bind the model with the data in a machine, and train using the first 500 images:\n\nmach = machine(clf, images, labels);\nfit!(mach, rows=1:500, verbosity=2);\nreport(mach)\nchain = fitted_params(mach)\nFlux.params(chain)[2]\n\nWe can tack on 20 more epochs by modifying the epochs field, and iteratively fit some more:\n\nclf.epochs = clf.epochs + 20\nfit!(mach, rows=1:500, verbosity=2);\n\nWe can also make predictions and calculate an out-of-sample loss estimate, using any MLJ measure (loss/score):\n\npredicted_labels = predict(mach, rows=501:1000);\ncross_entropy(predicted_labels, labels[501:1000])\n\nThe preceding fit!/predict/evaluate workflow can be alternatively executed as follows:\n\nevaluate!(mach,\n          resampling=Holdout(fraction_train=0.5),\n          measure=cross_entropy,\n          rows=1:1000,\n          verbosity=0)\n\nSee also NeuralNetworkClassifier.\n\n\n\n\n\n","category":"type"},{"location":"interface/Summary/#Models","page":"Summary","title":"Models","text":"MLJFlux provides the model types below, for use with input features X and targets y of the scientific type indicated in the table below. The parameters n_in, n_out and n_channels refer to information passed to the builder, as described under Defining Custom Builders.\n\nModel Type Prediction type scitype(X) <: _ scitype(y) <: _\nNeuralNetworkRegressor Deterministic AbstractMatrix{Continuous} or Table(Continuous) with n_in columns AbstractVector{<:Continuous) (n_out = 1)\nMultitargetNeuralNetworkRegressor Deterministic AbstractMatrix{Continuous} or Table(Continuous) with n_in columns <: Table(Continuous) with n_out columns\nNeuralNetworkClassifier Probabilistic AbstractMatrix{Continuous} or Table(Continuous) with n_in columns AbstractVector{<:Finite} with n_out classes\nNeuralNetworkBinaryClassifier Probabilistic AbstractMatrix{Continuous} or Table(Continuous) with n_in columns AbstractVector{<:Finite{2}} (but n_out = 1)\nImageClassifier Probabilistic AbstractVector(<:Image{W,H}) with n_in = (W, H) AbstractVector{<:Finite} with n_out classes\n\n<details><summary><b>What exactly is a \"model\"?</b></summary>\n\nIn MLJ a model is a mutable struct storing hyper-parameters for some learning algorithm indicated by the model name, and that's all. In particular, an MLJ model does not store learned parameters.\n\nwarning: Difference in Definition\nIn Flux the term \"model\" has another meaning. However, as all Flux \"models\" used in MLJFLux are Flux.Chain objects, we call them chains, and restrict use of \"model\" to models in the MLJ sense.\n\n</details>\n\n<details open><summary><b>Are oberservations rows or columns?</b></summary>\n\nIn MLJ the convention for two-dimensional data (tables and matrices) is rows=obervations. For matrices Flux has the opposite convention. If your data is a matrix with whose column index the observation index, then your optimal solution is to present the adjoint or transpose of your matrix to MLJFlux models. Otherwise, you can use the matrix as is, or transform one time with permutedims, and again present the adjoint or transpose as the optimal solution for MLJFlux training.\n\n</details>\n\nInstructions for coercing common image formats into some AbstractVector{<:Image} are here.\n\n<details closed><summary><b>Fitting and warm restarts</b></summary>\n\nMLJ machines cache state enabling the \"warm restart\" of model training, as demonstrated in the incremental training example. In the case of MLJFlux models, fit!(mach) will use a warm restart if:\n\nonly model.epochs has changed since the last call; or\nonly model.epochs or model.optimiser have changed since the last call and model.optimiser_changes_trigger_retraining == false (the default) (the \"state\" part of the optimiser is ignored in this comparison). This allows one to dynamically modify learning rates, for example.\n\nHere model=mach.model is the associated MLJ model.\n\nThe warm restart feature makes it possible to externally control iteration. See, for example, Early Stopping with MLJFlux and Using MLJ to classifiy the MNIST image dataset.\n\n</details>","category":"section"},{"location":"interface/Summary/#Model-Hyperparameters.","page":"Summary","title":"Model Hyperparameters.","text":"All models share the following hyper-parameters. See individual model docstrings for a full list.\n\nHyper-parameter Description Default\nbuilder Default builder for models. MLJFlux.Linear(σ=Flux.relu) (regressors) or MLJFlux.Short(n_hidden=0, dropout=0.5, σ=Flux.σ) (classifiers)\noptimiser The optimiser to use for training. Optimiser.Adam()\nloss The loss function used for training. Flux.mse (regressors) and Flux.crossentropy (classifiers)\nn_epochs Number of epochs to train for. 10\nbatch_size The batch size for the data. 1\nlambda The regularization strength. Range = [0, ∞). 0\nalpha The L2/L1 mix of regularization. Range = [0, 1]. 0\nrng The random number generator (RNG) passed to builders, for weight initialization, for example. Can be any AbstractRNG or the seed (integer) for a Xoshirio that is reset on every cold restart of model (machine) training. GLOBAL_RNG\nacceleration Use CUDALibs() for training on GPU; default is CPU1(). CPU1()\noptimiser_changes_trigger_retraining True if fitting an associated machine should trigger retraining from scratch whenever the optimiser changes. false\n\nThe classifiers have an additional hyperparameter finaliser (default is Flux.softmax, or Flux.σ in the binary case) which is the operation applied to the unnormalized output of the final layer to obtain probabilities (outputs summing to one). It should return a vector of the same length as its input.\n\nnote: Loss Functions\nCurrently, the loss function specified by loss=... is applied internally by Flux and needs to conform to the Flux API. You cannot, for example, supply one of MLJ's probabilistic loss functions, such as MLJ.cross_entropy to one of the classifier constructors.\n\nThat said, you can only use MLJ loss functions or metrics in evaluation meta-algorithms (such as cross validation) and they will work even if the underlying model comes from MLJFlux.\n\n<details closed><summary><b>More on accelerated training with GPUs</b></summary>\n\nAs in the table, when instantiating a model for training on a GPU, specify acceleration=CUDALibs(), as in\n\nusing MLJ\nImageClassifier = @load ImageClassifier\nmodel = ImageClassifier(epochs=10, acceleration=CUDALibs())\nmach = machine(model, X, y) |> fit!\n\nIn this example, the data X, y is copied onto the GPU under the hood on the call to fit! and cached for use in any warm restart (see above). The Flux chain used in training is always copied back to the CPU at then conclusion of fit!, and made available as fitted_params(mach).\n\n</details>","category":"section"},{"location":"interface/Summary/#Builders","page":"Summary","title":"Builders","text":"Builder Description\nMLJFlux.MLP(hidden=(10,)) General multi-layer perceptron\nMLJFlux.Short(n_hidden=0, dropout=0.5, σ=sigmoid) Fully connected network with one hidden layer and dropout\nMLJFlux.Linear(σ=relu) Vanilla linear network with no hidden layers and activation function σ\nMLJFlux.@builder Macro for customized builders\n ","category":"section"},{"location":"common_workflows/incremental_training/README/#Contents","page":"Contents","title":"Contents","text":"file description\nnotebook.ipynb Juptyer notebook (executed)\nnotebook.unexecuted.ipynb Jupyter notebook (unexecuted)\nnotebook.md static markdown (included in MLJFlux.jl docs)\nnotebook.jl executable Julia script annotated with comments\ngenerate.jl maintainers only: execute to generate MD and IPYNB from JL","category":"section"},{"location":"common_workflows/incremental_training/README/#Important","page":"Contents","title":"Important","text":"Scripts or notebooks in this folder cannot be reliably executed without the accompanying Manifest.toml and Project.toml files. Relative to the directory of this README.md, you can try looking for these files at ../../../docs/","category":"section"},{"location":"interface/Classification/#MLJFlux.NeuralNetworkClassifier","page":"Classification","title":"MLJFlux.NeuralNetworkClassifier","text":"NeuralNetworkClassifier\n\nA model type for constructing a neural network classifier, based on MLJFlux.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nNeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux\n\nDo model = NeuralNetworkClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in NeuralNetworkClassifier(builder=...).\n\nNeuralNetworkClassifier is for training a data-dependent Flux.jl neural network for making probabilistic predictions of a Multiclass or OrderedFactor target, given a table of Continuous features. Users provide a recipe for constructing  the network, based on properties of the data that is encountered, by specifying  an appropriate builder. See MLJFlux documentation for more on builders.\n\nIn addition to features with Continuous scientific element type, this model supports categorical features in the input table. If present, such features are embedded into dense vectors by the use of an additional EntityEmbedderLayer layer after the input, as described in Entity Embeddings of Categorical Variables by Cheng Guo, Felix Berkhahn arXiv, 2016.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX provides input features and is either: (i) a Matrix with Continuous element scitype (typically Float32); or (ii) a table of input features (eg, a DataFrame) whose columns have Continuous, Multiclass or OrderedFactor element scitype; check column scitypes with schema(X).  If any Multiclass or OrderedFactor features appear, the constructed network will use an EntityEmbedderLayer layer to transform them into dense vectors. If X is a Matrix, it is assumed that columns correspond to features and rows corresponding to observations.\n\ny is the target, which can be any AbstractVector whose element scitype is Multiclass or OrderedFactor; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).\n\nHyper-parameters\n\nbuilder=MLJFlux.Short(): An MLJFlux builder that constructs a neural network. Possible  builders include: MLJFlux.Linear, MLJFlux.Short, and MLJFlux.MLP. See  MLJFlux.jl documentation for examples of user-defined builders. See also finaliser  below.\noptimiser::Optimisers.Adam(): An Optimisers.jl optimiser. The optimiser performs the updating of the weights of the network. To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at 10e-3, and tune using powers of 10 between 1 and 1e-7.\nloss=Flux.crossentropy: The loss function which the network will optimize. Should be a function which can be called in the form loss(yhat, y).  Possible loss functions are listed in the Flux loss function documentation. For a classification task, the most natural loss functions are:\nFlux.crossentropy: Standard multiclass classification loss, also known as the log loss.\nFlux.logitcrossentopy: Mathematically equal to crossentropy, but numerically more stable than finalising the outputs with softmax and then calculating crossentropy. You will need to specify finaliser=identity to remove MLJFlux's default softmax finaliser, and understand that the output of predict is then unnormalized (no longer probabilistic).\nFlux.tversky_loss: Used with imbalanced data to give more weight to false negatives.\nFlux.focal_loss: Used with highly imbalanced data. Weights harder examples more than easier examples.\nCurrently MLJ measures are not supported values of loss.\nepochs::Int=10: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.\nbatch_size::int=1: the batch size to be used for training, representing the number of samples per update of the network weights.] Typically, batch size is between 8 and 512. Increassing batch size may accelerate training if acceleration=CUDALibs() and a GPU is available.\nlambda::Float64=0: The strength of the weight regularization penalty. Can be any value in the range [0, ∞). Note the history reports unpenalized losses.\nalpha::Float64=0: The L2/L1 mix of regularization, in the range [0, 1]. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.\nrng::Union{AbstractRNG, Int64}: The random number generator or seed used during training. The default is Random.default_rng().\noptimizer_changes_trigger_retraining::Bool=false: Defines what happens when re-fitting a machine if the associated optimiser has changed. If true, the associated machine will retrain from scratch on fit! call, otherwise it will not.\nacceleration::AbstractResource=CPU1(): Defines on what hardware training is done. For Training on GPU, use CUDALibs().\nfinaliser=Flux.softmax: The final activation function of the neural network (applied after the network defined by builder). Defaults to Flux.softmax.\nembedding_dims: a Dict whose keys are names of categorical features, given as symbols, and whose values are numbers representing the desired dimensionality of the entity embeddings of such features: an integer value of 7, say, sets the embedding dimensionality to 7; a float value of 0.5, say, sets the embedding dimensionality to ceil(0.5 * c), where c is the number of feature levels.  Unspecified feature dimensionality defaults to min(c - 1, 10).\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given new features Xnew, which should have the same scitype as X above. Predictions are probabilistic but uncalibrated.\npredict_mode(mach, Xnew): Return the modes of the probabilistic predictions returned above.\ntransform(mach, Xnew): Assuming Xnew has the same schema as X, transform the categorical features of Xnew into dense Continuous vectors using the MLJFlux.EntityEmbedderLayer layer present in the network. Does nothing in case the model was trained on an input X that lacks categorical features.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nchain: The trained \"chain\" (Flux.jl model), namely the series of layers,  functions, and activations which make up the neural network. This includes  the final layer specified by finaliser (eg, softmax).\n\nReport\n\nThe fields of report(mach) are:\n\ntraining_losses: A vector of training losses (penalised if lambda != 0) in  historical order, of length epochs + 1.  The first element is the pre-training loss.\n\nExamples\n\nIn this example we build a classification model using the Iris dataset. This is a very basic example, using a default builder and no standardization.  For a more advanced illustration, see NeuralNetworkRegressor or ImageClassifier, and examples in the MLJFlux.jl documentation.\n\nusing MLJ\nusing Flux\nimport RDatasets\nimport Optimisers\n\nFirst, we can load the data:\n\niris = RDatasets.dataset(\"datasets\", \"iris\");\ny, X = unpack(iris, ==(:Species), rng=123); # a vector and a table\nNeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux\nclf = NeuralNetworkClassifier()\n\nNext, we can train the model:\n\nmach = machine(clf, X, y)\nfit!(mach)\n\nWe can train the model in an incremental fashion, altering the learning rate as we go, provided optimizer_changes_trigger_retraining is false (the default). Here, we also change the number of (total) iterations:\n\nclf.optimiser = Optimisers.Adam(clf.optimiser.eta * 2)\nclf.epochs = clf.epochs + 5\n\nfit!(mach, verbosity=2) # trains 5 more epochs\n\nWe can inspect the mean training loss using the cross_entropy function:\n\ntraining_loss = cross_entropy(predict(mach, X), y)\n\nAnd we can access the Flux chain (model) using fitted_params:\n\nchain = fitted_params(mach).chain\n\nFinally, we can see how the out-of-sample performance changes over time, using MLJ's learning_curve function:\n\nr = range(clf, :epochs, lower=1, upper=200, scale=:log10)\ncurve = learning_curve(clf, X, y,\n                     range=r,\n                     resampling=Holdout(fraction_train=0.7),\n                     measure=cross_entropy)\nusing Plots\nplot(curve.parameter_values,\n     curve.measurements,\n     xlab=curve.parameter_name,\n     xscale=curve.parameter_scale,\n     ylab = \"Cross Entropy\")\n\n\nSee also ImageClassifier, NeuralNetworkBinaryClassifier.\n\n\n\n\n\n","category":"type"},{"location":"interface/Classification/#MLJFlux.NeuralNetworkBinaryClassifier","page":"Classification","title":"MLJFlux.NeuralNetworkBinaryClassifier","text":"NeuralNetworkBinaryClassifier\n\nA model type for constructing a neural network binary classifier, based on MLJFlux.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nNeuralNetworkBinaryClassifier = @load NeuralNetworkBinaryClassifier pkg=MLJFlux\n\nDo model = NeuralNetworkBinaryClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in NeuralNetworkBinaryClassifier(builder=...).\n\nNeuralNetworkBinaryClassifier is for training a data-dependent Flux.jl neural network for making probabilistic predictions of a binary (Multiclass{2} or OrderedFactor{2}) target, given a table of Continuous features. Users provide a recipe for constructing  the network, based on properties of the data that is encountered, by specifying  an appropriate builder. See MLJFlux documentation for more on builders.\n\nIn addition to features with Continuous scientific element type, this model supports categorical features in the input table. If present, such features are embedded into dense vectors by the use of an additional EntityEmbedderLayer layer after the input, as described in Entity Embeddings of Categorical Variables by Cheng Guo, Felix Berkhahn arXiv, 2016.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX provides input features and is either: (i) a Matrix with Continuous element scitype (typically Float32); or (ii) a table of input features (eg, a DataFrame) whose columns have Continuous, Multiclass or OrderedFactor element scitype; check column scitypes with schema(X).  If any Multiclass or OrderedFactor features appear, the constructed network will use an EntityEmbedderLayer layer to transform them into dense vectors. If X is a Matrix, it is assumed that columns correspond to features and rows corresponding to observations.\n\ny is the target, which can be any AbstractVector whose element scitype is Multiclass{2} or OrderedFactor{2}; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).\n\nHyper-parameters\n\nbuilder=MLJFlux.Short(): An MLJFlux builder that constructs a neural network. Possible  builders include: MLJFlux.Linear, MLJFlux.Short, and MLJFlux.MLP. See  MLJFlux.jl documentation for examples of user-defined builders. See also finaliser  below.\noptimiser::Flux.Adam(): A Flux.Optimise optimiser. The optimiser performs the updating of the weights of the network. For further reference, see the Flux optimiser documentation. To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at 10e-3, and tune using powers of 10 between 1 and 1e-7.\nloss=Flux.binarycrossentropy: The loss function which the network will optimize. Should be a function which can be called in the form loss(yhat, y).  Possible loss functions are listed in the Flux loss function documentation. For a classification task, the most natural loss functions are:\nFlux.binarycrossentropy: Standard binary classification loss, also known as the log loss.\nFlux.logitbinarycrossentropy: Mathematically equal to crossentropy, but numerically more stable than finalising the outputs with σ and then calculating crossentropy. You will need to specify finaliser=identity to remove MLJFlux's default sigmoid finaliser, and understand that the output of predict is then unnormalized (no longer probabilistic).\nFlux.tversky_loss: Used with imbalanced data to give more weight to false negatives.\nFlux.binary_focal_loss: Used with highly imbalanced data. Weights harder examples more than easier examples.\nCurrently MLJ measures are not supported values of loss.\nepochs::Int=10: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.\nbatch_size::int=1: the batch size to be used for training, representing the number of samples per update of the network weights. Typically, batch size is between 8 and 512. Increassing batch size may accelerate training if acceleration=CUDALibs() and a GPU is available.\nlambda::Float64=0: The strength of the weight regularization penalty. Can be any value in the range [0, ∞).\nalpha::Float64=0: The L2/L1 mix of regularization, in the range [0, 1]. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.\nrng::Union{AbstractRNG, Int64}: The random number generator or seed used during training.\noptimizer_changes_trigger_retraining::Bool=false: Defines what happens when re-fitting a machine if the associated optimiser has changed. If true, the associated machine will retrain from scratch on fit! call, otherwise it will not.\nacceleration::AbstractResource=CPU1(): Defines on what hardware training is done. For Training on GPU, use CUDALibs().\nfinaliser=Flux.σ: The final activation function of the neural network (applied after the network defined by builder). Defaults to Flux.σ.\nembedding_dims: a Dict whose keys are names of categorical features, given as symbols, and whose values are numbers representing the desired dimensionality of the entity embeddings of such features: an integer value of 7, say, sets the embedding dimensionality to 7; a float value of 0.5, say, sets the embedding dimensionality to ceil(0.5 * c), where c is the number of feature levels.  Unspecified feature dimensionality defaults to min(c - 1, 10).\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given new features Xnew, which should have the same scitype as X above. Predictions are probabilistic but uncalibrated.\npredict_mode(mach, Xnew): Return the modes of the probabilistic predictions returned above.\ntransform(mach, Xnew): Assuming Xnew has the same schema as X, transform the categorical features of Xnew into dense Continuous vectors using the MLJFlux.EntityEmbedderLayer layer present in the network. Does nothing in case the model was trained on an input X that lacks categorical features.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nchain: The trained \"chain\" (Flux.jl model), namely the series of layers,  functions, and activations which make up the neural network. This includes  the final layer specified by finaliser (eg, softmax).\n\nReport\n\nThe fields of report(mach) are:\n\ntraining_losses: A vector of training losses (penalised if lambda != 0) in  historical order, of length epochs + 1.  The first element is the pre-training loss.\n\nExamples\n\nIn this example we build a classification model using the Iris dataset. This is a very basic example, using a default builder and no standardization.  For a more advanced illustration, see NeuralNetworkRegressor or ImageClassifier, and examples in the MLJFlux.jl documentation.\n\nusing MLJ, Flux\nimport Optimisers\nimport RDatasets\n\nFirst, we can load the data:\n\nmtcars = RDatasets.dataset(\"datasets\", \"mtcars\");\ny, X = unpack(mtcars, ==(:VS), in([:MPG, :Cyl, :Disp, :HP, :WT, :QSec]));\n\nNote that y is a vector and X a table.\n\ny = categorical(y) # classifier takes catogorical input\nX_f32 = Float32.(X) # To match floating point type of the neural network layers\nNeuralNetworkBinaryClassifier = @load NeuralNetworkBinaryClassifier pkg=MLJFlux\nbclf = NeuralNetworkBinaryClassifier()\n\nNext, we can train the model:\n\nmach = machine(bclf, X_f32, y)\nfit!(mach)\n\nWe can train the model in an incremental fashion, altering the learning rate as we go, provided optimizer_changes_trigger_retraining is false (the default). Here, we also change the number of (total) iterations:\n\njulia> bclf.optimiser\nAdam(0.001, (0.9, 0.999), 1.0e-8)\n\nbclf.optimiser = Optimisers.Adam(eta = bclf.optimiser.eta * 2)\nbclf.epochs = bclf.epochs + 5\n\nfit!(mach, verbosity=2) # trains 5 more epochs\n\nWe can inspect the mean training loss using the cross_entropy function:\n\ntraining_loss = cross_entropy(predict(mach, X_f32), y)\n\nAnd we can access the Flux chain (model) using fitted_params:\n\nchain = fitted_params(mach).chain\n\nFinally, we can see how the out-of-sample performance changes over time, using MLJ's learning_curve function:\n\nr = range(bclf, :epochs, lower=1, upper=200, scale=:log10)\ncurve = learning_curve(\n    bclf,\n    X_f32,\n    y,\n    range=r,\n    resampling=Holdout(fraction_train=0.7),\n    measure=cross_entropy,\n)\nusing Plots\nplot(\n   curve.parameter_values,\n   curve.measurements,\n   xlab=curve.parameter_name,\n   xscale=curve.parameter_scale,\n   ylab = \"Cross Entropy\",\n)\n\n\nSee also ImageClassifier.\n\n\n\n\n\n","category":"type"},{"location":"common_workflows/architecture_search/README/#Contents","page":"Contents","title":"Contents","text":"file description\nnotebook.ipynb Juptyer notebook (executed)\nnotebook.unexecuted.ipynb Jupyter notebook (unexecuted)\nnotebook.md static markdown (included in MLJFlux.jl docs)\nnotebook.jl executable Julia script annotated with comments\ngenerate.jl maintainers only: execute to generate MD and IPYNB from JL","category":"section"},{"location":"common_workflows/architecture_search/README/#Important","page":"Contents","title":"Important","text":"Scripts or notebooks in this folder cannot be reliably executed without the accompanying Manifest.toml and Project.toml files. Relative to the directory of this README.md, you can try looking for these files at ../../../docs/","category":"section"},{"location":"common_workflows/live_training/notebook/#Live-Training-with-MLJFlux","page":"Live Training","title":"Live Training with MLJFlux","text":"This demonstration is available as a Jupyter notebook or julia script here.\n\nThis script tested using Julia 1.10","category":"section"},{"location":"common_workflows/live_training/notebook/#Basic-Imports","page":"Live Training","title":"Basic Imports","text":"using MLJ\nusing Flux\nimport Optimisers\nusing StableRNGs        # for reproducibility across Julia versions\n\nstable_rng() = StableRNGs.StableRNG(123)\n\nusing Plots","category":"section"},{"location":"common_workflows/live_training/notebook/#Loading-and-Splitting-the-Data","page":"Live Training","title":"Loading and Splitting the Data","text":"iris = load_iris() # a named-tuple of vectors\ny, X = unpack(iris, ==(:target), rng=stable_rng())\nX = fmap(column-> Float32.(column), X) # Flux prefers Float32 data","category":"section"},{"location":"common_workflows/live_training/notebook/#Instantiating-the-model","page":"Live Training","title":"Instantiating the model","text":"Now let's construct our model. This follows a similar setup to the one followed in the Quick Start.\n\nNeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux\n\nclf = NeuralNetworkClassifier(\n    builder=MLJFlux.MLP(; hidden=(5,4), σ=Flux.relu),\n    optimiser=Optimisers.Adam(0.01),\n    batch_size=8,\n    epochs=50,\n    rng=stable_rng(),\n)\n\nNow let's wrap this in an iterated model. We will use a callback that makes a plot for validation losses each iteration.\n\nstop_conditions = [\n    Step(1),            # Repeatedly train for one iteration\n    NumberLimit(100),   # Don't train for more than 100 iterations\n]\n\nvalidation_losses =  []\ngr(reuse=true)                  # use the same window for plots\nfunction plot_loss(loss)\n    push!(validation_losses, loss)\n    display(plot(validation_losses, label=\"validation loss\", xlim=(1, 100)))\n    sleep(.01)  # to catch up with the plots while they are being generated\nend\n\ncallbacks = [ WithLossDo(plot_loss),]\n\niterated_model = IteratedModel(\n    model=clf,\n    resampling=Holdout(),\n    measures=log_loss,\n    iteration_parameter=:(epochs),\n    controls=vcat(stop_conditions, callbacks),\n    retrain=true,\n)","category":"section"},{"location":"common_workflows/live_training/notebook/#Live-Training","page":"Live Training","title":"Live Training","text":"Simply fitting the model is all we need\n\nmach = machine(iterated_model, X, y)\nfit!(mach)\nvalidation_losses\n\nNote that the wrapped model sets aside some data on which to make out-of-sample estimates of the loss, which is how validation_losses are calculated. But if we use mach to make predictions on new input features, these are based on retraining the model on all provided data.\n\nXnew = (\n    sepal_length = Float32[5.8, 5.8, 5.8],\n    sepal_width = Float32[4.0, 2.6, 2.7],\n    petal_length = Float32[1.2, 4.0, 4.1],\n    petal_width = Float32[0.2, 1.2, 1.0],\n)\n\npredict_mode(mach, Xnew)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"#MLJFlux.jl","page":"Introduction","title":"MLJFlux.jl","text":"A Julia package integrating deep learning Flux models with MLJ.","category":"section"},{"location":"#Objectives","page":"Introduction","title":"Objectives","text":"Provide a user-friendly and high-level interface to fundamental Flux deep learning models while still being extensible by supporting custom models written with Flux\nMake building deep learning models more convenient to users already familiar with the MLJ workflow\nMake it easier to apply machine learning techniques provided by MLJ, including: out-of-sample performance evaluation, hyper-parameter optimization, iteration control, and more, to deep learning models\n\nnote: MLJFlux Scope\nMLJFlux support is focused on fundamental deep learning models for common supervised learning tasks. Sophisticated architectures and approaches, such as online learning, reinforcement learning, and adversarial networks, are currently outside its scope. Also, MLJFlux is limited to tasks where all (batches of) training data     fits into memory.","category":"section"},{"location":"#Installation","page":"Introduction","title":"Installation","text":"import Pkg\nPkg.activate(\"my_environment\", shared=true)\nPkg.add([\"MLJ\", \"MLJFlux\", \"Optimisers\", \"Flux\"])","category":"section"},{"location":"#Quick-Start","page":"Introduction","title":"Quick Start","text":"using MLJ, MLJFlux\nimport Flux\n\n# 1. Load Data\niris = load_iris() # a named-tuple of vectors (but most tables work here)\ny, X = unpack(iris, ==(:target), rng=123)\nX = Flux.fmap(column-> Float32.(column), X) # Flux prefers Float32 data\n\n# 2. Load and instantiate model\nNeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=\"MLJFlux\"\nclf = NeuralNetworkClassifier(\n    builder=MLJFlux.MLP(; hidden=(5,4), σ=Flux.relu),\n    optimiser=Flux.Adam(0.01),\n    batch_size=8,\n    epochs=100,\n    acceleration=CPU1() # the default; use instead `CUDALibs()` for GPU\n    )\n\n# 3. Wrap it in a machine\nmach = machine(clf, X, y)\n\n# 4. Evaluate the model\nevaluate!(mach, resampling=CV(nfolds=3), repeats=2, measure=[brier_loss, accuracy])\n\n# 5. Fit and predict on new data\nfit!(mach)\nXnew = (\n    sepal_length = [7.2, 4.4, 5.6],\n    sepal_width = [3.0, 2.9, 2.5],\n    petal_length = [5.8, 1.4, 3.9],\n    petal_width = [1.6, 0.2, 1.1],\n)\npredict(mach, Xnew)\n\nAs you can see we are able to use MLJ meta-functionality (in this case Monte Carlo cross-validation) with a Flux neural network.\n\nNotice that we are also able to define the neural network in a high-level fashion by only specifying the number of neurons in each hidden layer and the activation function. Meanwhile, MLJFlux is able to infer the input and output layer as well as use a suitable default for the loss function and output activation given the classification task. Notice as well that we did not need to manually implement a training or prediction loop.","category":"section"},{"location":"#Basic-idea:-\"builders\"-for-data-dependent-architecture","page":"Introduction","title":"Basic idea: \"builders\" for data-dependent architecture","text":"As in the example above, any MLJFlux model has a builder hyperparameter, an object encoding instructions for creating a neural network given the data that the model eventually sees (e.g., the number of classes in a classification problem). While each MLJ model has a simple default builder, users may need to define custom builders to get optimal results (see Defining Custom Builders and this will require familiarity with the Flux API for defining a neural network chain.","category":"section"},{"location":"#Flux-or-MLJFlux?","page":"Introduction","title":"Flux or MLJFlux?","text":"Flux is a deep learning framework in Julia that comes with everything you need to build deep learning models (i.e., GPU support, automatic differentiation, layers, activations, losses, optimizers, etc.). MLJFlux wraps models built with Flux which provides a more high-level interface for building and training such models. More importantly, it empowers Flux models by extending their support to many common machine learning workflows that are possible via MLJ such as:\n\nEstimating performance of your model using a holdout set or other resampling strategy (e.g., cross-validation) as measured by one or more metrics (e.g., loss functions) that may not have been used in training\nOptimizing hyper-parameters such as a regularization parameter (e.g., dropout) or a width/height/nchannnels of convolution layer\nCompose with other models such as introducing data pre-processing steps (e.g., missing data imputation) into a pipeline. It might make sense to include non-deep learning models in this pipeline. Other kinds of model composition could include blending predictions of a deep learner with some other kind of model (as in “model stacking”). Models composed with MLJ can be also tuned as a single unit.\nControlling iteration by adding an early stopping criterion based on an out-of-sample estimate of the loss, dynamically changing the learning rate (eg, cyclic learning rates), periodically save snapshots of the model, generate live plots of sample weights to judge training progress (as in tensor board)\n\nComparing your model with a non-deep learning models\n\nA comparable project, FastAI/FluxTraining, also provides a high-level interface for interacting with Flux models and supports a set of features that may overlap with (but not include all of) those supported by MLJFlux.\n\nMany of the features mentioned above are showcased in the workflow examples that you can access from the sidebar.","category":"section"},{"location":"interface/Custom Builders/#Defining-Custom-Builders","page":"Custom Builders","title":"Defining Custom Builders","text":"Following is an example defining a new builder for creating a simple fully-connected neural network with two hidden layers, with n1 nodes in the first hidden layer, and n2 nodes in the second, for use in any of the first three models in Table 1. The definition includes one mutable struct and one method:\n\nmutable struct MyBuilder <: MLJFlux.Builder\n\tn1 :: Int\n\tn2 :: Int\nend\n\nfunction MLJFlux.build(nn::MyBuilder, rng, n_in, n_out)\n\tinit = Flux.glorot_uniform(rng)\n        return Chain(\n            Dense(n_in, nn.n1, init=init),\n            Dense(nn.n1, nn.n2, init=init),\n            Dense(nn.n2, n_out, init=init),\n            )\nend\n\nNote here that n_in and n_out depend on the size of the data (see Table 1).\n\nFor a concrete image classification example, see Using MLJ to classifiy the MNIST image dataset.\n\nMore generally, defining a new builder means defining a new struct sub-typing MLJFlux.Builder and defining a new MLJFlux.build method with one of these signatures:\n\nMLJFlux.build(builder::MyBuilder, rng, n_in, n_out)\nMLJFlux.build(builder::MyBuilder, rng, n_in, n_out, n_channels) # for use with `ImageClassifier`\n\nThis method must return a Flux.Chain instance, chain, subject to the following conditions:\n\nchain(x) must make sense:\nfor any x <: Array{<:AbstractFloat, 2} of size (n_in, batch_size) where batch_size is any integer (for all models except ImageClassifier); or\nfor any x <: Array{<:Float32, 4} of size (W, H, n_channels, batch_size), where (W, H) = n_in, n_channels is 1 or 3, and batch_size is any integer (for use with ImageClassifier)\nThe object returned by chain(x) must be an AbstractFloat vector of length n_out.\n\nAlternatively, use MLJFlux.@builder(neural_net) to automatically create a builder for any valid Flux chain expression neural_net, where the symbols n_in, n_out, n_channels and rng can appear literally, with the interpretations explained above. For example,\n\nbuilder = MLJFlux.@builder Chain(Dense(n_in, 128), Dense(128, n_out, tanh))","category":"section"}]
}
