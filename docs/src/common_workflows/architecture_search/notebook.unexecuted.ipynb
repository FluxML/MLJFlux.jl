{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Architecture Search with MLJFlux"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This demonstration is available as a Jupyter notebook or julia script\n",
    "[here](https://github.com/FluxML/MLJFlux.jl/tree/dev/docs/src/common_workflows/architecture_search)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Neural Architecture Search (NAS) is an instance of hyperparameter tuning concerned\n",
    "with tuning model hyperparameters defining the architecture itself. Although it's\n",
    "typically performed with sophisticated search algorithms for efficiency, in this example\n",
    "we will be using a simple random search."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Pkg\n",
    "PKG_ENV = joinpath(@__DIR__, \"..\", \"..\", \"..\")\n",
    "Pkg.activate(PKG_ENV);\n",
    "Pkg.instantiate();"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**This script tested using Julia 1.10**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Basic Imports"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ               # Has MLJFlux models\n",
    "using Flux              # For more flexibility\n",
    "using DataFrames        # To view tuning results in a table\n",
    "import Optimisers       # native Flux.jl optimisers no longer supported\n",
    "using StableRNGs        # for reproducibility across Julia versions\n",
    "\n",
    "stable_rng() = StableRNGs.StableRNG(123)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading and Splitting the Data"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "iris = load_iris() # a named-tuple of vectors\n",
    "y, X = unpack(iris, ==(:target), rng=stable_rng())\n",
    "X = fmap(column-> Float32.(column), X) # Flux prefers Float32 data"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Instantiating the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's construct our model. This follows a similar setup the one followed in the\n",
    "[Quick Start](../../index.md#Quick-Start)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "NeuralNetworkClassifier = @load NeuralNetworkClassifier pkg = \"MLJFlux\"\n",
    "clf = NeuralNetworkClassifier(\n",
    "    builder = MLJFlux.MLP(; hidden = (1, 1, 1), σ = Flux.relu),\n",
    "    optimiser = Optimisers.ADAM(0.01),\n",
    "    batch_size = 8,\n",
    "    epochs = 10,\n",
    "    rng = stable_rng(),\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generating Network Architectures"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We know that the MLP builder takes a tuple of the form $(z_1, z_2, ..., z_k)$ to define\n",
    "a network with $k$ hidden layers and where the ith layer has $z_i$ neurons. We will\n",
    "proceed by defining a function that can generate all possible networks with a specific\n",
    "number of hidden layers, a minimum and maximum number of neurons per layer and\n",
    "increments to consider for the number of neurons."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function generate_networks(\n",
    "    ;min_neurons::Int,\n",
    "    max_neurons::Int,\n",
    "    neuron_step::Int,\n",
    "    num_layers::Int,\n",
    "    )\n",
    "    # Define the range of neurons\n",
    "    neuron_range = min_neurons:neuron_step:max_neurons\n",
    "\n",
    "    # Empty list to store the network configurations\n",
    "    networks = Vector{Tuple{Vararg{Int, num_layers}}}()\n",
    "\n",
    "    # Recursive helper function to generate all combinations of tuples\n",
    "    function generate_tuple(current_layers, remaining_layers)\n",
    "        if remaining_layers > 0\n",
    "            for n in neuron_range\n",
    "                # current_layers =[] then current_layers=[(min_neurons)],\n",
    "                # [(min_neurons+neuron_step)], [(min_neurons+2*neuron_step)],...\n",
    "                # for each of these we call generate_layers again which appends\n",
    "                # the n combinations for each one of them\n",
    "                generate_tuple(vcat(current_layers, [n]), remaining_layers - 1)\n",
    "            end\n",
    "        else\n",
    "            # in the base case, no more layers to \"recurse on\"\n",
    "            # and we just append the current_layers as a tuple\n",
    "            push!(networks, tuple(current_layers...))\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Generate networks for the given number of layers\n",
    "    generate_tuple([], num_layers)\n",
    "\n",
    "    return networks\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's generate an array of all possible neural networks with three hidden layers and\n",
    "number of neurons per layer ∈ [1,64] with a step of 4"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "networks_space =\n",
    "    generate_networks(\n",
    "        min_neurons = 1,\n",
    "        max_neurons = 64,\n",
    "        neuron_step = 4,\n",
    "        num_layers = 3,\n",
    "    )\n",
    "\n",
    "networks_space[1:5]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Wrapping the Model for Tuning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's use this array to define the range of hyperparameters and pass it along with the\n",
    "model to the `TunedModel` constructor."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r1 = range(clf, :(builder.hidden), values = networks_space)\n",
    "\n",
    "tuned_clf = TunedModel(\n",
    "    model = clf,\n",
    "    tuning = RandomSearch(),\n",
    "    resampling = CV(nfolds = 4, rng = 42),\n",
    "    range = [r1],\n",
    "    measure = cross_entropy,\n",
    "    n = 100,             # searching over 100 random samples are enough\n",
    ");"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Performing the Search"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similar to the last workflow example, all we need now is to fit our model and the search\n",
    "will take place automatically:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mach = machine(tuned_clf, X, y);\n",
    "fit!(mach, verbosity = 0);\n",
    "fitted_params(mach).best_model"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analyzing the Search Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's analyze the search results by converting the history array to a dataframe and\n",
    "viewing it:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "history = report(mach).history;\n",
    "history_df = DataFrame(\n",
    "    mlp = [x[:model].builder for x in history],\n",
    "    measurement = [x[:measurement][1] for x in history],\n",
    ")\n",
    "first(sort!(history_df, [order(:measurement)]), 10)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.10"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.10",
   "language": "julia"
  }
 },
 "nbformat": 4
}
