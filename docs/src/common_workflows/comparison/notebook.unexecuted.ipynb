{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model Comparison with MLJFlux"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This demonstration is available as a Jupyter notebook or julia script\n",
    "[here](https://github.com/FluxML/MLJFlux.jl/tree/dev/docs/src/common_workflows/comparison)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this workflow example, we see how we can compare different machine learning models\n",
    "with a neural network from MLJFlux."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Pkg\n",
    "PKG_ENV = joinpath(@__DIR__, \"..\", \"..\", \"..\")\n",
    "Pkg.activate(PKG_ENV);\n",
    "Pkg.instantiate();"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**This script tested using Julia 1.10**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Basic Imports"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ               # Has MLJFlux models\n",
    "using Flux              # For more flexibility\n",
    "using DataFrames        # To visualize hyperparameter search results\n",
    "import Optimisers       # native Flux.jl optimisers no longer supported\n",
    "using Measurements       # to get ± functionality\n",
    "import CategoricalArrays.unwrap\n",
    "using StableRNGs        # for reproducibility across Julia versions\n",
    "\n",
    "stable_rng() = StableRNG(123)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading and Splitting the Data"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "iris = load_iris() # a named-tuple of vectors\n",
    "y, X = unpack(iris, ==(:target), rng=stable_rng())"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Instantiating the models Now let's construct our model. This follows a similar setup\n",
    "to the one followed in the [Quick Start](../../index.md#Quick-Start)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "NeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux\n",
    "\n",
    "clf1 = NeuralNetworkClassifier(\n",
    "    builder=MLJFlux.MLP(; hidden=(5,4), σ=Flux.relu),\n",
    "    optimiser=Optimisers.Adam(0.01),\n",
    "    batch_size=8,\n",
    "    epochs=50,\n",
    "    rng=stable_rng(),\n",
    "    )"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's as well load and construct three other classical machine learning models:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "BayesianLDA = @load BayesianLDA pkg=MultivariateStats\n",
    "clf2 = BayesianLDA()\n",
    "RandomForestClassifier = @load RandomForestClassifier pkg=DecisionTree\n",
    "clf3 = RandomForestClassifier()\n",
    "XGBoostClassifier = @load XGBoostClassifier pkg=XGBoost\n",
    "clf4 = XGBoostClassifier();"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Wrapping One of the Models in a TunedModel"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instead of just comparing with four models with the default/given hyperparameters, we\n",
    "will give `XGBoostClassifier` an unfair advantage By wrapping it in a `TunedModel` that\n",
    "considers the best learning rate η for the model."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r1 = range(clf4, :eta, lower=0.01, upper=0.5, scale=:log10)\n",
    "tuned_model_xg = TunedModel(\n",
    "    model=clf4,\n",
    "    ranges=[r1],\n",
    "    tuning=Grid(resolution=10),\n",
    "    resampling=CV(nfolds=5, rng=stable_rng()),\n",
    "    measure=cross_entropy,\n",
    ");"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Of course, one can wrap each of the four in a TunedModel if they are interested in\n",
    "comparing the models over a large set of their hyperparameters."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparing the models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We simply pass the four models to the `models` argument of the `TunedModel` construct"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "tuned_model = TunedModel(\n",
    "    models=[clf1, clf2, clf3, tuned_model_xg],\n",
    "    tuning=Explicit(),\n",
    "    resampling=CV(nfolds=2, rng=stable_rng()),\n",
    "    repeats=5,\n",
    "    measure=cross_entropy,\n",
    ");"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice here we are using 5 x 2 Monte Carlo cross-validation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then wrapping our tuned model in a machine and fitting it."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mach = machine(tuned_model, X, y);\n",
    "fit!(mach, verbosity=0);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's see the history for more details on the performance for each of the models"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "history = report(mach).history\n",
    "history_df = DataFrame(\n",
    "    mlp = [x.model for x in history],\n",
    "    measurement = [\n",
    "        x.evaluation.measurement[1] ±\n",
    "            x.evaluation.uncertainty_radius_95[1] for x in history\n",
    "                ],\n",
    ")\n",
    "sort!(history_df, [order(:measurement)])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is Occam's razor in practice."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.10"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.10",
   "language": "julia"
  }
 },
 "nbformat": 4
}
