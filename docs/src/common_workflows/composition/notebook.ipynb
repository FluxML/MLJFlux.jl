{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model Composition with MLJFlux"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This demonstration is available as a Jupyter notebook or julia script\n",
    "[here](https://github.com/FluxML/MLJFlux.jl/tree/dev/docs/src/common_workflows/composition)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this workflow example, we see how MLJFlux enables composing MLJ models with MLJFlux\n",
    "models. We will assume a class imbalance setting and wrap an oversampler with a deep\n",
    "learning model from MLJFlux."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Activating project at `~/GoogleDrive/Julia/MLJ/MLJFlux/docs`\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "using Pkg\n",
    "PKG_ENV = joinpath(@__DIR__, \"..\", \"..\", \"..\")\n",
    "Pkg.activate(PKG_ENV);\n",
    "Pkg.instantiate();"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "**This script tested using Julia 1.10**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Basic Imports"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "stable_rng (generic function with 1 method)"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "cell_type": "code",
   "source": [
    "using MLJ               # Has MLJFlux models\n",
    "using Flux              # For more flexibility\n",
    "import Random           # To create imbalance\n",
    "import Imbalance        # To solve the imbalance\n",
    "import Optimisers       # native Flux.jl optimisers no longer supported\n",
    "using StableRNGs        # for reproducibility across Julia versions\n",
    "import CategoricalArrays.unwrap\n",
    "\n",
    "stable_rng() = StableRNGs.StableRNG(123)"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading and Splitting the Data"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(sepal_length = Float32[6.1, 7.3, 6.3, 4.8, 5.9, 7.1, 6.7, 5.4, 6.0, 6.9  …  5.0, 6.4, 5.7, 4.6, 5.5, 4.6, 5.6, 5.7, 6.0, 5.0], sepal_width = Float32[2.9, 2.9, 3.4, 3.4, 3.0, 3.0, 3.0, 3.9, 3.0, 3.1  …  3.3, 2.7, 2.5, 3.2, 2.4, 3.1, 2.8, 3.0, 2.9, 3.5], petal_length = Float32[4.7, 6.3, 5.6, 1.9, 5.1, 5.9, 5.0, 1.7, 4.8, 4.9  …  1.4, 5.3, 5.0, 1.4, 3.7, 1.5, 4.9, 4.2, 4.5, 1.6], petal_width = Float32[1.4, 1.8, 2.4, 0.2, 1.8, 2.1, 1.7, 0.4, 1.8, 1.5  …  0.2, 1.9, 2.0, 0.2, 1.0, 0.2, 2.0, 1.2, 1.5, 0.6])"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "cell_type": "code",
   "source": [
    "iris = load_iris() # a named-tuple of vectors\n",
    "y, X = unpack(iris, ==(:target), rng=stable_rng())\n",
    "X = fmap(column-> Float32.(column), X) # Flux prefers Float32 data"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "The iris dataset has a target with uniformly distributed values, `\"versicolor\"`,\n",
    "`\"setosa\"`, and `\"virginica\"`. To manufacture an unbalanced dataset, we'll combine the\n",
    "first two into a single classs, `\"colosa\"`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virginica: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 50 (50.0%) \n",
      "colosa:    ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 100 (100.0%) \n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "y = coerce(\n",
    "        map(y) do species\n",
    "            species == \"virginica\" ? unwrap(species) : \"colosa\"\n",
    "        end,\n",
    "        Multiclass,\n",
    ");\n",
    "Imbalance.checkbalance(y)"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Instantiating the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's load `BorderlineSMOTE1` to oversample the data and `Standardizer` to standardize\n",
    "it."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: For silent loading, specify `verbosity=0`. \n",
      "import MLJFlux ✔\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "MLJFlux.NeuralNetworkClassifier"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "cell_type": "code",
   "source": [
    "BorderlineSMOTE1 = @load BorderlineSMOTE1 pkg=Imbalance verbosity=0\n",
    "NeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "We didn't need to load Standardizer because it is a local model for MLJ (see\n",
    "`localmodels()`)"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "NeuralNetworkClassifier(\n  builder = MLP(\n        hidden = (5, 4), \n        σ = NNlib.relu), \n  finaliser = NNlib.softmax, \n  optimiser = Optimisers.Adam(eta=0.01, beta=(0.9, 0.999), epsilon=1.0e-8), \n  loss = Flux.Losses.crossentropy, \n  epochs = 50, \n  batch_size = 8, \n  lambda = 0.0, \n  alpha = 0.0, \n  rng = StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7), \n  optimiser_changes_trigger_retraining = false, \n  acceleration = ComputationalResources.CPU1{Nothing}(nothing), \n  embedding_dims = Dict{Symbol, Real}())"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "cell_type": "code",
   "source": [
    "clf = NeuralNetworkClassifier(\n",
    "    builder=MLJFlux.MLP(; hidden=(5,4), σ=Flux.relu),\n",
    "    optimiser=Optimisers.Adam(0.01),\n",
    "    batch_size=8,\n",
    "    epochs=50,\n",
    "    rng=stable_rng(),\n",
    ")"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we wrap the oversampler with the neural network via the `BalancedModel`\n",
    "construct. This comes from `MLJBalancing` And allows combining resampling methods with\n",
    "MLJ models in a sequential pipeline."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Standardizer(\n  features = Symbol[], \n  ignore = false, \n  ordered_factor = false, \n  count = false)"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "cell_type": "code",
   "source": [
    "oversampler = BorderlineSMOTE1(k=5, ratios=1.0, rng=stable_rng())\n",
    "balanced_model = BalancedModel(model=clf, balancer1=oversampler)\n",
    "standarizer = Standardizer()"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's compose the balanced model with a standardizer."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "ProbabilisticPipeline(\n  standardizer = Standardizer(\n        features = Symbol[], \n        ignore = false, \n        ordered_factor = false, \n        count = false), \n  balanced_model_probabilistic = BalancedModelProbabilistic(\n        model = NeuralNetworkClassifier(builder = MLP(hidden = (5, 4), …), …), \n        balancer1 = BorderlineSMOTE1(m = 5, …)), \n  cache = true)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "cell_type": "code",
   "source": [
    "pipeline = standarizer |> balanced_model"
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "By this, any training data will be standardized then oversampled then passed to the\n",
    "model. Meanwhile, for inference, the standardizer will automatically use the training\n",
    "set's mean and std and the oversampler will be play no role."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the Composed Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The pipeline model can be evaluated like any other model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Training machine(ProbabilisticPipeline(standardizer = Standardizer(features = Symbol[], …), …), …).\n",
      "[ Info: Training machine(:standardizer, …).\n",
      "[ Info: Training machine(:balanced_model_probabilistic, …).\n",
      "[ Info: Training machine(BorderlineSMOTE1(m = 5, …), …).\n",
      "[ Info: Training machine(:model, …).\n",
      "[ Info: After filtering, the mapping from each class to number of borderline points is (\"colosa\" => 3, \"virginica\" => 3).\n",
      "[ Info: After filtering, the mapping from each class to number of borderline points is (\"colosa\" => 3, \"virginica\" => 3).\n",
      "\rOptimising neural net:   4%[>                        ]  ETA: 0:06:01\u001b[K\rOptimising neural net: 100%[=========================] Time: 0:00:15\u001b[K\n",
      "[ Info: After filtering, the mapping from each class to number of borderline points is (\"colosa\" => 1, \"virginica\" => 4).\n",
      "[ Info: After filtering, the mapping from each class to number of borderline points is (\"colosa\" => 1, \"virginica\" => 4).\n",
      "[ Info: After filtering, the mapping from each class to number of borderline points is (\"colosa\" => 3, \"virginica\" => 4).\n",
      "[ Info: After filtering, the mapping from each class to number of borderline points is (\"colosa\" => 3, \"virginica\" => 4).\n",
      "\rEvaluating over 5 folds:  40%[==========>              ]  ETA: 0:00:12\u001b[K[ Info: After filtering, the mapping from each class to number of borderline points is (\"colosa\" => 2, \"virginica\" => 2).\n",
      "[ Info: After filtering, the mapping from each class to number of borderline points is (\"colosa\" => 2, \"virginica\" => 2).\n",
      "\rEvaluating over 5 folds:  60%[===============>         ]  ETA: 0:00:05\u001b[K[ Info: After filtering, the mapping from each class to number of borderline points is (\"colosa\" => 4, \"virginica\" => 3).\n",
      "[ Info: After filtering, the mapping from each class to number of borderline points is (\"colosa\" => 4, \"virginica\" => 3).\n",
      "[ Info: After filtering, the mapping from each class to number of borderline points is (\"colosa\" => 2, \"virginica\" => 2).\n",
      "[ Info: After filtering, the mapping from each class to number of borderline points is (\"colosa\" => 2, \"virginica\" => 2).\n",
      "\rEvaluating over 5 folds: 100%[=========================] Time: 0:00:08\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PerformanceEvaluation object with these fields:\n  model, tag, measure, operation,\n  measurement, uncertainty_radius_95, per_fold, per_observation,\n  fitted_params_per_fold, report_per_fold,\n  train_test_rows, resampling, repeats\nTag: ProbabilisticPipeline-721\nExtract:\n┌────────────┬──────────────┬─────────────┐\n│\u001b[30m measure    \u001b[0m│\u001b[30m operation    \u001b[0m│\u001b[30m measurement \u001b[0m│\n├────────────┼──────────────┼─────────────┤\n│ Accuracy() │ predict_mode │ 0.953       │\n└────────────┴──────────────┴─────────────┘\n┌─────────────────────────────────────┬─────────┐\n│\u001b[30m per_fold                            \u001b[0m│\u001b[30m 1.96*SE \u001b[0m│\n├─────────────────────────────────────┼─────────┤\n│ [0.933, 0.933, 0.967, 0.967, 0.967] │ 0.0179  │\n└─────────────────────────────────────┴─────────┘\n"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "cell_type": "code",
   "source": [
    "mach = machine(pipeline, X, y)\n",
    "fit!(mach)\n",
    "cv=CV(nfolds=5)\n",
    "evaluate!(mach, resampling=cv, measure=accuracy)"
   ],
   "metadata": {},
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.10"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.10",
   "language": "julia"
  }
 },
 "nbformat": 4
}
