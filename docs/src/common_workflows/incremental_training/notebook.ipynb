{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Incremental Training with MLJFlux"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This demonstration is available as a Jupyter notebook or julia script\n",
    "[here](https://github.com/FluxML/MLJFlux.jl/tree/dev/docs/src/common_workflows/incremental_training)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this workflow example we explore how to incrementally train MLJFlux models."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Activating project at `~/GoogleDrive/Julia/MLJ/MLJFlux/docs`\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "using Pkg\n",
    "PKG_ENV = joinpath(@__DIR__, \"..\", \"..\", \"..\")\n",
    "Pkg.activate(PKG_ENV);\n",
    "Pkg.instantiate();"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Julia version** is assumed to be 1.10.*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Basic Imports"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "stable_rng (generic function with 1 method)"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "cell_type": "code",
   "source": [
    "using MLJ               # Has MLJFlux models\n",
    "using Flux              # For more flexibility\n",
    "import Optimisers       # native Flux.jl optimisers no longer supported\n",
    "using StableRNGs        # for reproducibility across Julia versions\n",
    "\n",
    "stable_rng() = StableRNGs.StableRNG(123)"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading and Splitting the Data"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "iris = load_iris() # a named-tuple of vectors\n",
    "y, X = unpack(iris, ==(:target), rng=stable_rng())\n",
    "X = fmap(column-> Float32.(column), X) # Flux prefers Float32 data\n",
    "(X_train, X_test), (y_train, y_test) = partition(\n",
    "    (X, y), 0.8,\n",
    "    multi = true,\n",
    "    shuffle = true,\n",
    "    rng=stable_rng(),\n",
    ");"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Instantiating the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's construct our model. This follows a similar setup to the one followed in the\n",
    "[Quick Start](../../index.md#Quick-Start)."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: For silent loading, specify `verbosity=0`. \n",
      "import MLJFlux ✔\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "NeuralNetworkClassifier(\n  builder = MLP(\n        hidden = (5, 4), \n        σ = NNlib.relu), \n  finaliser = NNlib.softmax, \n  optimiser = Optimisers.Adam(eta=0.01, beta=(0.9, 0.999), epsilon=1.0e-8), \n  loss = Flux.Losses.crossentropy, \n  epochs = 10, \n  batch_size = 8, \n  lambda = 0.0, \n  alpha = 0.0, \n  rng = StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7), \n  optimiser_changes_trigger_retraining = false, \n  acceleration = ComputationalResources.CPU1{Nothing}(nothing), \n  embedding_dims = Dict{Symbol, Real}())"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "cell_type": "code",
   "source": [
    "NeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux\n",
    "clf = NeuralNetworkClassifier(\n",
    "    builder=MLJFlux.MLP(; hidden=(5,4), σ=Flux.relu),\n",
    "    optimiser=Optimisers.Adam(0.01),\n",
    "    batch_size=8,\n",
    "    epochs=10,\n",
    "    rng=stable_rng(),\n",
    ")"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initial round of training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's train the model. Calling fit! will automatically train it for 100 epochs as\n",
    "specified above."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "trained Machine; caches model-specific representations of data\n  model: NeuralNetworkClassifier(builder = MLP(hidden = (5, 4), …), …)\n  args: \n    1:\tSource @040 ⏎ ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}\n    2:\tSource @771 ⏎ AbstractVector{ScientificTypesBase.Multiclass{3}}\n"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "cell_type": "code",
   "source": [
    "mach = machine(clf, X_train, y_train)\n",
    "fit!(mach, verbosity=0)"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's evaluate the training loss and validation accuracy"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.22537291610526708"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "cell_type": "code",
   "source": [
    "training_loss = cross_entropy(predict(mach, X_train), y_train)"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9333333333333333"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "cell_type": "code",
   "source": [
    "val_acc = accuracy(predict_mode(mach, X_test), y_test)"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "Poor performance it seems."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Incremental Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's train it for another 30 epochs at half the original learning rate. All we need\n",
    "to do is changes these hyperparameters and call fit again. It won't reset the model\n",
    "parameters before training."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Updating machine(NeuralNetworkClassifier(builder = MLP(hidden = (5, 4), …), …), …).\n",
      "[ Info: Loss is 0.2124\n",
      "[ Info: Loss is 0.1803\n",
      "[ Info: Loss is 0.1545\n",
      "[ Info: Loss is 0.1345\n",
      "[ Info: Loss is 0.1192\n",
      "[ Info: Loss is 0.1076\n",
      "[ Info: Loss is 0.09877\n",
      "[ Info: Loss is 0.09212\n",
      "[ Info: Loss is 0.08716\n",
      "[ Info: Loss is 0.08347\n",
      "[ Info: Loss is 0.08059\n",
      "[ Info: Loss is 0.07807\n",
      "[ Info: Loss is 0.07564\n",
      "[ Info: Loss is 0.07322\n",
      "[ Info: Loss is 0.07085\n",
      "[ Info: Loss is 0.06863\n",
      "[ Info: Loss is 0.06663\n",
      "[ Info: Loss is 0.06484\n",
      "[ Info: Loss is 0.06322\n",
      "[ Info: Loss is 0.06174\n",
      "[ Info: Loss is 0.06039\n",
      "[ Info: Loss is 0.05912\n",
      "[ Info: Loss is 0.05794\n",
      "[ Info: Loss is 0.05687\n",
      "[ Info: Loss is 0.05586\n",
      "[ Info: Loss is 0.05491\n",
      "[ Info: Loss is 0.05402\n",
      "[ Info: Loss is 0.05317\n",
      "[ Info: Loss is 0.05238\n",
      "[ Info: Loss is 0.05162\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "clf.optimiser = Optimisers.Adam(clf.optimiser.eta/2)\n",
    "clf.epochs = clf.epochs + 30\n",
    "fit!(mach, verbosity=2);"
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's evaluate the training loss and validation accuracy"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.04072631383146386"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "cell_type": "code",
   "source": [
    "training_loss = cross_entropy(predict(mach, X_train), y_train)"
   ],
   "metadata": {},
   "execution_count": 9
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9333333333333333"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "cell_type": "code",
   "source": [
    "training_acc = accuracy(predict_mode(mach, X_test), y_test)"
   ],
   "metadata": {},
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's much better. If we are rather interested in resetting the model parameters before\n",
    "fitting, we can do `fit(mach, force=true)`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.10"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.10",
   "language": "julia"
  }
 },
 "nbformat": 4
}
