{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Incremental Training with MLJFlux"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this workflow example we explore how to incrementally train MLJFlux models."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Activating project at `~/GoogleDrive/Julia/MLJ/MLJFlux/docs/src/common_workflows/incremental_training`\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "using Pkg\n",
    "Pkg.activate(@__DIR__);\n",
    "Pkg.instantiate();"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Julia version** is assumed to be 1.10.* This tutorial is available as a Jupyter\n",
    "notebook or julia script\n",
    "[here](https://github.com/FluxML/MLJFlux.jl/tree/dev/docs/src/common_workflows/incremental_training)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Basic Imports"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ               # Has MLJFlux models\n",
    "using Flux              # For more flexibility\n",
    "import RDatasets        # Dataset source\n",
    "import Optimisers       # native Flux.jl optimisers no longer supported"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading and Splitting the Data"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "iris = RDatasets.dataset(\"datasets\", \"iris\");\n",
    "y, X = unpack(iris, ==(:Species), colname -> true, rng=123);\n",
    "X = Float32.(X)      # To be compatible with type of network network parameters\n",
    "(X_train, X_test), (y_train, y_test) = partition(\n",
    "    (X, y), 0.8,\n",
    "    multi = true,\n",
    "    shuffle = true,\n",
    "    rng=42,\n",
    ");"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Instantiating the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's construct our model. This follows a similar setup to the one followed in the\n",
    "[Quick Start](../../index.md#Quick-Start)."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: For silent loading, specify `verbosity=0`. \n",
      "import MLJFlux ✔\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "NeuralNetworkClassifier(\n  builder = MLP(\n        hidden = (5, 4), \n        σ = NNlib.relu), \n  finaliser = NNlib.softmax, \n  optimiser = Adam(0.01, (0.9, 0.999), 1.0e-8), \n  loss = Flux.Losses.crossentropy, \n  epochs = 10, \n  batch_size = 8, \n  lambda = 0.0, \n  alpha = 0.0, \n  rng = 42, \n  optimiser_changes_trigger_retraining = false, \n  acceleration = ComputationalResources.CPU1{Nothing}(nothing))"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "cell_type": "code",
   "source": [
    "NeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux\n",
    "clf = NeuralNetworkClassifier(\n",
    "    builder=MLJFlux.MLP(; hidden=(5,4), σ=Flux.relu),\n",
    "    optimiser=Optimisers.Adam(0.01),\n",
    "    batch_size=8,\n",
    "    epochs=10,\n",
    "    rng=42,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initial round of training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's train the model. Calling fit! will automatically train it for 100 epochs as\n",
    "specified above."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Training machine(NeuralNetworkClassifier(builder = MLP(hidden = (5, 4), …), …), …).\n",
      "\rOptimising neural net:  18%[====>                    ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  27%[======>                  ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  36%[=========>               ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  45%[===========>             ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  55%[=============>           ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  64%[===============>         ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  73%[==================>      ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  82%[====================>    ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  91%[======================>  ]  ETA: 0:00:00\u001b[K\rOptimising neural net: 100%[=========================] Time: 0:00:00\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "trained Machine; caches model-specific representations of data\n  model: NeuralNetworkClassifier(builder = MLP(hidden = (5, 4), …), …)\n  args: \n    1:\tSource @068 ⏎ ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}\n    2:\tSource @767 ⏎ AbstractVector{ScientificTypesBase.Multiclass{3}}\n"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "cell_type": "code",
   "source": [
    "mach = machine(clf, X_train, y_train)\n",
    "fit!(mach)"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's evaluate the training loss and validation accuracy"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.4392339631006042"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "cell_type": "code",
   "source": [
    "training_loss = cross_entropy(predict(mach, X_train), y_train)"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "cell_type": "code",
   "source": [
    "val_acc = accuracy(predict_mode(mach, X_test), y_test)"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "Poor performance it seems."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Incremental Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's train it for another 30 epochs at half the original learning rate. All we need\n",
    "to do is changes these hyperparameters and call fit again. It won't reset the model\n",
    "parameters before training."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Updating machine(NeuralNetworkClassifier(builder = MLP(hidden = (5, 4), …), …), …).\n",
      "[ Info: Loss is 0.4393\n",
      "[ Info: Loss is 0.4317\n",
      "[ Info: Loss is 0.4244\n",
      "[ Info: Loss is 0.4171\n",
      "[ Info: Loss is 0.4096\n",
      "[ Info: Loss is 0.4017\n",
      "[ Info: Loss is 0.3931\n",
      "[ Info: Loss is 0.3838\n",
      "[ Info: Loss is 0.3737\n",
      "[ Info: Loss is 0.3626\n",
      "[ Info: Loss is 0.3505\n",
      "[ Info: Loss is 0.3382\n",
      "[ Info: Loss is 0.3244\n",
      "[ Info: Loss is 0.3095\n",
      "[ Info: Loss is 0.2954\n",
      "[ Info: Loss is 0.2813\n",
      "[ Info: Loss is 0.2654\n",
      "[ Info: Loss is 0.25\n",
      "[ Info: Loss is 0.235\n",
      "[ Info: Loss is 0.2203\n",
      "[ Info: Loss is 0.2118\n",
      "[ Info: Loss is 0.196\n",
      "[ Info: Loss is 0.179\n",
      "[ Info: Loss is 0.1674\n",
      "[ Info: Loss is 0.1586\n",
      "[ Info: Loss is 0.1469\n",
      "[ Info: Loss is 0.1353\n",
      "[ Info: Loss is 0.1251\n",
      "[ Info: Loss is 0.1173\n",
      "[ Info: Loss is 0.1102\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "clf.optimiser = Optimisers.Adam(clf.optimiser.eta/2)\n",
    "clf.epochs = clf.epochs + 30\n",
    "fit!(mach, verbosity=2);"
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's evaluate the training loss and validation accuracy"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.10519664737051289"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "cell_type": "code",
   "source": [
    "training_loss = cross_entropy(predict(mach, X_train), y_train)"
   ],
   "metadata": {},
   "execution_count": 9
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9666666666666667"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "cell_type": "code",
   "source": [
    "training_acc = accuracy(predict_mode(mach, X_test), y_test)"
   ],
   "metadata": {},
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's much better. If we are rather interested in resetting the model parameters before\n",
    "fitting, we can do `fit(mach, force=true)`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
