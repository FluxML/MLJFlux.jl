{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Live Training with MLJFlux"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This demonstration is available as a Jupyter notebook or julia script\n",
    "[here](https://github.com/FluxML/MLJFlux.jl/tree/dev/docs/src/common_workflows/live_training)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Pkg\n",
    "PKG_ENV = joinpath(@__DIR__, \"..\", \"..\", \"..\")\n",
    "Pkg.activate(PKG_ENV);\n",
    "Pkg.instantiate();"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**This script tested using Julia 1.10**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Basic Imports"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "using Flux\n",
    "import Optimisers\n",
    "using StableRNGs        # for reproducibility across Julia versions\n",
    "\n",
    "stable_rng() = StableRNGs.StableRNG(123)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Plots"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading and Splitting the Data"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "iris = load_iris() # a named-tuple of vectors\n",
    "y, X = unpack(iris, ==(:target), rng=stable_rng())\n",
    "X = fmap(column-> Float32.(column), X) # Flux prefers Float32 data"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Instantiating the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's construct our model. This follows a similar setup to the one followed in the\n",
    "[Quick Start](../../index.md#Quick-Start)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "NeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux\n",
    "\n",
    "clf = NeuralNetworkClassifier(\n",
    "    builder=MLJFlux.MLP(; hidden=(5,4), Ïƒ=Flux.relu),\n",
    "    optimiser=Optimisers.Adam(0.01),\n",
    "    batch_size=8,\n",
    "    epochs=50,\n",
    "    rng=stable_rng(),\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's wrap this in an iterated model. We will use a callback that makes a plot for\n",
    "validation losses each iteration."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "stop_conditions = [\n",
    "    Step(1),            # Repeatedly train for one iteration\n",
    "    NumberLimit(100),   # Don't train for more than 100 iterations\n",
    "]\n",
    "\n",
    "validation_losses =  []\n",
    "gr(reuse=true)                  # use the same window for plots\n",
    "function plot_loss(loss)\n",
    "    push!(validation_losses, loss)\n",
    "    display(plot(validation_losses, label=\"validation loss\", xlim=(1, 100)))\n",
    "    sleep(.01)  # to catch up with the plots while they are being generated\n",
    "end\n",
    "\n",
    "callbacks = [ WithLossDo(plot_loss),]\n",
    "\n",
    "iterated_model = IteratedModel(\n",
    "    model=clf,\n",
    "    resampling=Holdout(),\n",
    "    measures=log_loss,\n",
    "    iteration_parameter=:(epochs),\n",
    "    controls=vcat(stop_conditions, callbacks),\n",
    "    retrain=true,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Live Training\n",
    "Simply fitting the model is all we need"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mach = machine(iterated_model, X, y)\n",
    "fit!(mach)\n",
    "validation_losses"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the wrapped model sets aside some data on which to make out-of-sample\n",
    "estimates of the loss, which is how `validation_losses` are calculated. But if we use\n",
    "`mach` to make predictions on new input features, these are based on retraining the model\n",
    "on *all* provided data."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Xnew = (\n",
    "    sepal_length = Float32[5.8, 5.8, 5.8],\n",
    "    sepal_width = Float32[4.0, 2.6, 2.7],\n",
    "    petal_length = Float32[1.2, 4.0, 4.1],\n",
    "    petal_width = Float32[0.2, 1.2, 1.0],\n",
    ")\n",
    "\n",
    "predict_mode(mach, Xnew)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.10"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.10",
   "language": "julia"
  }
 },
 "nbformat": 4
}
