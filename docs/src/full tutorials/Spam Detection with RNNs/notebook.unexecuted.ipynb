{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SMS Spam Detection with RNNs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this tutorial we use a custom RNN model from Flux with MLJFlux to classify text\n",
    "messages as spam or ham. We will be using the [SMS Collection\n",
    "Dataset](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset) from Kaggle."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Pkg\n",
    "Pkg.activate(@__DIR__);\n",
    "Pkg.instantiate();"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Basic Imports"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "using MLJFlux\n",
    "using Flux\n",
    "import Optimisers       # Flux.jl native optimisers no longer supported\n",
    "using CSV               # Read data\n",
    "using DataFrames        # Read data\n",
    "using ScientificTypes   # Type coercion\n",
    "using WordTokenizers    # For tokenization\n",
    "using Languages         # For stop words"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reading Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We assume the [SMS Collection\n",
    "Dataset](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset) has been\n",
    "downloaded and is in a file called \"sms.csv\" in the same directory as the this script."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "df = CSV.read(joinpath(@__DIR__, \"sms.csv\"), DataFrame);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Display the first 5 rows with DataFrames"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "first(df, 5)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Text Preprocessing\n",
    "Let's define a function that given an SMS message would:\n",
    "- Tokenize it (i.e., convert it into a vector of words)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Remove stop words (i.e., words that are not useful for the analysis, like \"the\", \"a\",\n",
    "  etc.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Return the filtered vector of words"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "const STOP_WORDS = [\"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\",\n",
    "                    \"if\", \"in\", \"into\", \"is\", \"it\", \"no\", \"not\", \"of\", \"on\", \"or\",\n",
    "                    \"such\", \"that\", \"the\", \"their\", \"then\", \"there\", \"these\", \"they\",\n",
    "                    \"this\", \"to\", \"was\", \"will\", \"with\"]\n",
    "\n",
    "function preprocess_text(text)\n",
    "    # (1) Splitting texts into words (so later it can be a sequence of vectors)\n",
    "    tokens = WordTokenizers.tokenize(text)\n",
    "\n",
    "    # (2) Stop word removal\n",
    "    top_words = Languages.stopwords(Languages.English())\n",
    "    filtered_tokens = filter(token -> !(token in STOP_WORDS), tokens)\n",
    "\n",
    "    return filtered_tokens\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the vocabulary to be the set of all words in our training set. We also need a\n",
    "function that would map each word in a given sequence of words into its index in the\n",
    "dictionary (which is equivalent to representing the words as one-hot vectors)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now after we do this the sequences will all be numerical vectors but they will be of\n",
    "unequal length. Thus, to facilitate batching of data for the deep learning model, we\n",
    "need to decide on a specific maximum length for all sequences and:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- If a sequence is longer than the maximum length, we need to truncate it"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- If a sequence is shorter than the maximum length, we need to pad it with a new token"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lastly, we must also handle the case that an incoming text sequence may involve words\n",
    "never seen in training by represent all such out-of-vocabulary words with a new token."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will define a function that would do this for us."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function encode_and_equalize(text_seq, vocab_dict, max_length, pad_val, oov_val)\n",
    "    # (1) encode using the vocabulary\n",
    "    text_seq_inds = [get(vocab_dict, word, oov_val) for word in text_seq]\n",
    "\n",
    "    # (2) truncate sequence if > max_length\n",
    "    length(text_seq_inds) > max_length && (text_seq_inds = text_seq_inds[1:max_length])\n",
    "\n",
    "    # (3) pad with pad_val\n",
    "    text_seq_inds = vcat(text_seq_inds, fill(pad_val, max_length - length(text_seq_inds)))\n",
    "\n",
    "    return text_seq_inds\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparing Data\n",
    "Splitting the data"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x_data, y_data = unpack(df, ==(:Message), ==(:Category))\n",
    "y_data = coerce(y_data, Multiclass);\n",
    "\n",
    "(x_train, x_val), (y_train, y_val) = partition(\n",
    "    (x_data, y_data),\n",
    "    0.8,\n",
    "    multi = true,\n",
    "    shuffle = true,\n",
    "    rng = 42,\n",
    ");"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's process the training and validation sets:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x_train_processed = [preprocess_text(text) for text in x_train]\n",
    "x_val_processed = [preprocess_text(text) for text in x_val];"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "sanity check"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "println(x_train_processed[1], \" is \", y_data[1])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the vocabulary from the training data"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "vocab = unique(vcat(x_train_processed...))\n",
    "vocab_dict = Dict(word => idx for (idx, word) in enumerate(vocab))\n",
    "vocab_size = length(vocab)\n",
    "pad_val, oov_val = vocab_size + 1, vocab_size + 2\n",
    "max_length = 12                 # can choose this more smartly if you wish"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Encode and equalize training and validation data:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x_train_processed_equalized = [\n",
    "    encode_and_equalize(seq, vocab_dict, max_length, pad_val, oov_val) for\n",
    "        seq in x_train_processed\n",
    "        ]\n",
    "x_val_processed_equalized = [\n",
    "    encode_and_equalize(seq, vocab_dict, max_length, pad_val, oov_val) for\n",
    "        seq in x_val_processed\n",
    "        ]\n",
    "x_train_processed_equalized[1:5]        # all sequences are encoded and of the same length"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert both structures into matrix form:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "matrixify(v) = reduce(hcat, v)'\n",
    "x_train_processed_equalized_fixed = matrixify(x_train_processed_equalized)\n",
    "x_val_processed_equalized_fixed = matrixify(x_val_processed_equalized)\n",
    "size(x_train_processed_equalized_fixed)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Instantiate Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the model, we will use a RNN from Flux. We will average the hidden states\n",
    "corresponding to any sequence then pass that to a dense layer for classification."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this, we need to define a custom Flux layer to perform the averaging operation:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "struct Mean end\n",
    "Flux.@layer Mean\n",
    "(m::Mean)(x) = mean(x, dims = 2)[:, 1, :]   # [batch_size, seq_len, hidden_dim] => [batch_size, 1, hidden_dim]=> [batch_size, hidden_dim]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For compatibility, we will also define a layer that simply casts the input to integers\n",
    "as the embedding layer in Flux expects integers but the MLJFlux model expects floats:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "struct Intify end\n",
    "Flux.@layer Intify\n",
    "(m::Intify)(x) = Int.(x)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we define our network:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "builder = MLJFlux.@builder begin\n",
    "    Chain(\n",
    "        Intify(),                         # Cast input to integer\n",
    "        Embedding(vocab_size + 2 => 300), # Embedding layer\n",
    "        RNN(300, 50, tanh),               # RNN layer\n",
    "        Mean(),                           # Mean pooling layer\n",
    "        Dense(50, 2),                     # Classification dense layer\n",
    "    )\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that we used an embedding layer with input dimensionality `vocab_size + 2` to\n",
    "take into account the padding and out-of-vocabulary tokens. Recall that the indices in\n",
    "our input correspond to one-hot-vectors and the embedding layer's purpose is to learn to\n",
    "map them into meaningful dense vectors (of dimensionality 300 here)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Load and instantiate model"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "NeuralNetworkClassifier = @load NeuralNetworkClassifier pkg = MLJFlux\n",
    "clf = NeuralNetworkClassifier(\n",
    "    builder = builder,\n",
    "    optimiser = Optimisers.Adam(0.1),\n",
    "    batch_size = 128,\n",
    "    epochs = 10,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Wrap it in a machine"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x_train_processed_equalized_fixed = coerce(x_train_processed_equalized_fixed, Continuous)\n",
    "mach = machine(clf, x_train_processed_equalized_fixed, y_train)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train the Model"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fit!(mach)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate the Model"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ŷ = predict_mode(mach, x_val_processed_equalized_fixed)\n",
    "balanced_accuracy(ŷ, y_val)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Acceptable performance. Let's see some live examples:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Random: Random;\n",
    "Random.seed!(99);\n",
    "\n",
    "z = rand(x_val)\n",
    "z_processed = preprocess_text(z)\n",
    "z_encoded_equalized =\n",
    "    encode_and_equalize(z_processed, vocab_dict, max_length, pad_val, oov_val)\n",
    "z_encoded_equalized_fixed = matrixify([z_encoded_equalized])\n",
    "z_encoded_equalized_fixed = coerce(z_encoded_equalized_fixed, Continuous)\n",
    "z_pred = predict_mode(mach, z_encoded_equalized_fixed)\n",
    "\n",
    "print(\"SMS: `$(z)` and the prediction is `$(z_pred)`\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
