{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model Comparison with MLJFlux"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this workflow example, we see how we can compare different machine learning models\n",
    "with a neural network from MLJFlux."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Activating project at `~/GoogleDrive/Julia/MLJ/MLJFlux/docs/src/workflow examples/Comparison`\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "using Pkg\n",
    "Pkg.activate(@__DIR__);\n",
    "Pkg.instantiate();"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Julia version** is assumed to be 1.10.*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Basic Imports"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ               # Has MLJFlux models\n",
    "using Flux              # For more flexibility\n",
    "import RDatasets        # Dataset source\n",
    "using DataFrames        # To visualize hyperparameter search results\n",
    "import Optimisers       # native Flux.jl optimisers no longer supported"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading and Splitting the Data"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "iris = RDatasets.dataset(\"datasets\", \"iris\");\n",
    "y, X = unpack(iris, ==(:Species), colname -> true, rng=123);"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Instantiating the models Now let's construct our model. This follows a similar setup\n",
    "to the one followed in the [Quick Start](../../index.md#Quick-Start)."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: For silent loading, specify `verbosity=0`. \n",
      "import MLJFlux ✔\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "NeuralNetworkClassifier(\n  builder = MLP(\n        hidden = (5, 4), \n        σ = NNlib.relu), \n  finaliser = NNlib.softmax, \n  optimiser = Adam(0.01, (0.9, 0.999), 1.0e-8), \n  loss = Flux.Losses.crossentropy, \n  epochs = 50, \n  batch_size = 8, \n  lambda = 0.0, \n  alpha = 0.0, \n  rng = 42, \n  optimiser_changes_trigger_retraining = false, \n  acceleration = CPU1{Nothing}(nothing))"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "cell_type": "code",
   "source": [
    "NeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux\n",
    "\n",
    "clf1 = NeuralNetworkClassifier(\n",
    "    builder=MLJFlux.MLP(; hidden=(5,4), σ=Flux.relu),\n",
    "    optimiser=Optimisers.Adam(0.01),\n",
    "    batch_size=8,\n",
    "    epochs=50,\n",
    "    rng=42\n",
    "    )"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's as well load and construct three other classical machine learning models:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: For silent loading, specify `verbosity=0`. \n",
      "import MLJMultivariateStatsInterface ✔\n",
      "[ Info: For silent loading, specify `verbosity=0`. \n",
      "import MLJDecisionTreeInterface ✔\n",
      "[ Info: For silent loading, specify `verbosity=0`. \n",
      "import MLJXGBoostInterface ✔\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "BayesianLDA = @load BayesianLDA pkg=MultivariateStats\n",
    "clf2 = BayesianLDA()\n",
    "RandomForestClassifier = @load RandomForestClassifier pkg=DecisionTree\n",
    "clf3 = RandomForestClassifier()\n",
    "XGBoostClassifier = @load XGBoostClassifier pkg=XGBoost\n",
    "clf4 = XGBoostClassifier();"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Wrapping One of the Models in a TunedModel Instead of just comparing with four\n",
    "models with the default/given hyperparameters, we will give `XGBoostClassifier` an\n",
    "unfair advantage By wrapping it in a `TunedModel` that considers the best learning rate\n",
    "η for the model."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "r1 = range(clf4, :eta, lower=0.01, upper=0.5, scale=:log10)\n",
    "tuned_model_xg = TunedModel(\n",
    "    model=clf4,\n",
    "    ranges=[r1],\n",
    "    tuning=Grid(resolution=10),\n",
    "    resampling=CV(nfolds=5, rng=42),\n",
    "    measure=cross_entropy,\n",
    ");"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Of course, one can wrap each of the four in a TunedModel if they are interested in\n",
    "comparing the models over a large set of their hyperparameters."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparing the models\n",
    "We simply pass the four models to the `models` argument of the `TunedModel` construct"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "tuned_model = TunedModel(\n",
    "    models=[clf1, clf2, clf3, tuned_model_xg],\n",
    "    tuning=Explicit(),\n",
    "    resampling=CV(nfolds=5, rng=42),\n",
    "    measure=cross_entropy,\n",
    ");"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then wrapping our tuned model in a machine and fitting it."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌ Warning: Layer with Float32 parameters got Float64 input.\n",
      "│   The input will be converted, but any earlier layers may be very slow.\n",
      "│   layer = Dense(4 => 5, relu)  # 25 parameters\n",
      "│   summary(x) = \"4×8 Matrix{Float64}\"\n",
      "└ @ Flux ~/.julia/packages/Flux/Wz6D4/src/layers/stateless.jl:60\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "mach = machine(tuned_model, X, y);\n",
    "fit!(mach, verbosity=0);"
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's see the history for more details on the performance for each of the models"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[1m4×2 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m mlp                               \u001b[0m\u001b[1m measurement \u001b[0m\n     │\u001b[90m Probabil…                         \u001b[0m\u001b[90m Float64     \u001b[0m\n─────┼────────────────────────────────────────────────\n   1 │ BayesianLDA(method = gevd, …)        0.0610826\n   2 │ NeuralNetworkClassifier(builder …    0.0857014\n   3 │ RandomForestClassifier(max_depth…    0.109869\n   4 │ ProbabilisticTunedModel(model = …    0.221056",
      "text/html": [
       "<div><div style = \"float: left;\"><span>4×2 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">mlp</th><th style = \"text-align: left;\">measurement</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Probabilistic\" style = \"text-align: left;\">Probabil…</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">BayesianLDA(method = gevd, …)</td><td style = \"text-align: right;\">0.0610826</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">NeuralNetworkClassifier(builder = MLP(hidden = (5, 4), …), …)</td><td style = \"text-align: right;\">0.0857014</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">RandomForestClassifier(max_depth = -1, …)</td><td style = \"text-align: right;\">0.109869</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">ProbabilisticTunedModel(model = XGBoostClassifier(test = 1, …), …)</td><td style = \"text-align: right;\">0.221056</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "cell_type": "code",
   "source": [
    "history = report(mach).history\n",
    "history_df = DataFrame(\n",
    "    mlp = [x[:model] for x in history],\n",
    "    measurement = [x[:measurement][1] for x in history],\n",
    ")\n",
    "sort!(history_df, [order(:measurement)])"
   ],
   "metadata": {},
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is Occam's razor in practice."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
