{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model Composition with MLJFlux"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this workflow example, we see how MLJFlux enables composing MLJ models with MLJFlux models. We will assume a\n",
    "class imbalance setting and wrap an oversampler with a deep learning model from MLJFlux."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Julia version** is assumed to be 1.10.*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Basic Imports"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ               # Has MLJFlux models\n",
    "using Flux              # For more flexibility\n",
    "import RDatasets        # Dataset source\n",
    "import Random           # To create imbalance\n",
    "import Imbalance        # To solve the imbalance"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading and Splitting the Data"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "iris = RDatasets.dataset(\"datasets\", \"iris\");\n",
    "y, X = unpack(iris, ==(:Species), colname -> true, rng=123);\n",
    "X = Float32.(X);      # To be compatible with type of network network parameters"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "To simulate an imbalanced dataset, we will take a random sample:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "versicolor: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 28 (65.1%) \n",
      "virginica:  ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 29 (67.4%) \n",
      "setosa:     ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 43 (100.0%) \n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "Random.seed!(803429)\n",
    "subset_indices = rand(1:size(X, 1), 100)\n",
    "X, y = X[subset_indices, :], y[subset_indices]\n",
    "Imbalance.checkbalance(y)"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Instantiating the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's load `BorderlineSMOTE1` to oversample the data and `Standardizer` to standardize it."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: For silent loading, specify `verbosity=0`. \n",
      "import MLJFlux ✔\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "NeuralNetworkClassifier(\n  builder = MLP(\n        hidden = (5, 4), \n        σ = NNlib.relu), \n  finaliser = NNlib.softmax, \n  optimiser = Adam(0.01, (0.9, 0.999), 1.0e-8, IdDict{Any, Any}()), \n  loss = Flux.Losses.crossentropy, \n  epochs = 50, \n  batch_size = 8, \n  lambda = 0.0, \n  alpha = 0.0, \n  rng = 42, \n  optimiser_changes_trigger_retraining = false, \n  acceleration = CPU1{Nothing}(nothing))"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "cell_type": "code",
   "source": [
    "BorderlineSMOTE1 = @load BorderlineSMOTE1 pkg=Imbalance verbosity=0\n",
    "NeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux\n",
    "# We didn't need to load Standardizer because it is a  local model for MLJ (see `localmodels()`)\n",
    "\n",
    "clf = NeuralNetworkClassifier(\n",
    "    builder=MLJFlux.MLP(; hidden=(5,4), σ=Flux.relu),\n",
    "    optimiser=Flux.ADAM(0.01),\n",
    "    batch_size=8,\n",
    "    epochs=50,\n",
    "    rng=42\n",
    "    )"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we wrap the oversampler with the neural network via the `BalancedModel` construct. This comes from `MLJBalancing`\n",
    "And allows combining resampling methods with MLJ models in a sequential pipeline."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Standardizer(\n  features = Symbol[], \n  ignore = false, \n  ordered_factor = false, \n  count = false)"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "cell_type": "code",
   "source": [
    "oversampler = BorderlineSMOTE1(k=5, ratios=1.0, rng=42)\n",
    "balanced_model = BalancedModel(model=clf, balancer1=oversampler)\n",
    "standarizer = Standardizer()"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's compose the balanced model with a standardizer."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "ProbabilisticPipeline(\n  standardizer = Standardizer(\n        features = Symbol[], \n        ignore = false, \n        ordered_factor = false, \n        count = false), \n  balanced_model_probabilistic = BalancedModelProbabilistic(\n        model = NeuralNetworkClassifier(builder = MLP(hidden = (5, 4), …), …), \n        balancer1 = BorderlineSMOTE1(m = 5, …)), \n  cache = true)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "cell_type": "code",
   "source": [
    "pipeline = standarizer |> balanced_model"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "By this, any training data will be standardized then oversampled then passed to the model. Meanwhile,\n",
    "for inference, the standardizer will automatically use the training set's mean and std and the oversampler\n",
    "will be transparent."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the Composed Model\n",
    "It's indistinguishable from training a single model."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Training machine(ProbabilisticPipeline(standardizer = Standardizer(features = Symbol[], …), …), …).\n",
      "[ Info: Training machine(:standardizer, …).\n",
      "[ Info: Training machine(:balanced_model_probabilistic, …).\n",
      "[ Info: Training machine(BorderlineSMOTE1(m = 5, …), …).\n",
      "[ Info: Training machine(:model, …).\n",
      "[ Info: After filtering, the mapping from each class to number of borderline points is (\"virginica\" => 1, \"versicolor\" => 2).\n",
      "[ Info: After filtering, the mapping from each class to number of borderline points is (\"virginica\" => 1, \"versicolor\" => 2).\n",
      "┌ Warning: Layer with Float32 parameters got Float64 input.\n",
      "│   The input will be converted, but any earlier layers may be very slow.\n",
      "│   layer = Dense(4 => 5, relu)  # 25 parameters\n",
      "│   summary(x) = \"4×8 Matrix{Float64}\"\n",
      "└ @ Flux ~/.julia/packages/Flux/Wz6D4/src/layers/stateless.jl:60\n",
      "\rOptimising neural net:   4%[>                        ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  47%[===========>             ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  49%[============>            ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  51%[============>            ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  53%[=============>           ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  55%[=============>           ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  57%[==============>          ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  59%[==============>          ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  61%[===============>         ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  63%[===============>         ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  65%[================>        ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  67%[================>        ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  69%[=================>       ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  71%[=================>       ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  73%[==================>      ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  75%[==================>      ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  76%[===================>     ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  78%[===================>     ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  80%[====================>    ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  82%[====================>    ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  84%[=====================>   ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  86%[=====================>   ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  88%[======================>  ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  90%[======================>  ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  92%[=======================> ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  94%[=======================> ]  ETA: 0:00:00\u001b[K\rOptimising neural net:  96%[========================>]  ETA: 0:00:00\u001b[K\rOptimising neural net:  98%[========================>]  ETA: 0:00:00\u001b[K\rOptimising neural net: 100%[=========================] Time: 0:00:00\u001b[K\n",
      "[ Info: After filtering, the mapping from each class to number of borderline points is (\"virginica\" => 3, \"versicolor\" => 1).\n",
      "[ Info: After filtering, the mapping from each class to number of borderline points is (\"virginica\" => 3, \"versicolor\" => 1).\n",
      "[ Info: After filtering, the mapping from each class to number of borderline points is (\"versicolor\" => 2).\n",
      "┌ Warning: Cannot oversample a class with no borderline points. Skipping.\n",
      "└ @ Imbalance ~/.julia/packages/Imbalance/knJL1/src/oversampling_methods/borderline_smote1/borderline_smote1.jl:67\n",
      "[ Info: After filtering, the mapping from each class to number of borderline points is (\"versicolor\" => 2).\n",
      "┌ Warning: Cannot oversample a class with no borderline points. Skipping.\n",
      "└ @ Imbalance ~/.julia/packages/Imbalance/knJL1/src/oversampling_methods/borderline_smote1/borderline_smote1.jl:67\n",
      "\rEvaluating over 5 folds:  40%[==========>              ]  ETA: 0:00:00\u001b[K[ Info: After filtering, the mapping from each class to number of borderline points is (\"virginica\" => 1, \"versicolor\" => 2).\n",
      "[ Info: After filtering, the mapping from each class to number of borderline points is (\"virginica\" => 1, \"versicolor\" => 2).\n",
      "[ Info: After filtering, the mapping from each class to number of borderline points is (\"virginica\" => 1).\n",
      "┌ Warning: Cannot oversample a class with no borderline points. Skipping.\n",
      "└ @ Imbalance ~/.julia/packages/Imbalance/knJL1/src/oversampling_methods/borderline_smote1/borderline_smote1.jl:67\n",
      "[ Info: After filtering, the mapping from each class to number of borderline points is (\"virginica\" => 1).\n",
      "┌ Warning: Cannot oversample a class with no borderline points. Skipping.\n",
      "└ @ Imbalance ~/.julia/packages/Imbalance/knJL1/src/oversampling_methods/borderline_smote1/borderline_smote1.jl:67\n",
      "[ Info: After filtering, the mapping from each class to number of borderline points is (\"virginica\" => 3, \"versicolor\" => 3).\n",
      "[ Info: After filtering, the mapping from each class to number of borderline points is (\"virginica\" => 3, \"versicolor\" => 3).\n",
      "\rEvaluating over 5 folds: 100%[=========================] Time: 0:00:00\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PerformanceEvaluation object with these fields:\n  model, measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows, resampling, repeats\nExtract:\n┌────────────┬──────────────┬─────────────┬─────────┬───────────────────────────\n│\u001b[22m measure    \u001b[0m│\u001b[22m operation    \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m 1.96*SE \u001b[0m│\u001b[22m per_fold                \u001b[0m ⋯\n├────────────┼──────────────┼─────────────┼─────────┼───────────────────────────\n│ Accuracy() │ predict_mode │ 0.98        │ 0.0268  │ [1.0, 1.0, 0.95, 0.95, 1 ⋯\n└────────────┴──────────────┴─────────────┴─────────┴───────────────────────────\n\u001b[36m                                                                1 column omitted\u001b[0m\n"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "cell_type": "code",
   "source": [
    "mach = machine(pipeline, X, y)\n",
    "fit!(mach)\n",
    "cv=CV(nfolds=5)\n",
    "evaluate!(mach, resampling=cv, measure=accuracy)"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.0",
   "language": "julia"
  }
 },
 "nbformat": 4
}
