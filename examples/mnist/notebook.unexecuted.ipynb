{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Using MLJ to classifiy the MNIST image dataset"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Pkg\n",
    "const DIR = @__DIR__\n",
    "Pkg.activate(DIR)\n",
    "Pkg.instantiate()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Julia version** is assumed to be ^1.10"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "using Flux\n",
    "import MLJFlux\n",
    "import MLUtils\n",
    "import MLJIteration # for `skip`"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "If running on a GPU, you will also need to `import CUDA` and `import cuDNN`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Plots\n",
    "gr(size=(600, 300*(sqrt(5)-1)));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Basic training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Downloading the MNIST image dataset:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import MLDatasets: MNIST\n",
    "\n",
    "ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true\n",
    "images, labels = MNIST(split=:train)[:];"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In MLJ, integers cannot be used for encoding categorical data, so we\n",
    "must force the labels to have the `Multiclass` [scientific\n",
    "type](https://juliaai.github.io/ScientificTypes.jl/dev/). For\n",
    "more on this, see [Working with Categorical\n",
    "Data](https://alan-turing-institute.github.io/MLJ.jl/dev/working_with_categorical_data/)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "labels = coerce(labels, Multiclass);\n",
    "images = coerce(images, GrayImage);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Checking scientific types:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@assert scitype(images) <: AbstractVector{<:Image}\n",
    "@assert scitype(labels) <: AbstractVector{<:Finite}"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looks good."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For general instructions on coercing image data, see [Type coercion\n",
    "for image\n",
    "data](https://juliaai.github.io/ScientificTypes.jl/dev/#Type-coercion-for-image-data)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "images[1]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start by defining a suitable `Builder` object. This is a recipe\n",
    "for building the neural network. Our builder will work for images of\n",
    "any (constant) size, whether they be color or black and white (ie,\n",
    "single or multi-channel).  The architecture always consists of six\n",
    "alternating convolution and max-pool layers, and a final dense\n",
    "layer; the filter size and the number of channels after each\n",
    "convolution layer is customisable."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import MLJFlux\n",
    "struct MyConvBuilder\n",
    "    filter_size::Int\n",
    "    channels1::Int\n",
    "    channels2::Int\n",
    "    channels3::Int\n",
    "end\n",
    "\n",
    "function MLJFlux.build(b::MyConvBuilder, rng, n_in, n_out, n_channels)\n",
    "    k, c1, c2, c3 = b.filter_size, b.channels1, b.channels2, b.channels3\n",
    "    mod(k, 2) == 1 || error(\"`filter_size` must be odd. \")\n",
    "    p = div(k - 1, 2) # padding to preserve image size\n",
    "    init = Flux.glorot_uniform(rng)\n",
    "    front = Chain(\n",
    "        Conv((k, k), n_channels => c1, pad=(p, p), relu, init=init),\n",
    "        MaxPool((2, 2)),\n",
    "        Conv((k, k), c1 => c2, pad=(p, p), relu, init=init),\n",
    "        MaxPool((2, 2)),\n",
    "        Conv((k, k), c2 => c3, pad=(p, p), relu, init=init),\n",
    "        MaxPool((2 ,2)),\n",
    "        MLUtils.flatten)\n",
    "    d = Flux.outputsize(front, (n_in..., n_channels, 1)) |> first\n",
    "    return Chain(front, Dense(d, n_out, init=init))\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Notes.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- There is no final `softmax` here, as this is applied by default in all MLJFLux\n",
    "  classifiers. Customisation of this behaviour is controlled using using the `finaliser`\n",
    "  hyperparameter of the classifier.\n",
    "\n",
    "- Instead of calculating the padding `p`, Flux can infer the required padding in each\n",
    "  dimension, which you enable by replacing `pad = (p, p)` with `pad = SamePad()`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now define the MLJ model."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ImageClassifier = @load ImageClassifier\n",
    "clf = ImageClassifier(\n",
    "    builder=MyConvBuilder(3, 16, 32, 32),\n",
    "    batch_size=50,\n",
    "    epochs=10,\n",
    "    rng=123,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can add Flux options `optimiser=...` and `loss=...` in the above constructor\n",
    "call. At present, `loss` must be a Flux-compatible loss, not an MLJ measure. To run on a\n",
    "GPU, add to the constructor `acceleration=CUDALib()` and omit `rng`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For illustration purposes, we won't use all the data here:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "train = 1:500\n",
    "test = 501:1000"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Binding the model with data in an MLJ machine:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mach = machine(clf, images, labels);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training for 10 epochs on the first 500 images:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fit!(mach, rows=train, verbosity=2);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inspecting:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "report(mach)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "chain = fitted_params(mach)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Flux.params(chain)[2]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adding 20 more epochs:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "clf.epochs = clf.epochs + 20\n",
    "fit!(mach, rows=train);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Computing an out-of-sample estimate of the loss:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "predicted_labels = predict(mach, rows=test);\n",
    "cross_entropy(predicted_labels, labels[test])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or, in one line:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "evaluate!(mach,\n",
    "          resampling=Holdout(fraction_train=0.5),\n",
    "          measure=cross_entropy,\n",
    "          rows=1:1000,\n",
    "          verbosity=0)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wrapping the MLJFlux model with iteration controls"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Any iterative MLJFlux model can be wrapped in *iteration controls*,\n",
    "as we demonstrate next. For more on MLJ's `IteratedModel` wrapper,\n",
    "see the [MLJ\n",
    "documentation](https://alan-turing-institute.github.io/MLJ.jl/dev/controlling_iterative_models/)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The \"self-iterating\" classifier, called `iterated_clf` below, is for\n",
    "iterating the image classifier defined above until one of the\n",
    "following stopping criterion apply:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- `Patience(3)`: 3 consecutive increases in the loss\n",
    "- `InvalidValue()`: an out-of-sample loss, or a training loss, is `NaN`, `Inf`, or `-Inf`\n",
    "- `TimeLimit(t=5/60)`: training time has exceeded 5 minutes\n",
    "\n",
    "These checks (and other controls) will be applied every two epochs\n",
    "(because of the `Step(2)` control). Additionally, training a\n",
    "machine bound to `iterated_clf` will:\n",
    "\n",
    "- save a snapshot of the machine every three control cycles (every six epochs)\n",
    "- record traces of the out-of-sample loss and training losses for plotting\n",
    "- record mean value traces of each Flux parameter for plotting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For a complete list of controls, see [this\n",
    "table](https://alan-turing-institute.github.io/MLJ.jl/dev/controlling_iterative_models/#Controls-provided)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Wrapping the classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some helpers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To extract Flux params from an MLJFlux machine"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "parameters(mach) = vec.(Flux.params(fitted_params(mach)));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To store the traces:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "losses = []\n",
    "training_losses = []\n",
    "parameter_means = Float32[];\n",
    "epochs = []"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To update the traces:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "update_loss(loss) = push!(losses, loss)\n",
    "update_training_loss(losses) = push!(training_losses, losses[end])\n",
    "update_means(mach) = append!(parameter_means, mean.(parameters(mach)));\n",
    "update_epochs(epoch) = push!(epochs, epoch)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The controls to apply:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "save_control =\n",
    "    MLJIteration.skip(Save(joinpath(DIR, \"mnist.jls\")), predicate=3)\n",
    "\n",
    "controls=[\n",
    "    Step(2),\n",
    "    Patience(3),\n",
    "    InvalidValue(),\n",
    "    TimeLimit(5/60),\n",
    "    save_control,\n",
    "    WithLossDo(),\n",
    "    WithLossDo(update_loss),\n",
    "    WithTrainingLossesDo(update_training_loss),\n",
    "    Callback(update_means),\n",
    "    WithIterationsDo(update_epochs),\n",
    "];"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The \"self-iterating\" classifier:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "iterated_clf = IteratedModel(\n",
    "    clf,\n",
    "    controls=controls,\n",
    "    resampling=Holdout(fraction_train=0.7),\n",
    "    measure=log_loss,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Binding the wrapped model to data:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mach = machine(iterated_clf, images, labels);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fit!(mach, rows=train);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparison of the training and out-of-sample losses:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(\n",
    "    epochs,\n",
    "    losses,\n",
    "    xlab = \"epoch\",\n",
    "    ylab = \"cross entropy\",\n",
    "    label=\"out-of-sample\",\n",
    ")\n",
    "plot!(epochs, training_losses, label=\"training\")\n",
    "\n",
    "savefig(joinpath(DIR, \"loss.png\"))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evolution of weights"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "n_epochs =  length(losses)\n",
    "n_parameters = div(length(parameter_means), n_epochs)\n",
    "parameter_means2 = reshape(copy(parameter_means), n_parameters, n_epochs)'\n",
    "plot(\n",
    "    epochs,\n",
    "    parameter_means2,\n",
    "    title=\"Flux parameter mean weights\",\n",
    "    xlab = \"epoch\",\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note.** The higher the number, the deeper the layer we are weight-averaging."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "savefig(joinpath(DIR, \"weights.png\"))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Retrieving a snapshot for a prediction:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mach2 = machine(joinpath(DIR, \"mnist3.jls\"))\n",
    "predict_mode(mach2, images[501:503])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Restarting training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mutating `iterated_clf.controls` or `clf.epochs` (which is otherwise\n",
    "ignored) will allow you to restart training from where it left off."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "iterated_clf.controls[2] = Patience(4)\n",
    "fit!(mach, rows=train)\n",
    "\n",
    "plot(\n",
    "    epochs,\n",
    "    losses,\n",
    "    xlab = \"epoch\",\n",
    "    ylab = \"cross entropy\",\n",
    "    label=\"out-of-sample\",\n",
    ")\n",
    "plot!(epochs, training_losses, label=\"training\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
